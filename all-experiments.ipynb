{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6 blocks pruned model vs oringinal","metadata":{}},{"cell_type":"code","source":"# -6-blocks\n# ============================================================\n# ViT (timm) ‚Äî Stability + Monitoring + Pruning + LIGHT VALIDATION\n# - 70/15/15 stratified split from all images (ignores folder splits)\n# - Temperature-scaled attention (T=1.5)\n# - Post-attention LayerNorm (safe wrapper)\n# - Label smoothing, AMP, gradient clipping\n# - Attention entropy monitoring\n# - ‚úÖ LIGHT VALIDATION: quick val passes (few batches) every N epochs\n# - ‚úÖ EARLY STOPPING: patience/min_delta, restore best checkpoint\n# - ‚úÖ LIGHT HYPERPARAM TUNING: 5 epochs per candidate (tiny subset)\n# - ‚úÇÔ∏è Prune 6 blocks (keep 6), brief fine-tune\n# - üìä Prints: train/test acc, precision/recall/F1, params, FLOPs, memory, times\n# ============================================================\n\n!pip install -q timm scikit-learn ptflops\n\nimport os, random, time, gc, math, copy\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.amp import GradScaler, autocast\n\n# -----------------------------\n# üîß Reproducibility\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# ‚öôÔ∏è Validation / Tuning knobs\n# -----------------------------\nVAL_EVERY_N_EPOCHS = 1         # run lightweight val every N epochs\nVAL_MAX_BATCHES     = 6        # validate on at most this many batches (keep small)\nPATIENCE            = 3        # early stopping patience (epochs)\nMIN_DELTA           = 1e-3     # minimum improvement in val loss to reset patience\n\nENABLE_TUNING       = True     # pilot sweep to pick LR/WD quickly\nTUNE_EPOCHS         = 5        # 5 epochs per candidate\nTUNE_TRAIN_FRAC     = 0.08     # use 8% of train for quick tuning\nTUNE_VAL_MAX_BATCH  = 3        # validate only a handful of batches in tuning\n\n# -------------------------------------------------------\n# üìÅ Data root ‚Äî your Kaggle dataset\n# -------------------------------------------------------\ndata_root = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\ntrain_dir = os.path.join(data_root, \"train\")\nval_dir   = os.path.join(data_root, \"val\")\ntest_dir  = os.path.join(data_root, \"test\")\n\n# -------------------------------------------------------\n# üñºÔ∏è Transforms\n# -------------------------------------------------------\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\nval_test_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\n# -------------------------------------------------------\n# üì¶ Build unified sample list then 70/15/15 split (stratified)\n# -------------------------------------------------------\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\ndef list_class_dirs(parent): return sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])\n\nif os.path.exists(train_dir) and len(list_class_dirs(train_dir)) > 0:\n    classes = list_class_dirs(train_dir)\nelif os.path.exists(val_dir) and len(list_class_dirs(val_dir)) > 0:\n    classes = list_class_dirs(val_dir)\nelse:\n    classes = list_class_dirs(test_dir)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\ndef gather_samples(split_dir):\n    samples = []\n    for cls in classes:\n        cdir = os.path.join(split_dir, cls)\n        if not os.path.isdir(cdir): continue\n        for root, _, files in os.walk(cdir):\n            for f in files:\n                if f.lower().endswith(IMG_EXTS):\n                    samples.append((os.path.join(root, f), class_to_idx[cls]))\n    return samples\n\nall_samples = []\nfor d in [train_dir, val_dir, test_dir]:\n    all_samples.extend(gather_samples(d))\nassert len(all_samples) > 0, \"No images found. Check dataset paths.\"\n\nlabels  = np.array([lbl for _, lbl in all_samples])\nindices = np.arange(len(all_samples))\n\nsss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, test_size=0.30, random_state=42)\ntrain_idx, temp_idx = next(sss1.split(indices, labels))\ntemp_labels = labels[temp_idx]\nsss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, test_size=0.50, random_state=42)\nval_rel_idx, test_rel_idx = next(sss2.split(temp_idx, temp_labels))\nval_idx, test_idx = temp_idx[val_rel_idx], temp_idx[test_rel_idx]\n\ndef take(idxs): return [all_samples[i] for i in idxs]\ntrain_samples, val_samples, test_samples = take(train_idx), take(val_idx), take(test_idx)\n\nprint(f\"‚úÖ Split sizes ‚Äî Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\nprint(f\"    Classes ({len(classes)}): {classes}\")\n\n# -------------------------------------------------------\n# üß∞ Dataset from file paths\n# -------------------------------------------------------\nclass PathImageDataset(Dataset):\n    def __init__(self, samples, classes, transform=None):\n        self.samples = samples; self.classes = classes; self.transform = transform\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img, target\n\ntrain_dataset = PathImageDataset(train_samples, classes, transform=train_transforms)\nval_dataset   = PathImageDataset(val_samples,   classes, transform=val_test_transforms)\ntest_dataset  = PathImageDataset(test_samples,  classes, transform=val_test_transforms)\n\ndef make_loader(dataset, batch_size=32, shuffle=False):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ntrain_loader = make_loader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = make_loader(val_dataset,   batch_size=32, shuffle=False)\ntest_loader  = make_loader(test_dataset,  batch_size=32, shuffle=False)\n\n# -------------------------------------------------------\n# üî• Attention wrapper (temperature scaling)\n# -------------------------------------------------------\nfrom timm.models.vision_transformer import Attention\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv        = base_attn.qkv\n        self.proj       = base_attn.proj\n        self.proj_drop  = base_attn.proj_drop\n        self.attn_drop  = base_attn.attn_drop\n        self.num_heads  = base_attn.num_heads\n        self.q_norm     = getattr(base_attn, 'q_norm', None)\n        self.k_norm     = getattr(base_attn, 'k_norm', None)\n        self.scale      = base_attn.scale\n        self.temperature= float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n        attn_logits = (q @ k.transpose(-2,-1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad(): self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n        attn = attn_logits.softmax(dim=-1); attn = self.attn_drop(attn)\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\ndef apply_temperature_to_vit(model, T=1.5):\n    for _, blk in enumerate(model.blocks):\n        blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\n# -------------------------------------------------------\n# ‚úÖ Post-attention LayerNorm (safe block wrapper)\n# -------------------------------------------------------\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1, self.ls2 = getattr(blk,\"ls1\",None), getattr(blk,\"ls2\",None)\n        self.gamma_1, self.gamma_2 = getattr(blk,\"gamma_1\",None), getattr(blk,\"gamma_2\",None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, gamma): return ls(x) if ls is not None else (gamma*x if gamma is not None else x)\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1)\n        x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2)\n        x = x + self._dp(y, self.dp2)\n        return x\n\ndef add_post_attn_layernorm_safe(model, eps=1e-6):\n    device = next(model.parameters()).device\n    new_blocks = [PostAttnLNBlock(blk, eps=eps).to(device) for blk in model.blocks]\n    model.blocks = nn.Sequential(*new_blocks)\n\n# -------------------------------------------------------\n# üß† Build model + patches\n# -------------------------------------------------------\nnum_classes = len(classes)\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes).to(device)\nATTN_T = 1.5\napply_temperature_to_vit(model, T=ATTN_T)\nadd_post_attn_layernorm_safe(model, eps=1e-6)\n\nLOGIT_CLIP = 0.0\n\n# -------------------------------------------------------\n# üìè FLOPs / Params helper (with safe fallback)\n# -------------------------------------------------------\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef flops_and_params(model, input_res=(3,224,224)):\n    # Try ptflops, fall back to \"N/A\" FLOPs if it fails\n    try:\n        from ptflops import get_model_complexity_info\n        model_eval = copy.deepcopy(model).to('cpu').eval()\n        with torch.no_grad():\n            macs, params = get_model_complexity_info(model_eval, input_res, as_strings=False, verbose=False)\n        # ptflops returns MACs (multiply-adds). FLOPs ‚âà 2*MACs for convs; for transformers it's common to report MACs.\n        return macs, count_params(model)\n    except Exception as e:\n        return None, count_params(model)\n\n# -------------------------------------------------------\n# üß™ Training / Eval helpers (with LIGHT validation)\n# -------------------------------------------------------\ndef forward_with_optional_logit_clip(model, images):\n    logits = model(images)\n    if LOGIT_CLIP and LOGIT_CLIP > 0:\n        logits = torch.clamp(logits, min=-LOGIT_CLIP, max=LOGIT_CLIP)\n    return logits\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\n@torch.no_grad()\ndef report_attention_stats(model, loader, max_batches=1, tag=\"\"):\n    model.eval()\n    batches, entropies, max_abs_logits = 0, [], []\n    for images, _ in loader:\n        images = images.to(device)\n        with autocast('cuda'): _ = model(images)\n        blk_ents, blk_max = [], []\n        for blk in model.blocks:\n            attn = blk.attn\n            if isinstance(attn, TempScaledAttention) and attn.last_entropy is not None:\n                blk_ents.append(attn.last_entropy.numpy()); blk_max.append(attn.last_max_abs_pre_softmax)\n            else:\n                blk_ents.append(None); blk_max.append(None)\n        entropies.append(blk_ents); max_abs_logits.append(blk_max)\n        batches += 1\n        if batches >= max_batches: break\n    print(f\"\\nüîé Attention saturation ({tag}) ‚Äî T={ATTN_T}\")\n    for i, _ in enumerate(model.blocks):\n        head_arrays = [e[i] for e in entropies if e[i] is not None]\n        max_vals    = [m[i] for m in max_abs_logits if m[i] is not None]\n        if head_arrays:\n            H = np.stack(head_arrays, axis=0)\n            mean_ent, p10, p90 = H.mean(), np.percentile(H,10), np.percentile(H,90)\n            mx = np.mean(max_vals) if len(max_vals)>0 else float('nan')\n            print(f\"  Block {i:02d}: entropy mean={mean_ent:.3f}, p10={p10:.3f}, p90={p90:.3f} | max|pre-softmax|‚âà{mx:.3f}\")\n        else:\n            print(f\"  Block {i:02d}: (no stats)\")\n\ndef make_optimizer(model, lr=3e-4, weight_decay=0.01):\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\ndef make_scheduler(optimizer, t_initial=10):\n    return CosineLRScheduler(optimizer, t_initial=t_initial, lr_min=1e-6, warmup_lr_init=1e-5, warmup_t=3)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ndef eval_light(model, loader, max_batches=VAL_MAX_BATCHES):\n    model.eval()\n    total_loss, correct, total, batches = 0.0, 0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n                loss   = criterion(logits, labels)\n            total_loss += loss.item()\n            correct    += (logits.argmax(1) == labels).sum().item()\n            total      += labels.size(0)\n            batches    += 1\n            if batches >= max_batches: break\n    avg_loss = total_loss / max(1, batches)\n    acc = 100.0 * correct / max(1, total)\n    return avg_loss, acc\n\ndef evaluate_full(model, loader, class_names):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n            y_true += labels.cpu().tolist(); y_pred += preds.cpu().tolist()\n    acc = 100.0 * correct / max(1,total)\n    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    return acc, report, (pr, rc, f1)\n\n# -----------------------------\n# üõë Early stopping helper\n# -----------------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience; self.min_delta = min_delta\n        self.best = float('inf'); self.count = 0\n    def step(self, val_loss):\n        if val_loss < self.best - self.min_delta:\n            self.best = val_loss; self.count = 0; return False\n        else:\n            self.count += 1; return self.count > self.patience\n\n# -----------------------------\n# üîç Tiny pilot hyperparam tuning (LR/WD)\n# -----------------------------\ndef make_subset(dataset, frac):\n    n = len(dataset); k = max(1, int(n*frac))\n    idx = np.random.RandomState(42).choice(n, size=k, replace=False)\n    return Subset(dataset, idx.tolist())\n\ndef pilot_tune(base_model, train_ds, val_loader, candidates, epochs=TUNE_EPOCHS):\n    print(\"\\nüß™ Tiny pilot tuning (subset) ...\")\n    best_cfg, best_score, best_loss = None, -1, float('inf')\n    for (lr, wd) in candidates:\n        model = copy.deepcopy(base_model).to(device)\n        opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        scaler = GradScaler('cuda')\n        loader = make_loader(make_subset(train_ds, TUNE_TRAIN_FRAC), batch_size=32, shuffle=True)\n        for ep in range(epochs):\n            model.train()\n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                opt.zero_grad(set_to_none=True)\n                with autocast('cuda'):\n                    logits = forward_with_optional_logit_clip(model, images)\n                    loss   = criterion(logits, labels)\n                scaler.scale(loss).backward()\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(opt); scaler.update()\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=TUNE_VAL_MAX_BATCH)\n        print(f\"  LR={lr:.1e}, WD={wd:.3f} -> light ValAcc={val_acc:.2f}%, ValLoss={val_loss:.4f}\")\n        if (val_acc > best_score) or (abs(val_acc - best_score) < 1e-6 and val_loss < best_loss):\n            best_score, best_loss = val_acc, val_loss\n            best_cfg = (lr, wd)\n    print(f\"‚úÖ Chosen hyperparams: LR={best_cfg[0]:.1e}, WD={best_cfg[1]:.3f} (best light ValAcc={best_score:.2f}%)\")\n    return best_cfg\n\n# -------------------------------------------------------\n# üöÄ Train with LIGHT validation + Early Stopping\n# -------------------------------------------------------\ncandidates = [\n    (5e-5, 0.005), (5e-5, 0.01), (1e-4, 0.005), (1e-4, 0.01),\n    (2e-4, 0.005), (2e-4, 0.01), (3e-4, 0.01), (3e-4, 0.02),\n    (1e-4, 0.02), (3e-4, 0.05)\n]\n\nif ENABLE_TUNING:\n    chosen_lr, chosen_wd = pilot_tune(model, train_dataset, val_loader, candidates)\nelse:\n    chosen_lr, chosen_wd = 3e-4, 0.01\nprint(f\"\\n‚öôÔ∏è Using LR={chosen_lr:.1e}, WD={chosen_wd:.3f}\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=10)\nscaler    = GradScaler('cuda')\n\n# --- FLOPs/Params BEFORE pruning\nmacs_before, params_before = flops_and_params(model, input_res=(3,224,224))\nif macs_before is not None:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: {macs_before/1e9:.2f} G, Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: N/A, Params: {params_before/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\nEPOCHS = 10\nearly = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\nbest_state, best_val_acc = None, -1\n\n# Track training time and peak memory\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\ntrain_t0 = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    train_loss = total_loss / max(1,len(train_loader))\n    train_acc_epoch  = 100.0 * correct / max(1,total)\n    scheduler.step(epoch)\n\n    if (epoch+1) % VAL_EVERY_N_EPOCHS == 0:\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}% | \"\n              f\"Light Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n        report_attention_stats(model, val_loader, max_batches=1, tag=f\"epoch {epoch+1}\")\n\n        improve = (val_acc > best_val_acc + 1e-6) or (abs(val_acc - best_val_acc) < 1e-6 and (early.best - val_loss) > MIN_DELTA)\n        if improve:\n            best_val_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n\n        if early.step(val_loss):\n            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}. Best light ValAcc={best_val_acc:.2f}%\")\n            break\n    else:\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}%\")\n\ntrain_elapsed = time.time() - train_t0\ntrain_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Training\")\n\n# Restore best model (model selection)\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(\"üìå Restored best checkpoint by validation performance (light).\")\n\n# -------------------------------------------------------\n# ‚úÖ Full evaluation on TRAIN and TEST before pruning\n# -------------------------------------------------------\ndef measure_inference_performance(model, loader, tag=\"\"):\n    model.eval()\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\n    t0 = time.time()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n    elapsed = time.time() - t0\n    peak = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\n    acc = 100.0 * correct / max(1,total)\n    print(f\"\\U0001f9e0 {tag} Inference - Acc: {acc:.2f}%, Time: {elapsed:.2f}s, Peak Memory: {peak:.2f} MB\")\n    return acc, elapsed, peak\n\n# Training set metrics (accuracy only quick), plus full metrics\ntrain_acc_full, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\ntest_acc_before, test_report_before, test_prf_before = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ BEFORE Pruning: Test Acc: {test_acc_before:.2f}%\")\nprint(test_report_before)\n_, test_time_before, test_peak_mem_before = measure_inference_performance(model, test_loader, tag=\"Before Pruning\")\n\n# --- Print consolidated BEFORE metrics\nprint(\"\\nüìä BEFORE Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_full:.2f}%\")\nprint(f\"  Test  Accuracy: {test_acc_before:.2f}%\")\nprint(f\"  Test  Macro Precision: {test_prf_before[0]*100:.2f}% | Recall: {test_prf_before[1]*100:.2f}% | F1: {test_prf_before[2]*100:.2f}%\")\nprint(f\"  Training Time: {train_elapsed:.2f}s | Training Peak Memory: {train_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_before:.2f}s | Testing  Peak Memory: {test_peak_mem_before:.2f} MB\")\nif macs_before is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_before/1e9:.2f} G | Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_before/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Block-importance pruning + brief fine-tune\n#   üîÅ PRUNE 6 LOW-IMPORTANCE BLOCKS (keep 6)\n# -------------------------------------------------------\ndef compute_block_importance(model, loader):\n    model.train()\n    scores = torch.zeros(len(model.blocks), device=device)\n    # single-batch approximation\n    images, labels = next(iter(loader))\n    images, labels = images.to(device), labels.to(device)\n    with autocast('cuda'):\n        logits = forward_with_optional_logit_clip(model, images)\n        loss   = criterion(logits, labels)\n    loss.backward()\n    for i, blk in enumerate(model.blocks):\n        s, c = 0.0, 0\n        for p in blk.parameters():\n            if p.grad is not None:\n                s += p.grad.abs().mean(); c += 1\n        if c>0: scores[i] = s/c\n    model.zero_grad(set_to_none=True)\n    return scores\n\ndef prune_transformer_blocks(model, indices_to_remove):\n    model.blocks = nn.Sequential(*[blk for i, blk in enumerate(model.blocks) if i not in indices_to_remove])\n\nscores = compute_block_importance(model, val_loader)\n# ‚¨áÔ∏è Select 6 least-important blocks\nprune_indices = torch.argsort(scores)[:6].tolist()\nprint(f\"\\n‚úÇÔ∏è Pruned Blocks (6): {prune_indices}\")\nprune_transformer_blocks(model, prune_indices)\n\n# FLOPs/params AFTER pruning\nmacs_after, params_after = flops_and_params(model, input_res=(3,224,224))\nif macs_after is not None:\n    print(f\"üìê AFTER pruning ‚Äî MACs: {macs_after/1e9:.2f} G, Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"üìê AFTER pruning ‚Äî MACs: N/A, Params: {params_after/1e6:.2f} M\")\n\nprint(\"\\nüîß Fine-tuning for 2 epochs after pruning (with light val checks)...\")\nprint_memory(\"Before Fine-tuning\")\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=2)\nscaler    = GradScaler('cuda')\n\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\nft_t0 = time.time()\n\nfor epoch in range(2):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n    scheduler.step(epoch)\n    vloss, vacc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n    print(f\"[Fine-tune] Epoch {epoch+1}/2 - TrainLoss {total_loss/len(train_loader):.4f}, \"\n          f\"TrainAcc {100.0*correct/max(1,total):.2f}% | Light ValLoss {vloss:.4f}, ValAcc {vacc:.2f}%\")\n\nft_elapsed = time.time() - ft_t0\nft_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Fine-tuning\")\n\n# -------------------------------------------------------\n# ‚úÖ Final TRAIN/TEST after pruning\n# -------------------------------------------------------\ntrain_acc_after, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\npruned_test_acc, pruned_report, pruned_prf = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ AFTER Pruning: Test Acc: {pruned_test_acc:.2f}%\")\nprint(pruned_report)\n_, test_time_after, test_peak_mem_after = measure_inference_performance(model, test_loader, tag=\"After Pruning\")\n\nprint(\"\\nüìä AFTER Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_after:.2f}%\")\nprint(f\"  Test  Accuracy: {pruned_test_acc:.2f}%\")\nprint(f\"  Test  Macro Precision: {pruned_prf[0]*100:.2f}% | Recall: {pruned_prf[1]*100:.2f}% | F1: {pruned_prf[2]*100:.2f}%\")\nprint(f\"  Fine-tune Time: {ft_elapsed:.2f}s | Fine-tune Peak Memory: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_after:.2f}s | Testing  Peak Memory: {test_peak_mem_after:.2f} MB\")\nif macs_after is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_after/1e9:.2f} G | Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_after/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# üíæ Save\n# -------------------------------------------------------\nprint(\"\\nüíæ Saving pruned model ...\")\nsave_path = \"/kaggle/working/pruned_vit_6blocks.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"   -> {save_path}\")\n\n# -------------------------------------------------------\n# üßæ Final Overall Summary\n# -------------------------------------------------------\nprint(\"\\n================ FINAL SUMMARY ================\")\nprint(f\"Blocks pruned: 6 (kept {len(model.blocks)} blocks)\")\nif (macs_before is not None) and (macs_after is not None):\n    macs_delta = (macs_before - macs_after)/1e9\n    print(f\"MACs change: {macs_before/1e9:.2f} G -> {macs_after/1e9:.2f} G (Œî {macs_delta:.2f} G)\")\nelse:\n    print(\"MACs change: N/A\")\nprint(f\"Params change: {params_before/1e6:.2f} M -> {params_after/1e6:.2f} M (Œî {(params_before-params_after)/1e6:.2f} M)\")\nprint(f\"Train time: {train_elapsed:.2f}s | Train peak mem: {train_peak_mem_mb:.2f} MB\")\nprint(f\"Fine-tune time: {ft_elapsed:.2f}s | Fine-tune peak mem: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"Test(before) acc: {test_acc_before:.2f}% | Test(after) acc: {pruned_test_acc:.2f}%\")\nprint(\"===============================================\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" FLOPS =2*MAcs","metadata":{}},{"cell_type":"markdown","source":"# 8-blocks pruned model vs original ","metadata":{}},{"cell_type":"code","source":"# 8-blocks vs orignial \n# ============================================================\n# ViT (timm) ‚Äî Stability + Monitoring + Pruning + LIGHT VALIDATION\n# - 70/15/15 stratified split from all images (ignores folder splits)\n# - Temperature-scaled attention (T=1.5)\n# - Post-attention LayerNorm (safe wrapper)\n# - Label smoothing, AMP, gradient clipping\n# - Attention entropy monitoring\n# - ‚úÖ LIGHT VALIDATION: quick val passes (few batches) every N epochs\n# - ‚úÖ EARLY STOPPING: patience/min_delta, restore best checkpoint\n# - ‚úÖ LIGHT HYPERPARAM TUNING: 5 epochs per candidate (tiny subset)\n# - ‚úÇÔ∏è Prune 4 blocks (keep 8), brief fine-tune\n# - üìä Prints: train/test acc, precision/recall/F1, params, FLOPs, memory, times\n# ============================================================\n\n!pip install -q timm scikit-learn ptflops\n\nimport os, random, time, gc, math, copy\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.amp import GradScaler, autocast\n\n# -----------------------------\n# üîß Reproducibility\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# ‚öôÔ∏è Validation / Tuning knobs\n# -----------------------------\nVAL_EVERY_N_EPOCHS = 1         # run lightweight val every N epochs\nVAL_MAX_BATCHES     = 6        # validate on at most this many batches (keep small)\nPATIENCE            = 3        # early stopping patience (epochs)\nMIN_DELTA           = 1e-3     # minimum improvement in val loss to reset patience\n\nENABLE_TUNING       = True     # pilot sweep to pick LR/WD quickly\nTUNE_EPOCHS         = 5        # 5 epochs per candidate\nTUNE_TRAIN_FRAC     = 0.08     # use 8% of train for quick tuning\nTUNE_VAL_MAX_BATCH  = 3        # validate only a handful of batches in tuning\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Pruning target\n# -------------------------------------------------------\nNUM_PRUNE = 4  # ‚úÖ remove 4 blocks -> keep 8 blocks (ViT-Base has 12)\n\n# -------------------------------------------------------\n# üìÅ Data root ‚Äî your Kaggle dataset\n# -------------------------------------------------------\ndata_root = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\ntrain_dir = os.path.join(data_root, \"train\")\nval_dir   = os.path.join(data_root, \"val\")\ntest_dir  = os.path.join(data_root, \"test\")\n\n# -------------------------------------------------------\n# üñºÔ∏è Transforms\n# -------------------------------------------------------\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\nval_test_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\n# -------------------------------------------------------\n# üì¶ Build unified sample list then 70/15/15 split (stratified)\n# -------------------------------------------------------\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\ndef list_class_dirs(parent):\n    return sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])\n\nif os.path.exists(train_dir) and len(list_class_dirs(train_dir)) > 0:\n    classes = list_class_dirs(train_dir)\nelif os.path.exists(val_dir) and len(list_class_dirs(val_dir)) > 0:\n    classes = list_class_dirs(val_dir)\nelse:\n    classes = list_class_dirs(test_dir)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\ndef gather_samples(split_dir):\n    samples = []\n    for cls in classes:\n        cdir = os.path.join(split_dir, cls)\n        if not os.path.isdir(cdir): \n            continue\n        for root, _, files in os.walk(cdir):\n            for f in files:\n                if f.lower().endswith(IMG_EXTS):\n                    samples.append((os.path.join(root, f), class_to_idx[cls]))\n    return samples\n\nall_samples = []\nfor d in [train_dir, val_dir, test_dir]:\n    all_samples.extend(gather_samples(d))\nassert len(all_samples) > 0, \"No images found. Check dataset paths.\"\n\nlabels  = np.array([lbl for _, lbl in all_samples])\nindices = np.arange(len(all_samples))\n\n# 70% train, 30% temp\nsss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, test_size=0.30, random_state=42)\ntrain_idx, temp_idx = next(sss1.split(indices, labels))\n\n# temp -> 15% val, 15% test (split temp 50/50)\ntemp_labels = labels[temp_idx]\nsss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, test_size=0.50, random_state=42)\nval_rel_idx, test_rel_idx = next(sss2.split(temp_idx, temp_labels))\nval_idx, test_idx = temp_idx[val_rel_idx], temp_idx[test_rel_idx]\n\ndef take(idxs): \n    return [all_samples[i] for i in idxs]\n\ntrain_samples, val_samples, test_samples = take(train_idx), take(val_idx), take(test_idx)\n\nprint(f\"‚úÖ Split sizes ‚Äî Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\nprint(f\"    Classes ({len(classes)}): {classes}\")\n\n# -------------------------------------------------------\n# üß∞ Dataset from file paths\n# -------------------------------------------------------\nclass PathImageDataset(Dataset):\n    def __init__(self, samples, classes, transform=None):\n        self.samples = samples\n        self.classes = classes\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, target\n\ntrain_dataset = PathImageDataset(train_samples, classes, transform=train_transforms)\nval_dataset   = PathImageDataset(val_samples,   classes, transform=val_test_transforms)\ntest_dataset  = PathImageDataset(test_samples,  classes, transform=val_test_transforms)\n\ndef make_loader(dataset, batch_size=32, shuffle=False):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ntrain_loader = make_loader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = make_loader(val_dataset,   batch_size=32, shuffle=False)\ntest_loader  = make_loader(test_dataset,  batch_size=32, shuffle=False)\n\n# -------------------------------------------------------\n# üî• Attention wrapper (temperature scaling)\n# -------------------------------------------------------\nfrom timm.models.vision_transformer import Attention\n\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv        = base_attn.qkv\n        self.proj       = base_attn.proj\n        self.proj_drop  = base_attn.proj_drop\n        self.attn_drop  = base_attn.attn_drop\n        self.num_heads  = base_attn.num_heads\n        self.q_norm     = getattr(base_attn, 'q_norm', None)\n        self.k_norm     = getattr(base_attn, 'k_norm', None)\n        self.scale      = base_attn.scale\n        self.temperature= float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n\n        attn_logits = (q @ k.transpose(-2,-1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n\n        attn = attn_logits.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)   # (B, heads)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\ndef apply_temperature_to_vit(model, T=1.5):\n    for _, blk in enumerate(model.blocks):\n        blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\n# -------------------------------------------------------\n# ‚úÖ Post-attention LayerNorm (safe block wrapper)\n# -------------------------------------------------------\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp = blk.attn, blk.mlp\n\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None)\n            self.dp1, self.dp2 = dp, dp\n\n        self.ls1, self.ls2 = getattr(blk,\"ls1\",None), getattr(blk,\"ls2\",None)\n        self.gamma_1, self.gamma_2 = getattr(blk,\"gamma_1\",None), getattr(blk,\"gamma_2\",None)\n\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n\n    def _dp(self, x, dp):\n        return dp(x) if dp is not None else x\n\n    def _ls(self, x, ls, gamma):\n        return ls(x) if ls is not None else (gamma*x if gamma is not None else x)\n\n    def forward(self, x):\n        y = self.attn(self.norm1(x))\n        y = self._ls(y, self.ls1, self.gamma_1)\n        x = x + self._dp(y, self.dp1)\n\n        x = self.post_ln(x)\n\n        y = self.mlp(self.norm2(x))\n        y = self._ls(y, self.ls2, self.gamma_2)\n        x = x + self._dp(y, self.dp2)\n        return x\n\ndef add_post_attn_layernorm_safe(model, eps=1e-6):\n    dev = next(model.parameters()).device\n    new_blocks = [PostAttnLNBlock(blk, eps=eps).to(dev) for blk in model.blocks]\n    model.blocks = nn.Sequential(*new_blocks)\n\n# -------------------------------------------------------\n# üß† Build model + patches\n# -------------------------------------------------------\nnum_classes = len(classes)\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes).to(device)\n\nATTN_T = 1.5\napply_temperature_to_vit(model, T=ATTN_T)\nadd_post_attn_layernorm_safe(model, eps=1e-6)\n\nLOGIT_CLIP = 0.0\n\n# -------------------------------------------------------\n# üìè FLOPs / Params helper (with safe fallback)\n# -------------------------------------------------------\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef flops_and_params(model, input_res=(3,224,224)):\n    try:\n        from ptflops import get_model_complexity_info\n        model_eval = copy.deepcopy(model).to('cpu').eval()\n        with torch.no_grad():\n            macs, _ = get_model_complexity_info(model_eval, input_res, as_strings=False, verbose=False)\n        return macs, count_params(model)\n    except Exception:\n        return None, count_params(model)\n\n# -------------------------------------------------------\n# üß™ Training / Eval helpers (with LIGHT validation)\n# -------------------------------------------------------\ndef forward_with_optional_logit_clip(model, images):\n    logits = model(images)\n    if LOGIT_CLIP and LOGIT_CLIP > 0:\n        logits = torch.clamp(logits, min=-LOGIT_CLIP, max=LOGIT_CLIP)\n    return logits\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\n@torch.no_grad()\ndef report_attention_stats(model, loader, max_batches=1, tag=\"\"):\n    model.eval()\n    batches, entropies, max_abs_logits = 0, [], []\n    for images, _ in loader:\n        images = images.to(device)\n        with autocast('cuda'):\n            _ = model(images)\n\n        blk_ents, blk_max = [], []\n        for blk in model.blocks:\n            attn = blk.attn\n            if isinstance(attn, TempScaledAttention) and attn.last_entropy is not None:\n                blk_ents.append(attn.last_entropy.numpy())\n                blk_max.append(attn.last_max_abs_pre_softmax)\n            else:\n                blk_ents.append(None)\n                blk_max.append(None)\n\n        entropies.append(blk_ents)\n        max_abs_logits.append(blk_max)\n        batches += 1\n        if batches >= max_batches:\n            break\n\n    print(f\"\\nüîé Attention saturation ({tag}) ‚Äî T={ATTN_T}\")\n    for i, _ in enumerate(model.blocks):\n        head_arrays = [e[i] for e in entropies if e[i] is not None]\n        max_vals    = [m[i] for m in max_abs_logits if m[i] is not None]\n        if head_arrays:\n            H = np.stack(head_arrays, axis=0)\n            mean_ent, p10, p90 = H.mean(), np.percentile(H,10), np.percentile(H,90)\n            mx = np.mean(max_vals) if len(max_vals)>0 else float('nan')\n            print(f\"  Block {i:02d}: entropy mean={mean_ent:.3f}, p10={p10:.3f}, p90={p90:.3f} | max|pre-softmax|‚âà{mx:.3f}\")\n        else:\n            print(f\"  Block {i:02d}: (no stats)\")\n\ndef make_optimizer(model, lr=3e-4, weight_decay=0.01):\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\ndef make_scheduler(optimizer, t_initial=10):\n    return CosineLRScheduler(optimizer, t_initial=t_initial, lr_min=1e-6, warmup_lr_init=1e-5, warmup_t=3)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ndef eval_light(model, loader, max_batches=VAL_MAX_BATCHES):\n    model.eval()\n    total_loss, correct, total, batches = 0.0, 0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n                loss   = criterion(logits, labels)\n            total_loss += loss.item()\n            correct    += (logits.argmax(1) == labels).sum().item()\n            total      += labels.size(0)\n            batches    += 1\n            if batches >= max_batches:\n                break\n    avg_loss = total_loss / max(1, batches)\n    acc = 100.0 * correct / max(1, total)\n    return avg_loss, acc\n\ndef evaluate_full(model, loader, class_names):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n            y_true += labels.cpu().tolist()\n            y_pred += preds.cpu().tolist()\n\n    acc = 100.0 * correct / max(1, total)\n    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    return acc, report, (pr, rc, f1)\n\n# -----------------------------\n# üõë Early stopping helper\n# -----------------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best = float('inf')\n        self.count = 0\n\n    def step(self, val_loss):\n        if val_loss < self.best - self.min_delta:\n            self.best = val_loss\n            self.count = 0\n            return False\n        else:\n            self.count += 1\n            return self.count > self.patience\n\n# -----------------------------\n# üîç Tiny pilot hyperparam tuning (LR/WD)\n# -----------------------------\ndef make_subset(dataset, frac):\n    n = len(dataset)\n    k = max(1, int(n * frac))\n    idx = np.random.RandomState(42).choice(n, size=k, replace=False)\n    return Subset(dataset, idx.tolist())\n\ndef pilot_tune(base_model, train_ds, val_loader, candidates, epochs=TUNE_EPOCHS):\n    print(\"\\nüß™ Tiny pilot tuning (subset) ...\")\n    best_cfg, best_score, best_loss = None, -1, float('inf')\n\n    for (lr, wd) in candidates:\n        m = copy.deepcopy(base_model).to(device)\n        opt = optim.AdamW(m.parameters(), lr=lr, weight_decay=wd)\n        sc  = GradScaler('cuda')\n        loader = make_loader(make_subset(train_ds, TUNE_TRAIN_FRAC), batch_size=32, shuffle=True)\n\n        for _ in range(epochs):\n            m.train()\n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                opt.zero_grad(set_to_none=True)\n                with autocast('cuda'):\n                    logits = forward_with_optional_logit_clip(m, images)\n                    loss   = criterion(logits, labels)\n                sc.scale(loss).backward()\n                sc.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n                sc.step(opt); sc.update()\n\n        val_loss, val_acc = eval_light(m, val_loader, max_batches=TUNE_VAL_MAX_BATCH)\n        print(f\"  LR={lr:.1e}, WD={wd:.3f} -> light ValAcc={val_acc:.2f}%, ValLoss={val_loss:.4f}\")\n\n        if (val_acc > best_score) or (abs(val_acc - best_score) < 1e-6 and val_loss < best_loss):\n            best_score, best_loss = val_acc, val_loss\n            best_cfg = (lr, wd)\n\n    print(f\"‚úÖ Chosen hyperparams: LR={best_cfg[0]:.1e}, WD={best_cfg[1]:.3f} (best light ValAcc={best_score:.2f}%)\")\n    return best_cfg\n\n# -------------------------------------------------------\n# üöÄ Train with LIGHT validation + Early Stopping\n# -------------------------------------------------------\ncandidates = [\n    (5e-5, 0.005), (5e-5, 0.01), (1e-4, 0.005), (1e-4, 0.01),\n    (2e-4, 0.005), (2e-4, 0.01), (3e-4, 0.01), (3e-4, 0.02),\n    (1e-4, 0.02), (3e-4, 0.05)\n]\n\nif ENABLE_TUNING:\n    chosen_lr, chosen_wd = pilot_tune(model, train_dataset, val_loader, candidates)\nelse:\n    chosen_lr, chosen_wd = 3e-4, 0.01\n\nprint(f\"\\n‚öôÔ∏è Using LR={chosen_lr:.1e}, WD={chosen_wd:.3f}\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=10)\nscaler    = GradScaler('cuda')\n\n# --- FLOPs/Params BEFORE pruning\nmacs_before, params_before = flops_and_params(model, input_res=(3,224,224))\nif macs_before is not None:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: {macs_before/1e9:.2f} G, Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: N/A, Params: {params_before/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\nEPOCHS = 10\nearly = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\nbest_state, best_val_acc = None, -1\n\n# Track training time and peak memory\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats()\n    torch.cuda.synchronize()\n\ntrain_t0 = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    train_loss = total_loss / max(1, len(train_loader))\n    train_acc_epoch = 100.0 * correct / max(1, total)\n    scheduler.step(epoch)\n\n    if (epoch + 1) % VAL_EVERY_N_EPOCHS == 0:\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}% | \"\n              f\"Light Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n\n        report_attention_stats(model, val_loader, max_batches=1, tag=f\"epoch {epoch+1}\")\n\n        improve = (val_acc > best_val_acc + 1e-6) or (abs(val_acc - best_val_acc) < 1e-6 and (early.best - val_loss) > MIN_DELTA)\n        if improve:\n            best_val_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n\n        if early.step(val_loss):\n            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}. Best light ValAcc={best_val_acc:.2f}%\")\n            break\n    else:\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}%\")\n\ntrain_elapsed = time.time() - train_t0\ntrain_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Training\")\n\n# Restore best model (model selection)\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(\"üìå Restored best checkpoint by validation performance (light).\")\n\n# -------------------------------------------------------\n# ‚úÖ Full evaluation on TRAIN and TEST before pruning\n# -------------------------------------------------------\ndef measure_inference_performance(model, loader, tag=\"\"):\n    model.eval()\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n\n    t0 = time.time()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n\n    elapsed = time.time() - t0\n    peak = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\n    acc = 100.0 * correct / max(1, total)\n    print(f\"\\U0001f9e0 {tag} Inference - Acc: {acc:.2f}%, Time: {elapsed:.2f}s, Peak Memory: {peak:.2f} MB\")\n    return acc, elapsed, peak\n\ntrain_acc_full, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\ntest_acc_before, test_report_before, test_prf_before = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ BEFORE Pruning: Test Acc: {test_acc_before:.2f}%\")\nprint(test_report_before)\n_, test_time_before, test_peak_mem_before = measure_inference_performance(model, test_loader, tag=\"Before Pruning\")\n\nprint(\"\\nüìä BEFORE Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_full:.2f}%\")\nprint(f\"  Test  Accuracy: {test_acc_before:.2f}%\")\nprint(f\"  Test  Macro Precision: {test_prf_before[0]*100:.2f}% | Recall: {test_prf_before[1]*100:.2f}% | F1: {test_prf_before[2]*100:.2f}%\")\nprint(f\"  Training Time: {train_elapsed:.2f}s | Training Peak Memory: {train_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_before:.2f}s | Testing  Peak Memory: {test_peak_mem_before:.2f} MB\")\nif macs_before is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_before/1e9:.2f} G | Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_before/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Block-importance pruning + brief fine-tune\n#   ‚úÖ PRUNE 4 LOW-IMPORTANCE BLOCKS (keep 8)\n# -------------------------------------------------------\ndef compute_block_importance(model, loader):\n    model.train()\n    scores = torch.zeros(len(model.blocks), device=device)\n\n    images, labels = next(iter(loader))\n    images, labels = images.to(device), labels.to(device)\n\n    with autocast('cuda'):\n        logits = forward_with_optional_logit_clip(model, images)\n        loss   = criterion(logits, labels)\n\n    loss.backward()\n\n    for i, blk in enumerate(model.blocks):\n        s, c = 0.0, 0\n        for p in blk.parameters():\n            if p.grad is not None:\n                s += p.grad.abs().mean()\n                c += 1\n        if c > 0:\n            scores[i] = s / c\n\n    model.zero_grad(set_to_none=True)\n    return scores\n\ndef prune_transformer_blocks(model, indices_to_remove):\n    model.blocks = nn.Sequential(*[\n        blk for i, blk in enumerate(model.blocks) if i not in indices_to_remove\n    ])\n\nscores = compute_block_importance(model, val_loader)\n\n# ‚úÖ Remove 4 blocks (keep 8)\nprune_indices = torch.argsort(scores)[:NUM_PRUNE].tolist()\nprint(f\"\\n‚úÇÔ∏è Pruned Blocks ({NUM_PRUNE}) -> keeping {12-NUM_PRUNE}: {prune_indices}\")\nprune_transformer_blocks(model, prune_indices)\n\n# FLOPs/params AFTER pruning\nmacs_after, params_after = flops_and_params(model, input_res=(3,224,224))\nif macs_after is not None:\n    print(f\"üìê AFTER pruning ‚Äî MACs: {macs_after/1e9:.2f} G, Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"üìê AFTER pruning ‚Äî MACs: N/A, Params: {params_after/1e6:.2f} M\")\n\nprint(\"\\nüîß Fine-tuning for 2 epochs after pruning (with light val checks)...\")\nprint_memory(\"Before Fine-tuning\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=2)\nscaler    = GradScaler('cuda')\n\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats()\n    torch.cuda.synchronize()\n\nft_t0 = time.time()\n\nfor epoch in range(2):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    scheduler.step(epoch)\n    vloss, vacc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n    print(f\"[Fine-tune] Epoch {epoch+1}/2 - TrainLoss {total_loss/max(1,len(train_loader)):.4f}, \"\n          f\"TrainAcc {100.0*correct/max(1,total):.2f}% | Light ValLoss {vloss:.4f}, ValAcc {vacc:.2f}%\")\n\nft_elapsed = time.time() - ft_t0\nft_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Fine-tuning\")\n\n# -------------------------------------------------------\n# ‚úÖ Final TRAIN/TEST after pruning\n# -------------------------------------------------------\ntrain_acc_after, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\npruned_test_acc, pruned_report, pruned_prf = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ AFTER Pruning: Test Acc: {pruned_test_acc:.2f}%\")\nprint(pruned_report)\n_, test_time_after, test_peak_mem_after = measure_inference_performance(model, test_loader, tag=\"After Pruning\")\n\nprint(\"\\nüìä AFTER Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_after:.2f}%\")\nprint(f\"  Test  Accuracy: {pruned_test_acc:.2f}%\")\nprint(f\"  Test  Macro Precision: {pruned_prf[0]*100:.2f}% | Recall: {pruned_prf[1]*100:.2f}% | F1: {pruned_prf[2]*100:.2f}%\")\nprint(f\"  Fine-tune Time: {ft_elapsed:.2f}s | Fine-tune Peak Memory: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_after:.2f}s | Testing  Peak Memory: {test_peak_mem_after:.2f} MB\")\nif macs_after is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_after/1e9:.2f} G | Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_after/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# üíæ Save\n# -------------------------------------------------------\nprint(\"\\nüíæ Saving pruned model ...\")\nsave_path = \"/kaggle/working/pruned_vit_8blocks.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"   -> {save_path}\")\n\n# -------------------------------------------------------\n# üßæ Final Overall Summary\n# -------------------------------------------------------\nprint(\"\\n================ FINAL SUMMARY ================\")\nprint(f\"Blocks pruned: {NUM_PRUNE} (kept {len(model.blocks)} blocks)\")\nif (macs_before is not None) and (macs_after is not None):\n    macs_delta = (macs_before - macs_after)/1e9\n    print(f\"MACs change: {macs_before/1e9:.2f} G -> {macs_after/1e9:.2f} G (Œî {macs_delta:.2f} G)\")\nelse:\n    print(\"MACs change: N/A\")\nprint(f\"Params change: {params_before/1e6:.2f} M -> {params_after/1e6:.2f} M (Œî {(params_before-params_after)/1e6:.2f} M)\")\nprint(f\"Train time: {train_elapsed:.2f}s | Train peak mem: {train_peak_mem_mb:.2f} MB\")\nprint(f\"Fine-tune time: {ft_elapsed:.2f}s | Fine-tune peak mem: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"Test(before) acc: {test_acc_before:.2f}% | Test(after) acc: {pruned_test_acc:.2f}%\")\nprint(\"===============================================\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4-blocks pruned","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ViT (timm) ‚Äî Stability + Monitoring + Pruning + LIGHT VALIDATION\n# - 70/15/15 stratified split from all images (ignores folder splits)\n# - Temperature-scaled attention (T=1.5)\n# - Post-attention LayerNorm (safe wrapper)\n# - Label smoothing, AMP, gradient clipping\n# - Attention entropy monitoring\n# - ‚úÖ LIGHT VALIDATION: quick val passes (few batches) every N epochs\n# - ‚úÖ EARLY STOPPING: patience/min_delta, restore best checkpoint\n# - ‚úÖ LIGHT HYPERPARAM TUNING: 5 epochs per candidate (tiny subset)\n# - ‚úÇÔ∏è Prune 8 blocks (keep 4), brief fine-tune\n# - üìä Prints: train/test acc, precision/recall/F1, params, FLOPs, memory, times\n# ============================================================\n\n!pip install -q timm scikit-learn ptflops\n\nimport os, random, time, gc, math, copy\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.amp import GradScaler, autocast\n\n# -----------------------------\n# üîß Reproducibility\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# ‚öôÔ∏è Validation / Tuning knobs\n# -----------------------------\nVAL_EVERY_N_EPOCHS = 1         # run lightweight val every N epochs\nVAL_MAX_BATCHES     = 6        # validate on at most this many batches (keep small)\nPATIENCE            = 3        # early stopping patience (epochs)\nMIN_DELTA           = 1e-3     # minimum improvement in val loss to reset patience\n\nENABLE_TUNING       = True     # pilot sweep to pick LR/WD quickly\nTUNE_EPOCHS         = 5        # 5 epochs per candidate\nTUNE_TRAIN_FRAC     = 0.08     # use 8% of train for quick tuning\nTUNE_VAL_MAX_BATCH  = 3        # validate only a handful of batches in tuning\n\n# -------------------------------------------------------\n# üìÅ Data root ‚Äî your Kaggle dataset\n# -------------------------------------------------------\ndata_root = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\ntrain_dir = os.path.join(data_root, \"train\")\nval_dir   = os.path.join(data_root, \"val\")\ntest_dir  = os.path.join(data_root, \"test\")\n\n# -------------------------------------------------------\n# üñºÔ∏è Transforms\n# -------------------------------------------------------\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\nval_test_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\n# -------------------------------------------------------\n# üì¶ Build unified sample list then 70/15/15 split (stratified)\n# -------------------------------------------------------\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\ndef list_class_dirs(parent): return sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])\n\nif os.path.exists(train_dir) and len(list_class_dirs(train_dir)) > 0:\n    classes = list_class_dirs(train_dir)\nelif os.path.exists(val_dir) and len(list_class_dirs(val_dir)) > 0:\n    classes = list_class_dirs(val_dir)\nelse:\n    classes = list_class_dirs(test_dir)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\ndef gather_samples(split_dir):\n    samples = []\n    for cls in classes:\n        cdir = os.path.join(split_dir, cls)\n        if not os.path.isdir(cdir): continue\n        for root, _, files in os.walk(cdir):\n            for f in files:\n                if f.lower().endswith(IMG_EXTS):\n                    samples.append((os.path.join(root, f), class_to_idx[cls]))\n    return samples\n\nall_samples = []\nfor d in [train_dir, val_dir, test_dir]:\n    all_samples.extend(gather_samples(d))\nassert len(all_samples) > 0, \"No images found. Check dataset paths.\"\n\nlabels  = np.array([lbl for _, lbl in all_samples])\nindices = np.arange(len(all_samples))\n\nsss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, test_size=0.30, random_state=42)\ntrain_idx, temp_idx = next(sss1.split(indices, labels))\ntemp_labels = labels[temp_idx]\nsss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, test_size=0.50, random_state=42)\nval_rel_idx, test_rel_idx = next(sss2.split(temp_idx, temp_labels))\nval_idx, test_idx = temp_idx[val_rel_idx], temp_idx[test_rel_idx]\n\ndef take(idxs): return [all_samples[i] for i in idxs]\ntrain_samples, val_samples, test_samples = take(train_idx), take(val_idx), take(test_idx)\n\nprint(f\"‚úÖ Split sizes ‚Äî Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\nprint(f\"    Classes ({len(classes)}): {classes}\")\n\n# -------------------------------------------------------\n# üß∞ Dataset from file paths\n# -------------------------------------------------------\nclass PathImageDataset(Dataset):\n    def __init__(self, samples, classes, transform=None):\n        self.samples = samples; self.classes = classes; self.transform = transform\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img, target\n\ntrain_dataset = PathImageDataset(train_samples, classes, transform=train_transforms)\nval_dataset   = PathImageDataset(val_samples,   classes, transform=val_test_transforms)\ntest_dataset  = PathImageDataset(test_samples,  classes, transform=val_test_transforms)\n\ndef make_loader(dataset, batch_size=32, shuffle=False):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ntrain_loader = make_loader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = make_loader(val_dataset,   batch_size=32, shuffle=False)\ntest_loader  = make_loader(test_dataset,  batch_size=32, shuffle=False)\n\n# -------------------------------------------------------\n# üî• Attention wrapper (temperature scaling)\n# -------------------------------------------------------\nfrom timm.models.vision_transformer import Attention\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv        = base_attn.qkv\n        self.proj       = base_attn.proj\n        self.proj_drop  = base_attn.proj_drop\n        self.attn_drop  = base_attn.attn_drop\n        self.num_heads  = base_attn.num_heads\n        self.q_norm     = getattr(base_attn, 'q_norm', None)\n        self.k_norm     = getattr(base_attn, 'k_norm', None)\n        self.scale      = base_attn.scale\n        self.temperature= float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n        attn_logits = (q @ k.transpose(-2,-1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad(): self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n        attn = attn_logits.softmax(dim=-1); attn = self.attn_drop(attn)\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\ndef apply_temperature_to_vit(model, T=1.5):\n    for _, blk in enumerate(model.blocks):\n        blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\n# -------------------------------------------------------\n# ‚úÖ Post-attention LayerNorm (safe block wrapper)\n# -------------------------------------------------------\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1, self.ls2 = getattr(blk,\"ls1\",None), getattr(blk,\"ls2\",None)\n        self.gamma_1, self.gamma_2 = getattr(blk,\"gamma_1\",None), getattr(blk,\"gamma_2\",None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, gamma): return ls(x) if ls is not None else (gamma*x if gamma is not None else x)\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1)\n        x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2)\n        x = x + self._dp(y, self.dp2)\n        return x\n\ndef add_post_attn_layernorm_safe(model, eps=1e-6):\n    device = next(model.parameters()).device\n    new_blocks = [PostAttnLNBlock(blk, eps=eps).to(device) for blk in model.blocks]\n    model.blocks = nn.Sequential(*new_blocks)\n\n# -------------------------------------------------------\n# üß† Build model + patches\n# -------------------------------------------------------\nnum_classes = len(classes)\nmodel = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes).to(device)\nATTN_T = 1.5\napply_temperature_to_vit(model, T=ATTN_T)\nadd_post_attn_layernorm_safe(model, eps=1e-6)\n\nLOGIT_CLIP = 0.0\n\n# -------------------------------------------------------\n# üìè FLOPs / Params helper (with safe fallback)\n# -------------------------------------------------------\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef flops_and_params(model, input_res=(3,224,224)):\n    # Try ptflops, fall back to \"N/A\" FLOPs if it fails\n    try:\n        from ptflops import get_model_complexity_info\n        model_eval = copy.deepcopy(model).to('cpu').eval()\n        with torch.no_grad():\n            macs, params = get_model_complexity_info(model_eval, input_res, as_strings=False, verbose=False)\n        # ptflops returns MACs (multiply-adds). FLOPs ‚âà 2*MACs for convs; for transformers it's common to report MACs.\n        return macs, count_params(model)\n    except Exception as e:\n        return None, count_params(model)\n\n# -------------------------------------------------------\n# üß™ Training / Eval helpers (with LIGHT validation)\n# -------------------------------------------------------\ndef forward_with_optional_logit_clip(model, images):\n    logits = model(images)\n    if LOGIT_CLIP and LOGIT_CLIP > 0:\n        logits = torch.clamp(logits, min=-LOGIT_CLIP, max=LOGIT_CLIP)\n    return logits\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\n@torch.no_grad()\ndef report_attention_stats(model, loader, max_batches=1, tag=\"\"):\n    model.eval()\n    batches, entropies, max_abs_logits = 0, [], []\n    for images, _ in loader:\n        images = images.to(device)\n        with autocast('cuda'): _ = model(images)\n        blk_ents, blk_max = [], []\n        for blk in model.blocks:\n            attn = blk.attn\n            if isinstance(attn, TempScaledAttention) and attn.last_entropy is not None:\n                blk_ents.append(attn.last_entropy.numpy()); blk_max.append(attn.last_max_abs_pre_softmax)\n            else:\n                blk_ents.append(None); blk_max.append(None)\n        entropies.append(blk_ents); max_abs_logits.append(blk_max)\n        batches += 1\n        if batches >= max_batches: break\n    print(f\"\\nüîé Attention saturation ({tag}) ‚Äî T={ATTN_T}\")\n    for i, _ in enumerate(model.blocks):\n        head_arrays = [e[i] for e in entropies if e[i] is not None]\n        max_vals    = [m[i] for m in max_abs_logits if m[i] is not None]\n        if head_arrays:\n            H = np.stack(head_arrays, axis=0)\n            mean_ent, p10, p90 = H.mean(), np.percentile(H,10), np.percentile(H,90)\n            mx = np.mean(max_vals) if len(max_vals)>0 else float('nan')\n            print(f\"  Block {i:02d}: entropy mean={mean_ent:.3f}, p10={p10:.3f}, p90={p90:.3f} | max|pre-softmax|‚âà{mx:.3f}\")\n        else:\n            print(f\"  Block {i:02d}: (no stats)\")\n\ndef make_optimizer(model, lr=3e-4, weight_decay=0.01):\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\ndef make_scheduler(optimizer, t_initial=10):\n    return CosineLRScheduler(optimizer, t_initial=t_initial, lr_min=1e-6, warmup_lr_init=1e-5, warmup_t=3)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ndef eval_light(model, loader, max_batches=VAL_MAX_BATCHES):\n    model.eval()\n    total_loss, correct, total, batches = 0.0, 0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n                loss   = criterion(logits, labels)\n            total_loss += loss.item()\n            correct    += (logits.argmax(1) == labels).sum().item()\n            total      += labels.size(0)\n            batches    += 1\n            if batches >= max_batches: break\n    avg_loss = total_loss / max(1, batches)\n    acc = 100.0 * correct / max(1, total)\n    return avg_loss, acc\n\ndef evaluate_full(model, loader, class_names):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n            y_true += labels.cpu().tolist(); y_pred += preds.cpu().tolist()\n    acc = 100.0 * correct / max(1,total)\n    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    return acc, report, (pr, rc, f1)\n\n# -----------------------------\n# üõë Early stopping helper\n# -----------------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience; self.min_delta = min_delta\n        self.best = float('inf'); self.count = 0\n    def step(self, val_loss):\n        if val_loss < self.best - self.min_delta:\n            self.best = val_loss; self.count = 0; return False\n        else:\n            self.count += 1; return self.count > self.patience\n\n# -----------------------------\n# üîç Tiny pilot hyperparam tuning (LR/WD)\n# -----------------------------\ndef make_subset(dataset, frac):\n    n = len(dataset); k = max(1, int(n*frac))\n    idx = np.random.RandomState(42).choice(n, size=k, replace=False)\n    return Subset(dataset, idx.tolist())\n\ndef pilot_tune(base_model, train_ds, val_loader, candidates, epochs=TUNE_EPOCHS):\n    print(\"\\nüß™ Tiny pilot tuning (subset) ...\")\n    best_cfg, best_score, best_loss = None, -1, float('inf')\n    for (lr, wd) in candidates:\n        model = copy.deepcopy(base_model).to(device)\n        opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        scaler = GradScaler('cuda')\n        loader = make_loader(make_subset(train_ds, TUNE_TRAIN_FRAC), batch_size=32, shuffle=True)\n        for ep in range(epochs):\n            model.train()\n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                opt.zero_grad(set_to_none=True)\n                with autocast('cuda'):\n                    logits = forward_with_optional_logit_clip(model, images)\n                    loss   = criterion(logits, labels)\n                scaler.scale(loss).backward()\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(opt); scaler.update()\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=TUNE_VAL_MAX_BATCH)\n        print(f\"  LR={lr:.1e}, WD={wd:.3f} -> light ValAcc={val_acc:.2f}%, ValLoss={val_loss:.4f}\")\n        if (val_acc > best_score) or (abs(val_acc - best_score) < 1e-6 and val_loss < best_loss):\n            best_score, best_loss = val_acc, val_loss\n            best_cfg = (lr, wd)\n    print(f\"‚úÖ Chosen hyperparams: LR={best_cfg[0]:.1e}, WD={best_cfg[1]:.3f} (best light ValAcc={best_score:.2f}%)\")\n    return best_cfg\n\n# -------------------------------------------------------\n# üöÄ Train with LIGHT validation + Early Stopping\n# -------------------------------------------------------\ncandidates = [\n    (5e-5, 0.005), (5e-5, 0.01), (1e-4, 0.005), (1e-4, 0.01),\n    (2e-4, 0.005), (2e-4, 0.01), (3e-4, 0.01), (3e-4, 0.02),\n    (1e-4, 0.02), (3e-4, 0.05)\n]\n\nif ENABLE_TUNING:\n    chosen_lr, chosen_wd = pilot_tune(model, train_dataset, val_loader, candidates)\nelse:\n    chosen_lr, chosen_wd = 3e-4, 0.01\nprint(f\"\\n‚öôÔ∏è Using LR={chosen_lr:.1e}, WD={chosen_wd:.3f}\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=10)\nscaler    = GradScaler('cuda')\n\n# --- FLOPs/Params BEFORE pruning\nmacs_before, params_before = flops_and_params(model, input_res=(3,224,224))\nif macs_before is not None:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: {macs_before/1e9:.2f} G, Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: N/A, Params: {params_before/1e6:.2f} M\")\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\nprint_memory(\"Before Training\")\nEPOCHS = 10\nearly = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\nbest_state, best_val_acc = None, -1\n\n# Track training time and peak memory\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\ntrain_t0 = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    train_loss = total_loss / max(1,len(train_loader))\n    train_acc_epoch  = 100.0 * correct / max(1,total)\n    scheduler.step(epoch)\n\n    if (epoch+1) % VAL_EVERY_N_EPOCHS == 0:\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}% | \"\n              f\"Light Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n        report_attention_stats(model, val_loader, max_batches=1, tag=f\"epoch {epoch+1}\")\n\n        improve = (val_acc > best_val_acc + 1e-6) or (abs(val_acc - best_val_acc) < 1e-6 and (early.best - val_loss) > MIN_DELTA)\n        if improve:\n            best_val_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n\n        if early.step(val_loss):\n            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}. Best light ValAcc={best_val_acc:.2f}%\")\n            break\n    else:\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}%\")\n\ntrain_elapsed = time.time() - train_t0\ntrain_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Training\")\n\n# Restore best model (model selection)\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(\"üìå Restored best checkpoint by validation performance (light).\")\n\n# -------------------------------------------------------\n# ‚úÖ Full evaluation on TRAIN and TEST before pruning\n# -------------------------------------------------------\ndef measure_inference_performance(model, loader, tag=\"\"):\n    model.eval()\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\n    t0 = time.time()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n    elapsed = time.time() - t0\n    peak = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\n    acc = 100.0 * correct / max(1,total)\n    print(f\"\\U0001f9e0 {tag} Inference - Acc: {acc:.2f}%, Time: {elapsed:.2f}s, Peak Memory: {peak:.2f} MB\")\n    return acc, elapsed, peak\n\n# Training set metrics (accuracy only quick), plus full metrics\ntrain_acc_full, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\ntest_acc_before, test_report_before, test_prf_before = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ BEFORE Pruning: Test Acc: {test_acc_before:.2f}%\")\nprint(test_report_before)\n_, test_time_before, test_peak_mem_before = measure_inference_performance(model, test_loader, tag=\"Before Pruning\")\n\n# --- Print consolidated BEFORE metrics\nprint(\"\\nüìä BEFORE Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_full:.2f}%\")\nprint(f\"  Test  Accuracy: {test_acc_before:.2f}%\")\nprint(f\"  Test  Macro Precision: {test_prf_before[0]*100:.2f}% | Recall: {test_prf_before[1]*100:.2f}% | F1: {test_prf_before[2]*100:.2f}%\")\nprint(f\"  Training Time: {train_elapsed:.2f}s | Training Peak Memory: {train_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_before:.2f}s | Testing  Peak Memory: {test_peak_mem_before:.2f} MB\")\nif macs_before is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_before/1e9:.2f} G | Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_before/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Block-importance pruning + brief fine-tune\n#   üîÅ PRUNE 8 LOW-IMPORTANCE BLOCKS (keep 4)\n# -------------------------------------------------------\ndef compute_block_importance(model, loader):\n    model.train()\n    scores = torch.zeros(len(model.blocks), device=device)\n    # single-batch approximation\n    images, labels = next(iter(loader))\n    images, labels = images.to(device), labels.to(device)\n    with autocast('cuda'):\n        logits = forward_with_optional_logit_clip(model, images)\n        loss   = criterion(logits, labels)\n    loss.backward()\n    for i, blk in enumerate(model.blocks):\n        s, c = 0.0, 0\n        for p in blk.parameters():\n            if p.grad is not None:\n                s += p.grad.abs().mean(); c += 1\n        if c>0: scores[i] = s/c\n    model.zero_grad(set_to_none=True)\n    return scores\n\ndef prune_transformer_blocks(model, indices_to_remove):\n    model.blocks = nn.Sequential(*[blk for i, blk in enumerate(model.blocks) if i not in indices_to_remove])\n\nscores = compute_block_importance(model, val_loader)\n# ‚¨áÔ∏è Select 8 least-important blocks\nPRUNE_REMOVE = 8\nprune_indices = torch.argsort(scores)[:PRUNE_REMOVE].tolist()\nprint(f\"\\n‚úÇÔ∏è Pruned Blocks ({PRUNE_REMOVE}): {prune_indices}\")\nprune_transformer_blocks(model, prune_indices)\n\n# FLOPs/params AFTER pruning\nmacs_after, params_after = flops_and_params(model, input_res=(3,224,224))\nif macs_after is not None:\n    print(f\"üìê AFTER pruning ‚Äî MACs: {macs_after/1e9:.2f} G, Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"üìê AFTER pruning ‚Äî MACs: N/A, Params: {params_after/1e6:.2f} M\")\n\nprint(\"\\nüîß Fine-tuning for 2 epochs after pruning (with light val checks)...\")\nprint_memory(\"Before Fine-tuning\")\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=2)\nscaler    = GradScaler('cuda')\n\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\nft_t0 = time.time()\n\nfor epoch in range(2):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n    scheduler.step(epoch)\n    vloss, vacc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n    print(f\"[Fine-tune] Epoch {epoch+1}/2 - TrainLoss {total_loss/len(train_loader):.4f}, \"\n          f\"TrainAcc {100.0*correct/max(1,total):.2f}% | Light ValLoss {vloss:.4f}, ValAcc {vacc:.2f}%\")\n\nft_elapsed = time.time() - ft_t0\nft_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Fine-tuning\")\n\n# -------------------------------------------------------\n# ‚úÖ Final TRAIN/TEST after pruning\n# -------------------------------------------------------\ntrain_acc_after, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\npruned_test_acc, pruned_report, pruned_prf = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ AFTER Pruning: Test Acc: {pruned_test_acc:.2f}%\")\nprint(pruned_report)\n_, test_time_after, test_peak_mem_after = measure_inference_performance(model, test_loader, tag=\"After Pruning\")\n\nprint(\"\\nüìä AFTER Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_after:.2f}%\")\nprint(f\"  Test  Accuracy: {pruned_test_acc:.2f}%\")\nprint(f\"  Test  Macro Precision: {pruned_prf[0]*100:.2f}% | Recall: {pruned_prf[1]*100:.2f}% | F1: {pruned_prf[2]*100:.2f}%\")\nprint(f\"  Fine-tune Time: {ft_elapsed:.2f}s | Fine-tune Peak Memory: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_after:.2f}s | Testing  Peak Memory: {test_peak_mem_after:.2f} MB\")\nif macs_after is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_after/1e9:.2f} G | Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_after/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# üíæ Save (both architecture and weights)\n# -------------------------------------------------------\nprint(\"\\nüíæ Saving model architecture and weights ...\")\narch_path = \"/kaggle/working/model_architecture.pth\"\nweights_path = \"/kaggle/working/weights.pth\"\ntorch.save(model, arch_path)                 # full model object (architecture)\ntorch.save(model.state_dict(), weights_path) # weights only\nprint(f\"   -> Architecture: {arch_path}\")\nprint(f\"   -> Weights:      {weights_path}\")\n\n# -------------------------------------------------------\n# üßæ Final Overall Summary\n# -------------------------------------------------------\nprint(\"\\n================ FINAL SUMMARY ================\")\nkept_blocks = len(model.blocks)\nprint(f\"Blocks pruned: {PRUNE_REMOVE} (kept {kept_blocks} blocks)\")\nif (macs_before is not None) and (macs_after is not None):\n    macs_delta = (macs_before - macs_after)/1e9\n    print(f\"MACs change: {macs_before/1e9:.2f} G -> {macs_after/1e9:.2f} G (Œî {macs_delta:.2f} G)\")\nelse:\n    print(\"MACs change: N/A\")\nprint(f\"Params change: {params_before/1e6:.2f} M -> {params_after/1e6:.2f} M (Œî {(params_before-params_after)/1e6:.2f} M)\")\nprint(f\"Train time: {train_elapsed:.2f}s | Train peak mem: {train_peak_mem_mb:.2f} MB\")\nprint(f\"Fine-tune time: {ft_elapsed:.2f}s | Fine-tune peak mem: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"Test(before) acc: {test_acc_before:.2f}% | Test(after) acc: {pruned_test_acc:.2f}%\")\nprint(f\"Saved: {arch_path} and {weights_path}\")\nprint(\"===============================================\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6-block adversarial training pipeline Vs adversarial training attacks all hyperparameters for experiement 2 and 4 + calibration ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ViT (pruned to N blocks auto-detected from weights) ‚Äî PGD+BIM Mixed Adv Training\n# + 1,000-img VAL from 'val/' + Early Stopping + Full Metrics\n# Paths are customized for your 6-block model & dataset\n# ============================================================\n\n!pip install -q timm fvcore ptflops\n\nimport os, time, gc, random, json, math, collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import accuracy_score\n\nfrom timm import create_model\nfrom timm.models.vision_transformer import VisionTransformer, Attention\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.format import Format\nfrom torch.nn import Identity, Conv2d\nfrom torch.serialization import add_safe_globals\n\nfrom fvcore.nn import FlopCountAnalysis\nfrom ptflops import get_model_complexity_info\n\n# --------------------- Config ---------------------\nNUM_CLASSES   = 8\nIMG_SIZE      = 224\nATTN_T        = 1.5\nPOST_LN_EPS   = 1e-6\nBATCH_SIZE    = 32\nNUM_WORKERS   = min(8, os.cpu_count())\nMAX_GRAD_NORM = 1.0\n\nDATA_DIR      = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\nVAL_DIR_NAME  = \"val\"  # your folder is 'val', not 'validation'\n\nTS_MODEL_PATH = \"/kaggle/input/6blocksprunednoadv/pytorch/default/1/after_pruning_model_ts.pt\"\nWEIGHTS_PATH  = \"/kaggle/input/6blocksweights/pytorch/default/1/after_pruning_weights.pth\"\n\n# Adversarial params\nEPSILON    = 8/255\nALPHA      = 2/255\nPGD_STEPS  = 7\nBIM_STEPS  = 10\n\n# --- epsilon list for evaluation sweeps ---\nEPS_LIST = [\n    2.0/255.0,   # light noise\n    4.0/255.0,   # moderate\n    8.0/255.0,   # strong\n    16.0/255.0,  # extreme\n]\n\n# Train config\nEPOCHS      = 6\nLR          = 1e-4\nPATIENCE    = 2\nMIN_DELTA   = 0.0\n\n# --------------------- Utils ---------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\ndef print_memory(tag=\"\"):\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserv = torch.cuda.memory_reserved() / 1024**2\n        print(f\"üß† {tag} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\ndef _sync():\n    if torch.cuda.is_available(): torch.cuda.synchronize()\n\nadd_safe_globals([VisionTransformer, PatchEmbed, Conv2d, Identity, Format, nn.Dropout])\n\n# --------------------- Anti-saturation blocks ---------------------\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv = base_attn.qkv\n        self.proj = base_attn.proj\n        self.proj_drop = base_attn.proj_drop\n        self.attn_drop = base_attn.attn_drop\n        self.num_heads = base_attn.num_heads\n        self.q_norm = getattr(base_attn, 'q_norm', None)\n        self.k_norm = getattr(base_attn, 'k_norm', None)\n        self.scale = base_attn.scale\n        self.temperature = float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n\n        attn_logits = (q @ k.transpose(-2, -1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n\n        attn = attn_logits.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp   = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1 = getattr(blk, \"ls1\", None); self.ls2 = getattr(blk, \"ls2\", None)\n        self.gamma_1 = getattr(blk, \"gamma_1\", None); self.gamma_2 = getattr(blk, \"gamma_2\", None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, g): return ls(x) if ls is not None else (g * x if g is not None else x)\n\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1); x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2); x = x + self._dp(y, self.dp2)\n        return x\n\ndef apply_temperature_to_vit(v: VisionTransformer, T=1.5):\n    for blk in v.blocks: blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\ndef add_post_attn_layernorm_safe(v: VisionTransformer, eps=1e-6):\n    dev = next(v.parameters()).device\n    v.blocks = nn.Sequential(*[PostAttnLNBlock(blk, eps=eps).to(dev) for blk in v.blocks])\n\ndef prune_vit_to_n_blocks(v: VisionTransformer, n_keep: int):\n    v.blocks = nn.Sequential(*[blk for i, blk in enumerate(v.blocks) if i < n_keep])\n\n# --------------------- Model loading ---------------------\ndef detect_num_blocks_from_state(state_dict):\n    # Find largest \"blocks.X.\" index in state_dict\n    max_idx = -1\n    for k in state_dict.keys():\n        if k.startswith(\"blocks.\") and k[7:].split('.',1)[0].isdigit():\n            idx = int(k[7:].split('.',1)[0]); max_idx = max(max_idx, idx)\n    return max_idx + 1 if max_idx >= 0 else None  # count (0..max_idx)\n\ndef build_model_from_weights(weights_path, num_classes=NUM_CLASSES):\n    print(f\"üì¶ Loading weights from: {weights_path}\")\n    sd = torch.load(weights_path, map_location=\"cpu\")\n    if \"state_dict\" in sd and isinstance(sd[\"state_dict\"], dict): sd = sd[\"state_dict\"]\n\n    n_blocks = detect_num_blocks_from_state(sd)\n    if n_blocks is None:\n        raise RuntimeError(\"Could not detect number of blocks from state_dict keys.\")\n\n    print(f\"üîß Detected pruned block count in weights: {n_blocks}\")\n\n    m = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=num_classes)\n    apply_temperature_to_vit(m, T=ATTN_T)\n    add_post_attn_layernorm_safe(m, eps=POST_LN_EPS)\n\n    # TIMM ViT has 12 blocks by default ‚Üí prune to n_blocks\n    prune_vit_to_n_blocks(m, n_blocks)\n\n    missing, unexpected = m.load_state_dict(sd, strict=False)\n    print(f\"‚úÖ Weights loaded with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n    if missing[:5]: print(\"‚ö†Ô∏è Missing (first 5):\", missing[:5])\n\n    return m, n_blocks\n\ndef try_load_full_model():\n    # Prefer reconstruct-from-weights (patchable & trainable), else TorchScript fallback\n    try:\n        m, n = build_model_from_weights(WEIGHTS_PATH, num_classes=NUM_CLASSES)\n        print(f\"‚úÖ Reconstructed TIMM model with {n} blocks\")\n        return m, n, True  # (model, n_blocks, patchable=True)\n    except Exception as e:\n        print(f\"‚ùå Reconstruct-from-weights failed: {e}\\nüîÑ Falling back to TorchScript: {TS_MODEL_PATH}\")\n        ts = torch.jit.load(TS_MODEL_PATH, map_location=\"cpu\")\n        # Best-effort probe\n        n = None\n        try:\n            n = len(list(ts.blocks))  # may work for scripted timm\n        except Exception:\n            pass\n        if n is not None:\n            print(f\"‚úÖ TorchScript model loaded; reported blocks: {n}\")\n        else:\n            print(\"‚úÖ TorchScript model loaded; blocks count not introspectable.\")\n        return ts, n, False  # patchable=False\n\n# --------------------- Normalization wrapper ---------------------\nclass NormalizeLayer(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1,3,1,1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1,3,1,1))\n    def forward(self, x): return (x - self.mean) / self.std\n\nclass ModelWithNorm(nn.Module):\n    def __init__(self, base_model, mean, std, logit_clip=0.0):\n        super().__init__()\n        self.norm = NormalizeLayer(mean, std)\n        self.base = base_model\n        self.logit_clip = float(logit_clip)\n    def forward(self, x):\n        x = self.norm(x); logits = self.base(x)\n        if self.logit_clip > 0.0:\n            logits = torch.clamp(logits, min=-self.logit_clip, max=self.logit_clip)\n        return logits\n\n# --------------------- Data ---------------------\ntfm = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=tfm)\nval_full      = datasets.ImageFolder(os.path.join(DATA_DIR, VAL_DIR_NAME), transform=tfm)\ntest_dataset  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"),  transform=tfm)\n\ndef stratified_subset_indices(dataset, num_classes=NUM_CLASSES, total=1000, seed=42):\n    per_class = total // num_classes\n    cls2idxs = {c: [] for c in range(num_classes)}\n    for i, (_, c) in enumerate(dataset.samples): cls2idxs[c].append(i)\n    rng = np.random.default_rng(seed)\n    chosen = []\n    for c in range(num_classes):\n        idxs = cls2idxs[c]; rng.shuffle(idxs); chosen.extend(idxs[:per_class])\n    return chosen\n\nVAL_COUNT = 1000\nval_indices = stratified_subset_indices(val_full, total=VAL_COUNT)\nval_dataset = Subset(val_full, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n\nprint(f\"üì¶ Datasets ‚Üí train:{len(train_dataset)} | val(full):{len(val_full)} | val(subset used):{len(val_dataset)} | test:{len(test_dataset)}\")\n\n# --------------------- Attacks ---------------------\ndef fgsm_attack(images, labels, model, epsilon=EPSILON):\n    images = images.clone().detach().to(device).requires_grad_(True)\n    labels = labels.to(device)\n    with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n    grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n    return (images + epsilon * grad.sign()).clamp(0,1).detach()\n\ndef pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=PGD_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); ori = images.clone()\n    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n    images = (ori + delta).clamp(0,1).detach()\n    for _ in range(steps):\n        images.requires_grad_(True)\n        with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n        images = images.detach() + alpha * grad.sign()\n        images = torch.max(torch.min(images, ori + epsilon), ori - epsilon).clamp(0,1).detach()\n    model.train(was_training); return images\n\ndef bim_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=BIM_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); adv = images.clone()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, images + epsilon), images - epsilon).clamp(0,1)\n    model.train(was_training); return adv.detach()\n\ndef hybrid_fgsm_pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, refinement_steps=5):\n    was_training = model.training; model.eval()\n    base = images.detach().clone().to(device); labels = labels.to(device)\n    adv = fgsm_attack(base, labels, model, epsilon)\n    for _ in range(refinement_steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, base + epsilon), base - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\n# --------------------- Eval / Bench ---------------------\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n@torch.no_grad()\ndef evaluate(model, loader, attack=None, return_loss=False):\n    model.eval(); correct=0; total=0; run_loss=0.0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n        data = ims\n        if   attack==\"FGSM\":  torch.set_grad_enabled(True); data = fgsm_attack(ims, lbs, model); torch.set_grad_enabled(False)\n        elif attack==\"PGD\":   torch.set_grad_enabled(True); data = pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"BIM\":   torch.set_grad_enabled(True); data = bim_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"Hybrid_FGSM_PGD\":\n            torch.set_grad_enabled(True); data = hybrid_fgsm_pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        with autocast('cuda'):\n            logits = model(data); loss = F.cross_entropy(logits, lbs); preds = logits.argmax(1)\n        correct += (preds==lbs).sum().item(); total += lbs.size(0); run_loss += loss.item()\n    acc = correct / max(total,1)\n    return (acc, run_loss / max(len(loader),1)) if return_loss else acc\n\ndef get_model_complexity(vit_or_ts, img_size=IMG_SIZE):\n    # FLOPs/params for patchable (timm) models only; best-effort for TS if possible\n    try:\n        m = vit_or_ts.eval().to(device)\n        dummy = torch.randn(1,3,img_size,img_size, device=device)\n        _sync()\n        try:\n            flops = FlopCountAnalysis(m, dummy).total()\n        except Exception:\n            m_cpu = m.to(\"cpu\")\n            with torch.no_grad():\n                macs, _ = get_model_complexity_info(m_cpu, (3,img_size,img_size), as_strings=False, print_per_layer_stat=False)\n            flops = macs * 2\n            m = m.to(device)\n        params = sum(p.numel() for p in m.parameters())\n        return flops, params\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Complexity analysis failed: {e}\")\n        return None, None\n\n@torch.no_grad()\ndef benchmark_inference(model, loader, max_batches=20, warmup_batches=3):\n    model.eval(); total_imgs=0; total_time=0.0\n    # warmup\n    w=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        with autocast('cuda'): _ = model(ims)\n        w+=1\n        if w>=warmup_batches: break\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    _sync(); measured=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        _sync(); t0=time.time()\n        with autocast('cuda'): _ = model(ims)\n        _sync(); dt=time.time()-t0\n        total_time += dt; total_imgs += ims.size(0); measured+=1\n        if measured>=max_batches: break\n    lat_ms = (total_time/max(1,measured))*1000.0\n    thr    = total_imgs / max(total_time,1e-9)\n    peak   = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"batches\":measured,\"images\":total_imgs,\"avg_batch_latency_ms\":lat_ms,\"throughput_img_per_s\":thr,\"peak_infer_mem_mb\":peak}\n\n# ---------- helper to compute Avg Max Softmax Probability (clean/adversarial) ----------\ndef avg_maxp_over_loader(model, loader, attack=None, max_batches=None):\n    model.eval()\n    total_sum, total_cnt, seen = 0.0, 0, 0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        lbs = lbs.to(device)\n        data = ims\n        # enable grad only while CRAFTING adversarial examples\n        if attack == \"FGSM\":\n            with torch.enable_grad():\n                data = fgsm_attack(ims, lbs, model)\n        elif attack == \"PGD\":\n            with torch.enable_grad():\n                data = pgd_attack(model, ims, lbs)\n        elif attack == \"BIM\":\n            with torch.enable_grad():\n                data = bim_attack(model, ims, lbs)\n        elif attack == \"Hybrid_FGSM_PGD\":\n            with torch.enable_grad():\n                data = hybrid_fgsm_pgd_attack(model, ims, lbs)\n        # forward WITHOUT grad to measure probs\n        with torch.no_grad():\n            with autocast('cuda'):\n                logits = model(data)\n            probs = logits.softmax(dim=1).max(dim=1).values\n        total_sum += probs.sum().item()\n        total_cnt += probs.numel()\n        seen += 1\n        if max_batches is not None and seen >= max_batches:\n            break\n    return (total_sum / max(total_cnt, 1)) if total_cnt > 0 else float(\"nan\")\n\n# --- evaluation helper for arbitrary epsilons ---\ndef evaluate_with_eps(model, loader, attack, epsilon, alpha_scale=True):\n    \"\"\"\n    Evaluate accuracy on `loader` using a given attack and epsilon.\n    Keeps your original attacks; only passes custom eps (and scaled alpha).\n    \"\"\"\n    model.eval()\n    correct, total = 0, 0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        lbs = lbs.to(device)\n        data = ims\n\n        if attack is not None:\n            with torch.enable_grad():\n                if attack == \"FGSM\":\n                    data = fgsm_attack(ims, lbs, model, epsilon=epsilon)\n                elif attack == \"PGD\":\n                    alpha = ALPHA * (epsilon / EPSILON) if alpha_scale else ALPHA\n                    data  = pgd_attack(model, ims, lbs, epsilon=epsilon, alpha=alpha, steps=PGD_STEPS)\n                elif attack == \"BIM\":\n                    alpha = ALPHA * (epsilon / EPSILON) if alpha_scale else ALPHA\n                    data  = bim_attack(model, ims, lbs, epsilon=epsilon, alpha=alpha, steps=BIM_STEPS)\n                elif attack == \"Hybrid_FGSM_PGD\":\n                    alpha = ALPHA * (epsilon / EPSILON) if alpha_scale else ALPHA\n                    data  = hybrid_fgsm_pgd_attack(model, ims, lbs, epsilon=epsilon, alpha=alpha, refinement_steps=5)\n\n        with torch.no_grad():\n            with autocast('cuda'):\n                logits = model(data)\n            preds = logits.argmax(1)\n        correct += (preds == lbs).sum().item()\n        total   += lbs.size(0)\n    return correct / max(total, 1)\n\n# --- NEW: calibration metrics (ECE, MCE, Brier) ---\ndef calibration_metrics_over_loader(model, loader, attack=None, n_bins=15, max_batches=None):\n    \"\"\"\n    Compute ECE, MCE, and Brier score over a loader (clean or adversarial).\n    \"\"\"\n    model.eval()\n    all_confs = []\n    all_correct = []\n    all_probs = []\n    all_labels = []\n\n    seen = 0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        lbs = lbs.to(device)\n        data = ims\n\n        # craft adversarial examples if needed\n        if attack == \"FGSM\":\n            with torch.enable_grad():\n                data = fgsm_attack(ims, lbs, model)\n        elif attack == \"PGD\":\n            with torch.enable_grad():\n                data = pgd_attack(model, ims, lbs)\n        elif attack == \"BIM\":\n            with torch.enable_grad():\n                data = bim_attack(model, ims, lbs)\n        elif attack == \"Hybrid_FGSM_PGD\":\n            with torch.enable_grad():\n                data = hybrid_fgsm_pgd_attack(model, ims, lbs)\n\n        with torch.no_grad():\n            with autocast('cuda'):\n                logits = model(data)\n            probs = F.softmax(logits, dim=1)\n            confs, preds = probs.max(dim=1)\n            correct = (preds == lbs).float()\n\n        all_confs.append(confs.cpu())\n        all_correct.append(correct.cpu())\n        all_probs.append(probs.cpu())\n        all_labels.append(lbs.cpu())\n\n        seen += 1\n        if max_batches is not None and seen >= max_batches:\n            break\n\n    if len(all_confs) == 0:\n        return {\"ece\": float(\"nan\"), \"mce\": float(\"nan\"), \"brier\": float(\"nan\"),\n                \"avg_conf\": float(\"nan\"), \"acc\": float(\"nan\")}\n\n    confs = torch.cat(all_confs)\n    correct = torch.cat(all_correct)\n    probs = torch.cat(all_probs)\n    labels = torch.cat(all_labels)\n\n    N = confs.numel()\n    acc = correct.mean().item()\n    avg_conf = confs.mean().item()\n\n    # Brier score (multi-class)\n    num_classes = probs.size(1)\n    one_hot = F.one_hot(labels, num_classes=num_classes).float()\n    brier = ((probs - one_hot) ** 2).sum(dim=1).mean().item()\n\n    # ECE & MCE\n    bin_boundaries = torch.linspace(0.0, 1.0, steps=n_bins + 1)\n    ece = 0.0\n    mce = 0.0\n    for i in range(n_bins):\n        start = bin_boundaries[i].item()\n        end = bin_boundaries[i+1].item()\n        mask = (confs >= start) & (confs < end if i < n_bins-1 else confs <= end)\n        if mask.sum() == 0:\n            continue\n        bin_conf = confs[mask].mean().item()\n        bin_acc = correct[mask].mean().item()\n        gap = abs(bin_acc - bin_conf)\n        ece += (mask.float().mean().item()) * gap\n        mce = max(mce, gap)\n\n    return {\n        \"ece\": ece,\n        \"mce\": mce,\n        \"brier\": brier,\n        \"avg_conf\": avg_conf,\n        \"acc\": acc,\n    }\n\n# --------------------- Early stopping ---------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, restore_best=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best = restore_best\n        self.best = math.inf\n        self.bad = 0\n        self.best_state = None\n    def step(self, val_loss, model):\n        if (self.best - val_loss) > self.min_delta:\n            self.best = val_loss; self.bad=0\n            if self.restore_best:\n                self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n            return False\n        else:\n            self.bad+=1; return self.bad>self.patience\n    def restore(self, model):\n        if self.restore_best and self.best_state is not None: model.load_state_dict(self.best_state)\n\n# --------------------- Train loop ---------------------\ndef train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    model.train(); opt = torch.optim.AdamW(model.parameters(), lr=lr); scaler = GradScaler('cuda'); stopper = EarlyStopper()\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    hist=[]\n    for ep in range(1,epochs+1):\n        model.train(); run_loss=0.0; correct=0; total=0\n\n        # accumulators for Avg MaxP during TRAIN\n        sum_clean = 0.0; sum_pgd = 0.0; sum_bim = 0.0; sum_fgsm = 0.0\n        cnt_clean = 0;   cnt_pgd = 0;   cnt_bim = 0;   cnt_fgsm = 0\n\n        for bi,(ims,lbs) in enumerate(train_loader):\n            ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n            adv_pgd = pgd_attack(model, ims, lbs); adv_bim = bim_attack(model, ims, lbs)\n            mix_x = torch.cat([ims, adv_pgd, adv_bim], dim=0); mix_y = torch.cat([lbs, lbs, lbs], dim=0)\n            opt.zero_grad(set_to_none=True)\n            with autocast('cuda'): logits = model(mix_x); loss = loss_fn(logits, mix_y)\n            preds = logits.argmax(1); correct += (preds==mix_y).sum().item(); total += mix_y.size(0)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n            run_loss += loss.item()\n\n            # Avg MaxP for Clean/PGD/BIM from current batch logits\n            n = ims.size(0)\n            with torch.no_grad():\n                probs_clean = logits[:n].softmax(dim=1).max(dim=1).values\n                probs_pgd   = logits[n:2*n].softmax(dim=1).max(dim=1).values\n                probs_bim   = logits[2*n:3*n].softmax(dim=1).max(dim=1).values\n                sum_clean  += probs_clean.sum().item(); cnt_clean += probs_clean.numel()\n                sum_pgd    += probs_pgd.sum().item();   cnt_pgd   += probs_pgd.numel()\n                sum_bim    += probs_bim.sum().item();   cnt_bim   += probs_bim.numel()\n\n            # FGSM Avg MaxP metric (not used in loss; metric only)\n            with torch.enable_grad():\n                adv_fgsm   = fgsm_attack(ims, lbs, model)\n            with torch.no_grad():\n                logits_fgsm = model(adv_fgsm)\n                probs_fgsm  = logits_fgsm.softmax(dim=1).max(dim=1).values\n                sum_fgsm   += probs_fgsm.sum().item();  cnt_fgsm  += probs_fgsm.numel()\n\n            if bi%50==0: print(f\"Epoch {ep}/{epochs} | Batch {bi}/{len(train_loader)} | Loss {loss.item():.4f}\")\n\n        train_loss = run_loss/len(train_loader); train_acc = correct/max(total,1)\n        val_acc, val_loss = evaluate(model, val_loader, attack=None, return_loss=True)\n        print(f\"‚úÖ Epoch {ep}/{epochs} | TrainLoss {train_loss:.4f} | TrainAcc(mixed) {train_acc*100:.2f}% | ValLoss {val_loss:.4f} | ValAcc {val_acc*100:.2f}%\")\n\n        # Print TRAIN Avg MaxP for the epoch\n        avg_clean = sum_clean/max(cnt_clean,1)\n        avg_pgd   = sum_pgd  /max(cnt_pgd,1)\n        avg_bim   = sum_bim  /max(cnt_bim,1)\n        avg_fgsm  = sum_fgsm /max(cnt_fgsm,1)\n        print(f\"üîé TrainAvgMaxP (epoch {ep}) ‚Äî Clean {avg_clean:.3f} | FGSM {avg_fgsm:.3f} | PGD {avg_pgd:.3f} | BIM {avg_bim:.3f}\")\n\n        hist.append({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc_mixed\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc})\n        if stopper.step(val_loss, model):\n            print(f\"‚õî Early stopping at epoch {ep}\")\n            break\n    stopper.restore(model)\n    peak_train_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"train_peak_mem_mb\":peak_train_mb, \"history\":hist}\n\n# --------------------- Load model ---------------------\nprint(\"üß© Loading your pruned model...\")\nbase_model, detected_blocks, patchable = try_load_full_model()\nbase_model = base_model.to(device).eval()\nprint(f\"üìä Model ready on {device} | blocks: {detected_blocks if detected_blocks is not None else 'unknown (TS)'} | patchable={patchable}\")\n\n# Wrap with ImageNet normalization\nmodel = ModelWithNorm(base_model, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], logit_clip=0.0).to(device)\nmodel.to(memory_format=torch.channels_last).eval()\n\n# --------------------- FLOPs / Params ---------------------\nflops, params = get_model_complexity(base_model, IMG_SIZE)\nif flops is not None: print(f\"üßÆ Complexity: {flops/1e9:.3f} GFLOPs | Params: {params/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\n\n# --------------------- BEFORE training (TEST) ---------------------\nprint(\"\\nüîç BEFORE adversarial training (TEST set)...\")\nclean_b = evaluate(model, test_loader, attack=None)\nfgsm_b  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_b   = evaluate(model, test_loader, attack=\"PGD\")\nbim_b   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_b   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_b = benchmark_inference(model, test_loader, max_batches=20)\n\n# Avg MaxP BEFORE (TEST)\navgp_clean_b = avg_maxp_over_loader(model, test_loader, attack=None)\navgp_fgsm_b  = avg_maxp_over_loader(model, test_loader, attack=\"FGSM\")\navgp_pgd_b   = avg_maxp_over_loader(model, test_loader, attack=\"PGD\")\navgp_bim_b   = avg_maxp_over_loader(model, test_loader, attack=\"BIM\")\navgp_hyb_b   = avg_maxp_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\n\nprint(f\"üìä BEFORE (TEST): Clean {clean_b*100:.2f}% | FGSM {fgsm_b*100:.2f}% | PGD {pgd_b*100:.2f}% | BIM {bim_b*100:.2f}% | Hybrid {hyb_b*100:.2f}%\")\nprint(f\"üîé BEFORE (AvgMaxP):  Clean {avgp_clean_b:.3f} | FGSM {avgp_fgsm_b:.3f} | PGD {avgp_pgd_b:.3f} | BIM {avgp_bim_b:.3f} | Hybrid {avgp_hyb_b:.3f}\")\nprint(f\"‚ö° Inference BEFORE: latency {infer_b['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_b['throughput_img_per_s']:.2f} img/s | peak mem {infer_b['peak_infer_mem_mb']:.2f} MB\")\n\n# --- NEW: Calibration BEFORE (TEST) ---\nprint(\"\\nüìè BEFORE calibration metrics (TEST set)...\")\ncal_clean_b = calibration_metrics_over_loader(model, test_loader, attack=None)\ncal_fgsm_b  = calibration_metrics_over_loader(model, test_loader, attack=\"FGSM\")\ncal_pgd_b   = calibration_metrics_over_loader(model, test_loader, attack=\"PGD\")\ncal_bim_b   = calibration_metrics_over_loader(model, test_loader, attack=\"BIM\")\ncal_hyb_b   = calibration_metrics_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\n\ndef _print_cal(tag, cal):\n    print(f\"{tag}: Acc={cal['acc']*100:.2f}% | AvgConf={cal['avg_conf']:.3f} | \"\n          f\"ECE={cal['ece']:.4f} | MCE={cal['mce']:.4f} | Brier={cal['brier']:.4f}\")\n\n_print_cal(\"  Clean \", cal_clean_b)\n_print_cal(\"  FGSM  \", cal_fgsm_b)\n_print_cal(\"  PGD   \", cal_pgd_b)\n_print_cal(\"  BIM   \", cal_bim_b)\n_print_cal(\"  Hybrid\", cal_hyb_b)\n\n# --------------------- Train ---------------------\nprint(\"\\nüöÄ Training (PGD + BIM mixed) with validation & early stopping...\")\nt0 = time.time()\ntrain_metrics = train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\ntrain_time = time.time() - t0\nprint(f\"‚è±Ô∏è Training time: {train_time:.2f} s\")\nprint(f\"üß† Peak training memory: {train_metrics['train_peak_mem_mb']:.2f} MB\")\nprint_memory(\"After Training\")\n\n# --------------------- AFTER training ---------------------\ntrain_acc_after = evaluate(model, train_loader, attack=None)\nval_acc_after   = evaluate(model, val_loader,   attack=None)\nprint(f\"\\nüéØ AFTER Training ‚Üí Train(clean): {train_acc_after*100:.2f}% | Val(clean,1000): {val_acc_after*100:.2f}%\")\n\nprint(\"\\nüîç AFTER adversarial training (TEST set)...\")\nclean_a = evaluate(model, test_loader, attack=None)\nfgsm_a  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_a   = evaluate(model, test_loader, attack=\"PGD\")\nbim_a   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_a   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_a = benchmark_inference(model, test_loader, max_batches=20)\n\n# Avg MaxP AFTER (TEST)\navgp_clean_a = avg_maxp_over_loader(model, test_loader, attack=None)\navgp_fgsm_a  = avg_maxp_over_loader(model, test_loader, attack=\"FGSM\")\navgp_pgd_a   = avg_maxp_over_loader(model, test_loader, attack=\"PGD\")\navgp_bim_a   = avg_maxp_over_loader(model, test_loader, attack=\"BIM\")\navgp_hyb_a   = avg_maxp_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\n\nprint(f\"üìä AFTER (TEST): Clean {clean_a*100:.2f}% | FGSM {fgsm_a*100:.2f}% | PGD {pgd_a*100:.2f}% | BIM {bim_a*100:.2f}% | Hybrid {hyb_a*100:.2f}%\")\nprint(f\"üîé AFTER  (AvgMaxP): Clean {avgp_clean_a:.3f} | FGSM {avgp_fgsm_a:.3f} | PGD {avgp_pgd_a:.3f} | BIM {avgp_bim_a:.3f} | Hybrid {avgp_hyb_a:.3f}\")\nprint(f\"‚ö° Inference AFTER: latency {infer_a['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_a['throughput_img_per_s']:.2f} img/s | peak mem {infer_a['peak_infer_mem_mb']:.2f} MB\")\n\n# --- NEW: Calibration AFTER (TEST) ---\nprint(\"\\nüìè AFTER calibration metrics (TEST set)...\")\ncal_clean_a = calibration_metrics_over_loader(model, test_loader, attack=None)\ncal_fgsm_a  = calibration_metrics_over_loader(model, test_loader, attack=\"FGSM\")\ncal_pgd_a   = calibration_metrics_over_loader(model, test_loader, attack=\"PGD\")\ncal_bim_a   = calibration_metrics_over_loader(model, test_loader, attack=\"BIM\")\ncal_hyb_a   = calibration_metrics_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\n\n_print_cal(\"  Clean \", cal_clean_a)\n_print_cal(\"  FGSM  \", cal_fgsm_a)\n_print_cal(\"  PGD   \", cal_pgd_a)\n_print_cal(\"  BIM   \", cal_bim_a)\n_print_cal(\"  Hybrid\", cal_hyb_a)\n\n# --------- Epsilon sweep evaluation on TEST set ---------\nprint(\"\\nüìä Epsilon sweep on TEST set (FGSM, PGD, BIM, Hybrid):\")\nfor eps in EPS_LIST:\n    fg = evaluate_with_eps(model, test_loader, \"FGSM\", eps)\n    pg = evaluate_with_eps(model, test_loader, \"PGD\", eps)\n    bm = evaluate_with_eps(model, test_loader, \"BIM\", eps)\n    hy = evaluate_with_eps(model, test_loader, \"Hybrid_FGSM_PGD\", eps)\n    print(f\"  Œµ = {eps:.5f} ‚Üí FGSM {fg*100:.2f}% | PGD {pg*100:.2f}% | BIM {bm*100:.2f}% | Hybrid {hy*100:.2f}%\")\n\n# --------------------- Save ---------------------\ntorch.save(model, \"/kaggle/working/ViT_6blocks_adv_withVAL.pth\")\ntorch.save(model.state_dict(), \"/kaggle/working/ViT_6blocks_adv_withVAL_weights.pth\")\nprint(\"\\nüíæ Saved models to /kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# proposed model vs external dataset experiment 5","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ViT (pruned to N blocks auto-detected from weights) ‚Äî PGD+BIM Mixed Adv Training\n# + 1,000-img VAL from 'val/' + Early Stopping + Full Metrics\n# Paths are customized for your 6-block model & dataset\n# + Cross-dataset evaluation on UCSD 3-class test set\n# ============================================================\n\n!pip install -q timm fvcore ptflops\n\nimport os, time, gc, random, json, math, collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import accuracy_score\n\nfrom timm import create_model\nfrom timm.models.vision_transformer import VisionTransformer, Attention\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.format import Format\nfrom torch.nn import Identity, Conv2d\nfrom torch.serialization import add_safe_globals\n\nfrom fvcore.nn import FlopCountAnalysis\nfrom ptflops import get_model_complexity_info\n\n# --------------------- Config ---------------------\nNUM_CLASSES   = 8\nIMG_SIZE      = 224\nATTN_T        = 1.5\nPOST_LN_EPS   = 1e-6\nBATCH_SIZE    = 32\nNUM_WORKERS   = min(8, os.cpu_count())\nMAX_GRAD_NORM = 1.0\n\nDATA_DIR      = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\nVAL_DIR_NAME  = \"val\"  # your folder is 'val', not 'validation'\n\nTS_MODEL_PATH = \"/kaggle/input/6blocksprunednoadv/pytorch/default/1/after_pruning_model_ts.pt\"\nWEIGHTS_PATH  = \"/kaggle/input/6blocksweights/pytorch/default/1/after_pruning_weights.pth\"\n\n# Adversarial params\nEPSILON    = 8/255\nALPHA      = 2/255\nPGD_STEPS  = 7\nBIM_STEPS  = 10\n\n# Train config\nEPOCHS      = 6\nLR          = 1e-4\nPATIENCE    = 2\nMIN_DELTA   = 0.0\n\n# --------------------- Utils ---------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\ndef print_memory(tag=\"\"):\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserv = torch.cuda.memory_reserved() / 1024**2\n        print(f\"üß† {tag} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\ndef _sync():\n    if torch.cuda.is_available(): torch.cuda.synchronize()\n\nadd_safe_globals([VisionTransformer, PatchEmbed, Conv2d, Identity, Format, nn.Dropout])\n\n# --------------------- Anti-saturation blocks ---------------------\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv = base_attn.qkv\n        self.proj = base_attn.proj\n        self.proj_drop = base_attn.proj_drop\n        self.attn_drop = base_attn.attn_drop\n        self.num_heads = base_attn.num_heads\n        self.q_norm = getattr(base_attn, 'q_norm', None)\n        self.k_norm = getattr(base_attn, 'k_norm', None)\n        self.scale = base_attn.scale\n        self.temperature = float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n\n        attn_logits = (q @ k.transpose(-2, -1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n\n        attn = attn_logits.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp   = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1 = getattr(blk, \"ls1\", None); self.ls2 = getattr(blk, \"ls2\", None)\n        self.gamma_1 = getattr(blk, \"gamma_1\", None); self.gamma_2 = getattr(blk, \"gamma_2\", None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, g): return ls(x) if ls is not None else (g * x if g is not None else x)\n\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1); x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2); x = x + self._dp(y, self.dp2)\n        return x\n\ndef apply_temperature_to_vit(v: VisionTransformer, T=1.5):\n    for blk in v.blocks: blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\ndef add_post_attn_layernorm_safe(v: VisionTransformer, eps=1e-6):\n    dev = next(v.parameters()).device\n    v.blocks = nn.Sequential(*[PostAttnLNBlock(blk, eps=eps).to(dev) for blk in v.blocks])\n\ndef prune_vit_to_n_blocks(v: VisionTransformer, n_keep: int):\n    v.blocks = nn.Sequential(*[blk for i, blk in enumerate(v.blocks) if i < n_keep])\n\n# --------------------- Model loading ---------------------\ndef detect_num_blocks_from_state(state_dict):\n    # Find largest \"blocks.X.\" index in state_dict\n    max_idx = -1\n    for k in state_dict.keys():\n        if k.startswith(\"blocks.\") and k[7:].split('.',1)[0].isdigit():\n            idx = int(k[7:].split('.',1)[0]); max_idx = max(max_idx, idx)\n    return max_idx + 1 if max_idx >= 0 else None  # count (0..max_idx)\n\ndef build_model_from_weights(weights_path, num_classes=NUM_CLASSES):\n    print(f\"üì¶ Loading weights from: {weights_path}\")\n    sd = torch.load(weights_path, map_location=\"cpu\")\n    if \"state_dict\" in sd and isinstance(sd[\"state_dict\"], dict): sd = sd[\"state_dict\"]\n\n    n_blocks = detect_num_blocks_from_state(sd)\n    if n_blocks is None:\n        raise RuntimeError(\"Could not detect number of blocks from state_dict keys.\")\n\n    print(f\"üîß Detected pruned block count in weights: {n_blocks}\")\n\n    m = create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=num_classes)\n    apply_temperature_to_vit(m, T=ATTN_T)\n    add_post_attn_layernorm_safe(m, eps=POST_LN_EPS)\n\n    # TIMM ViT has 12 blocks by default ‚Üí prune to n_blocks\n    prune_vit_to_n_blocks(m, n_blocks)\n\n    missing, unexpected = m.load_state_dict(sd, strict=False)\n    print(f\"‚úÖ Weights loaded with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n    if missing[:5]: print(\"‚ö†Ô∏è Missing (first 5):\", missing[:5])\n\n    return m, n_blocks\n\ndef try_load_full_model():\n    # Prefer reconstruct-from-weights (patchable & trainable), else TorchScript fallback\n    try:\n        m, n = build_model_from_weights(WEIGHTS_PATH, num_classes=NUM_CLASSES)\n        print(f\"‚úÖ Reconstructed TIMM model with {n} blocks\")\n        return m, n, True  # (model, n_blocks, patchable=True)\n    except Exception as e:\n        print(f\"‚ùå Reconstruct-from-weights failed: {e}\\nüîÑ Falling back to TorchScript: {TS_MODEL_PATH}\")\n        ts = torch.jit.load(TS_MODEL_PATH, map_location=\"cpu\")\n        # Best-effort probe\n        n = None\n        try:\n            n = len(list(ts.blocks))  # may work for scripted timm\n        except Exception:\n            pass\n        if n is not None:\n            print(f\"‚úÖ TorchScript model loaded; reported blocks: {n}\")\n        else:\n            print(\"‚úÖ TorchScript model loaded; blocks count not introspectable.\")\n        return ts, n, False  # patchable=False\n\n# --------------------- Normalization wrapper ---------------------\nclass NormalizeLayer(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1,3,1,1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1,3,1,1))\n    def forward(self, x): return (x - self.mean) / self.std\n\nclass ModelWithNorm(nn.Module):\n    def __init__(self, base_model, mean, std, logit_clip=0.0):\n        super().__init__()\n        self.norm = NormalizeLayer(mean, std)\n        self.base = base_model\n        self.logit_clip = float(logit_clip)\n    def forward(self, x):\n        x = self.norm(x); logits = self.base(x)\n        if self.logit_clip > 0.0:\n            logits = torch.clamp(logits, min=-self.logit_clip, max=self.logit_clip)\n        return logits\n\n# --------------------- Data ---------------------\ntfm = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=tfm)\nval_full      = datasets.ImageFolder(os.path.join(DATA_DIR, VAL_DIR_NAME), transform=tfm)\n\n# UCSD 3-class labeled test set\ntest_dataset  = datasets.ImageFolder(\n    \"/kaggle/input/ucsd-3-class-labeled-retinal-oct-images/OCTUCSD-3class/OCTUCSD-3class/OCT/test\",\n    transform=tfm\n)\n\ndef stratified_subset_indices(dataset, num_classes=NUM_CLASSES, total=1000, seed=42):\n    per_class = total // num_classes\n    cls2idxs = {c: [] for c in range(num_classes)}\n    for i, (_, c) in enumerate(dataset.samples): cls2idxs[c].append(i)\n    rng = np.random.default_rng(seed)\n    chosen = []\n    for c in range(num_classes):\n        idxs = cls2idxs[c]; rng.shuffle(idxs); chosen.extend(idxs[:per_class])\n    return chosen\n\nVAL_COUNT = 1000\nval_indices = stratified_subset_indices(val_full, total=VAL_COUNT)\nval_dataset = Subset(val_full, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n\nprint(f\"üì¶ Datasets ‚Üí train:{len(train_dataset)} | val(full):{len(val_full)} | val(subset used):{len(val_dataset)} | test:{len(test_dataset)}\")\n\n# --------- CHECK: class_to_idx and build UCSD -> C8 label mapping ----------\nprint(\"üî§ C8 train class_to_idx:\", train_dataset.class_to_idx)\nprint(\"üî§ UCSD test class_to_idx:\", test_dataset.class_to_idx)\n\ntrain_c2i = train_dataset.class_to_idx\ntest_c2i  = test_dataset.class_to_idx\n\n# Build mapping from UCSD label index ‚Üí C8 label index,\n# ordered by UCSD label index (0,1,2,...)\nucsd_to_c8_list = []\nfor cls_name, uc_idx in sorted(test_c2i.items(), key=lambda x: x[1]):\n    if cls_name not in train_c2i:\n        raise ValueError(f\"Class {cls_name} in UCSD test not found in C8 train classes.\")\n    ucsd_to_c8_list.append(train_c2i[cls_name])\n\nucsd_label_map = torch.tensor(ucsd_to_c8_list, dtype=torch.long, device=device)\nprint(\"üîÅ UCSD label idx ‚Üí C8 idx mapping:\", {i: int(v) for i, v in enumerate(ucsd_label_map)})\n\n# --------------------- Attacks ---------------------\ndef fgsm_attack(images, labels, model, epsilon=EPSILON):\n    images = images.clone().detach().to(device).requires_grad_(True)\n    labels = labels.to(device)\n    with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n    grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n    return (images + epsilon * grad.sign()).clamp(0,1).detach()\n\ndef pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=PGD_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); ori = images.clone()\n    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n    images = (ori + delta).clamp(0,1).detach()\n    for _ in range(steps):\n        images.requires_grad_(True)\n        with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n        images = images.detach() + alpha * grad.sign()\n        images = torch.max(torch.min(images, ori + epsilon), ori - epsilon).clamp(0,1).detach()\n    model.train(was_training); return images\n\ndef bim_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=BIM_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); adv = images.clone()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, images + epsilon), images - epsilon).clamp(0,1)\n    model.train(was_training); return adv.detach()\n\ndef hybrid_fgsm_pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, refinement_steps=5):\n    was_training = model.training; model.eval()\n    base = images.detach().clone().to(device); labels = labels.to(device)\n    adv = fgsm_attack(base, labels, model, epsilon)\n    for _ in range(refinement_steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, base + epsilon), base - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\n# --------------------- Eval / Bench ---------------------\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n@torch.no_grad()\ndef evaluate(model, loader, attack=None, return_loss=False, label_map=None):\n    \"\"\"\n    label_map: optional LongTensor where label_map[loader_label] = target_label_for_model\n               used for UCSD test (3-class) to match C8 (8-class) indices.\n    \"\"\"\n    model.eval(); correct=0; total=0; run_loss=0.0\n    lm = label_map.to(device) if label_map is not None else None\n\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        lbs = lbs.to(device)\n        lbs_eval = lm[lbs] if lm is not None else lbs\n\n        data = ims\n        if   attack==\"FGSM\":\n            torch.set_grad_enabled(True)\n            data = fgsm_attack(ims, lbs_eval, model)\n            torch.set_grad_enabled(False)\n        elif attack==\"PGD\":\n            torch.set_grad_enabled(True)\n            data = pgd_attack(model, ims, lbs_eval)\n            torch.set_grad_enabled(False)\n        elif attack==\"BIM\":\n            torch.set_grad_enabled(True)\n            data = bim_attack(model, ims, lbs_eval)\n            torch.set_grad_enabled(False)\n        elif attack==\"Hybrid_FGSM_PGD\":\n            torch.set_grad_enabled(True)\n            data = hybrid_fgsm_pgd_attack(model, ims, lbs_eval)\n            torch.set_grad_enabled(False)\n\n        with autocast('cuda'):\n            logits = model(data); loss = F.cross_entropy(logits, lbs_eval); preds = logits.argmax(1)\n        correct += (preds==lbs_eval).sum().item(); total += lbs_eval.size(0); run_loss += loss.item()\n    acc = correct / max(total,1)\n    return (acc, run_loss / max(len(loader),1)) if return_loss else acc\n\ndef get_model_complexity(vit_or_ts, img_size=IMG_SIZE):\n    # FLOPs/params for patchable (timm) models only; best-effort for TS if possible\n    try:\n        m = vit_or_ts.eval().to(device)\n        dummy = torch.randn(1,3,img_size,img_size, device=device)\n        _sync()\n        try:\n            flops = FlopCountAnalysis(m, dummy).total()\n        except Exception:\n            m_cpu = m.to(\"cpu\")\n            with torch.no_grad():\n                macs, _ = get_model_complexity_info(m_cpu, (3,img_size,img_size), as_strings=False, print_per_layer_stat=False)\n            flops = macs * 2\n            m = m.to(device)\n        params = sum(p.numel() for p in m.parameters())\n        return flops, params\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Complexity analysis failed: {e}\")\n        return None, None\n\n@torch.no_grad()\ndef benchmark_inference(model, loader, max_batches=20, warmup_batches=3):\n    model.eval(); total_imgs=0; total_time=0.0\n    # warmup\n    w=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        with autocast('cuda'): _ = model(ims)\n        w+=1\n        if w>=warmup_batches: break\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    _sync(); measured=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        _sync(); t0=time.time()\n        with autocast('cuda'): _ = model(ims)\n        _sync(); dt=time.time()-t0\n        total_time += dt; total_imgs += ims.size(0); measured+=1\n        if measured>=max_batches: break\n    lat_ms = (total_time/max(1,measured))*1000.0\n    thr    = total_imgs / max(total_time,1e-9)\n    peak   = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"batches\":measured,\"images\":total_imgs,\"avg_batch_latency_ms\":lat_ms,\"throughput_img_per_s\":thr,\"peak_infer_mem_mb\":peak}\n\n# ---------- helper to compute Avg Max Softmax Probability ----------\n@torch.no_grad()\ndef avg_maxp_over_loader(model, loader, attack=None, max_batches=None, label_map=None):\n    model.eval()\n    total_sum, total_cnt, seen = 0.0, 0, 0\n    lm = label_map.to(device) if label_map is not None else None\n\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        lbs = lbs.to(device)\n        lbs_eval = lm[lbs] if lm is not None else lbs\n\n        data = ims\n        # enable grad only while CRAFTING adversarial examples\n        if attack == \"FGSM\":\n            with torch.enable_grad():\n                data = fgsm_attack(ims, lbs_eval, model)\n        elif attack == \"PGD\":\n            with torch.enable_grad():\n                data = pgd_attack(model, ims, lbs_eval)\n        elif attack == \"BIM\":\n            with torch.enable_grad():\n                data = bim_attack(model, ims, lbs_eval)\n        elif attack == \"Hybrid_FGSM_PGD\":\n            with torch.enable_grad():\n                data = hybrid_fgsm_pgd_attack(model, ims, lbs_eval)\n\n        # forward WITHOUT grad to measure probs\n        with autocast('cuda'):\n            logits = model(data)\n        probs = logits.softmax(dim=1).max(dim=1).values\n        total_sum += probs.sum().item()\n        total_cnt += probs.numel()\n        seen += 1\n        if max_batches is not None and seen >= max_batches:\n            break\n    return (total_sum / max(total_cnt, 1)) if total_cnt > 0 else float(\"nan\")\n\n# --------------------- Early stopping ---------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, restore_best=True):\n        self.patience=patience; self.min_delta=min_delta; self.restore_best=restore_best\n        self.best=math.inf; self.bad=0; self.best_state=None\n    def step(self, val_loss, model):\n        if (self.best - val_loss) > self.min_delta:\n            self.best = val_loss; self.bad=0\n            if self.restore_best:\n                self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n            return False\n        else:\n            self.bad+=1; return self.bad>self.patience\n    def restore(self, model):\n        if self.restore_best and self.best_state is not None: model.load_state_dict(self.best_state)\n\n# --------------------- Train loop ---------------------\ndef train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    model.train(); opt = torch.optim.AdamW(model.parameters(), lr=lr); scaler = GradScaler('cuda'); stopper = EarlyStopper()\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    hist=[]\n    for ep in range(1,epochs+1):\n        model.train(); run_loss=0.0; correct=0; total=0\n\n        # accumulators for Avg MaxP during TRAIN\n        sum_clean = 0.0; sum_pgd = 0.0; sum_bim = 0.0; sum_fgsm = 0.0\n        cnt_clean = 0;   cnt_pgd = 0;   cnt_bim = 0;   cnt_fgsm = 0\n\n        for bi,(ims,lbs) in enumerate(train_loader):\n            ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n            adv_pgd = pgd_attack(model, ims, lbs); adv_bim = bim_attack(model, ims, lbs)\n            mix_x = torch.cat([ims, adv_pgd, adv_bim], dim=0); mix_y = torch.cat([lbs, lbs, lbs], dim=0)\n            opt.zero_grad(set_to_none=True)\n            with autocast('cuda'): logits = model(mix_x); loss = loss_fn(logits, mix_y)\n            preds = logits.argmax(1); correct += (preds==mix_y).sum().item(); total += mix_y.size(0)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n            run_loss += loss.item()\n\n            # Avg MaxP for Clean/PGD/BIM from current batch logits\n            n = ims.size(0)\n            with torch.no_grad():\n                probs_clean = logits[:n].softmax(dim=1).max(dim=1).values\n                probs_pgd   = logits[n:2*n].softmax(dim=1).max(dim=1).values\n                probs_bim   = logits[2*n:3*n].softmax(dim=1).max(dim=1).values\n                sum_clean  += probs_clean.sum().item(); cnt_clean += probs_clean.numel()\n                sum_pgd    += probs_pgd.sum().item();   cnt_pgd   += probs_pgd.numel()\n                sum_bim    += probs_bim.sum().item();   cnt_bim   += probs_bim.numel()\n\n            # FGSM Avg MaxP metric (not used in loss; metric only)\n            with torch.enable_grad():\n                adv_fgsm   = fgsm_attack(ims, lbs, model)\n            with torch.no_grad():\n                logits_fgsm = model(adv_fgsm)\n                probs_fgsm  = logits_fgsm.softmax(dim=1).max(dim=1).values\n                sum_fgsm   += probs_fgsm.sum().item();  cnt_fgsm  += probs_fgsm.numel()\n\n            if bi%50==0: print(f\"Epoch {ep}/{epochs} | Batch {bi}/{len(train_loader)} | Loss {loss.item():.4f}\")\n\n        train_loss = run_loss/len(train_loader); train_acc = correct/max(total,1)\n        val_acc, val_loss = evaluate(model, val_loader, attack=None, return_loss=True)\n        print(f\"‚úÖ Epoch {ep}/{epochs} | TrainLoss {train_loss:.4f} | TrainAcc(mixed) {train_acc*100:.2f}% | ValLoss {val_loss:.4f} | ValAcc {val_acc*100:.2f}%\")\n\n        # Print TRAIN Avg MaxP for the epoch\n        avg_clean = sum_clean/max(cnt_clean,1)\n        avg_pgd   = sum_pgd  /max(cnt_pgd,1)\n        avg_bim   = sum_bim  /max(cnt_bim,1)\n        avg_fgsm  = sum_fgsm /max(cnt_fgsm,1)\n        print(f\"üîé TrainAvgMaxP (epoch {ep}) ‚Äî Clean {avg_clean:.3f} | FGSM {avg_fgsm:.3f} | PGD {avg_pgd:.3f} | BIM {avg_bim:.3f}\")\n\n        hist.append({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc_mixed\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc})\n        if stopper.step(val_loss, model):\n            print(f\"‚õî Early stopping at epoch {ep}\")\n            break\n    stopper.restore(model)\n    peak_train_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"train_peak_mem_mb\":peak_train_mb, \"history\":hist}\n\n# --------------------- Load model ---------------------\nprint(\"üß© Loading your pruned model...\")\nbase_model, detected_blocks, patchable = try_load_full_model()\nbase_model = base_model.to(device).eval()\nprint(f\"üìä Model ready on {device} | blocks: {detected_blocks if detected_blocks is not None else 'unknown (TS)'} | patchable={patchable}\")\n\n# Wrap with ImageNet normalization\nmodel = ModelWithNorm(base_model, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], logit_clip=0.0).to(device)\nmodel.to(memory_format=torch.channels_last).eval()\n\n# --------------------- FLOPs / Params ---------------------\nflops, params = get_model_complexity(base_model, IMG_SIZE)\nif flops is not None: print(f\"üßÆ Complexity: {flops/1e9:.3f} GFLOPs | Params: {params/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\n\n# --------------------- BEFORE training (UCSD TEST) ---------------------\nprint(\"\\nüîç BEFORE adversarial training (UCSD TEST set)...\")\nclean_b = evaluate(model, test_loader, attack=None, label_map=ucsd_label_map)\nfgsm_b  = evaluate(model, test_loader, attack=\"FGSM\", label_map=ucsd_label_map)\npgd_b   = evaluate(model, test_loader, attack=\"PGD\",  label_map=ucsd_label_map)\nbim_b   = evaluate(model, test_loader, attack=\"BIM\",  label_map=ucsd_label_map)\nhyb_b   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\", label_map=ucsd_label_map)\ninfer_b = benchmark_inference(model, test_loader, max_batches=20)\n\n# Avg MaxP BEFORE (TEST)\navgp_clean_b = avg_maxp_over_loader(model, test_loader, attack=None,                   label_map=ucsd_label_map)\navgp_fgsm_b  = avg_maxp_over_loader(model, test_loader, attack=\"FGSM\",                label_map=ucsd_label_map)\navgp_pgd_b   = avg_maxp_over_loader(model, test_loader, attack=\"PGD\",                 label_map=ucsd_label_map)\navgp_bim_b   = avg_maxp_over_loader(model, test_loader, attack=\"BIM\",                 label_map=ucsd_label_map)\navgp_hyb_b   = avg_maxp_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\",     label_map=ucsd_label_map)\n\nprint(f\"üìä BEFORE (UCSD TEST): Clean {clean_b*100:.2f}% | FGSM {fgsm_b*100:.2f}% | PGD {pgd_b*100:.2f}% | BIM {bim_b*100:.2f}% | Hybrid {hyb_b*100:.2f}%\")\nprint(f\"üîé BEFORE (AvgMaxP):  Clean {avgp_clean_b:.3f} | FGSM {avgp_fgsm_b:.3f} | PGD {avgp_pgd_b:.3f} | BIM {avgp_bim_b:.3f} | Hybrid {avgp_hyb_b:.3f}\")\nprint(f\"‚ö° Inference BEFORE: latency {infer_b['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_b['throughput_img_per_s']:.2f} img/s | peak mem {infer_b['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Train ---------------------\nprint(\"\\nüöÄ Training (PGD + BIM mixed) with validation & early stopping...\")\nt0 = time.time()\ntrain_metrics = train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\ntrain_time = time.time() - t0\nprint(f\"‚è±Ô∏è Training time: {train_time:.2f} s\")\nprint(f\"üß† Peak training memory: {train_metrics['train_peak_mem_mb']:.2f} MB\")\nprint_memory(\"After Training\")\n\n# --------------------- AFTER training ---------------------\ntrain_acc_after = evaluate(model, train_loader, attack=None)\nval_acc_after   = evaluate(model, val_loader,   attack=None)\nprint(f\"\\nüéØ AFTER Training ‚Üí Train(clean): {train_acc_after*100:.2f}% | Val(clean,1000): {val_acc_after*100:.2f}%\")\n\nprint(\"\\nüîç AFTER adversarial training (UCSD TEST set)...\")\nclean_a = evaluate(model, test_loader, attack=None,               label_map=ucsd_label_map)\nfgsm_a  = evaluate(model, test_loader, attack=\"FGSM\",             label_map=ucsd_label_map)\npgd_a   = evaluate(model, test_loader, attack=\"PGD\",              label_map=ucsd_label_map)\nbim_a   = evaluate(model, test_loader, attack=\"BIM\",              label_map=ucsd_label_map)\nhyb_a   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\",  label_map=ucsd_label_map)\ninfer_a = benchmark_inference(model, test_loader, max_batches=20)\n\n# Avg MaxP AFTER (TEST)\navgp_clean_a = avg_maxp_over_loader(model, test_loader, attack=None,               label_map=ucsd_label_map)\navgp_fgsm_a  = avg_maxp_over_loader(model, test_loader, attack=\"FGSM\",            label_map=ucsd_label_map)\navgp_pgd_a   = avg_maxp_over_loader(model, test_loader, attack=\"PGD\",             label_map=ucsd_label_map)\navgp_bim_a   = avg_maxp_over_loader(model, test_loader, attack=\"BIM\",             label_map=ucsd_label_map)\navgp_hyb_a   = avg_maxp_over_loader(model, test_loader, attack=\"Hybrid_FGSM_PGD\", label_map=ucsd_label_map)\n\nprint(f\"üìä AFTER (UCSD TEST): Clean {clean_a*100:.2f}% | FGSM {fgsm_a*100:.2f}% | PGD {pgd_a*100:.2f}% | BIM {bim_a*100:.2f}% | Hybrid {hyb_a*100:.2f}%\")\nprint(f\"üîé AFTER  (AvgMaxP): Clean {avgp_clean_a:.3f} | FGSM {avgp_fgsm_a:.3f} | PGD {avgp_pgd_a:.3f} | BIM {avgp_bim_a:.3f} | Hybrid {avgp_hyb_a:.3f}\")\nprint(f\"‚ö° Inference AFTER: latency {infer_a['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_a['throughput_img_per_s']:.2f} img/s | peak mem {infer_a['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Save ---------------------\ntorch.save(model, \"/kaggle/working/ViT_6blocks_adv_withVAL.pth\")\ntorch.save(model.state_dict(), \"/kaggle/working/ViT_6blocks_adv_withVAL_weights.pth\")\nprint(\"\\nüíæ Saved models to /kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment three alll models baselines","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiny-VIT clean training pipeline","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# ViT-TINY (timm) ‚Äî Stability + Monitoring + Pruning + LIGHT VALIDATION\n# - 70/15/15 stratified split from all images (ignores folder splits)\n# - Temperature-scaled attention (T=1.5)\n# - Post-attention LayerNorm (safe wrapper)\n# - Label smoothing, AMP, gradient clipping\n# - Attention entropy monitoring\n# - ‚úÖ LIGHT VALIDATION: quick val passes (few batches) every N epochs\n# - ‚úÖ EARLY STOPPING: patience/min_delta, restore best checkpoint\n# - ‚úÖ LIGHT HYPERPARAM TUNING: 5 epochs per candidate (tiny subset)\n# - ‚úÇÔ∏è Prune 6 blocks (keep 6), brief fine-tune\n# - üìä Prints: train/test acc, precision/recall/F1, params, FLOPs, memory, times\n# ============================================================\n\n!pip install -q timm scikit-learn ptflops\n\nimport os, random, time, gc, math, copy\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.amp import GradScaler, autocast\n\n# -----------------------------\n# üîß Reproducibility\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# ‚öôÔ∏è Validation / Tuning knobs\n# -----------------------------\nVAL_EVERY_N_EPOCHS = 1         # run lightweight val every N epochs\nVAL_MAX_BATCHES     = 6        # validate on at most this many batches (keep small)\nPATIENCE            = 3        # early stopping patience (epochs)\nMIN_DELTA           = 1e-3     # minimum improvement in val loss to reset patience\n\nENABLE_TUNING       = True     # pilot sweep to pick LR/WD quickly\nTUNE_EPOCHS         = 5        # 5 epochs per candidate\nTUNE_TRAIN_FRAC     = 0.08     # use 8% of train for quick tuning\nTUNE_VAL_MAX_BATCH  = 3        # validate only a handful of batches in tuning\n\n# -------------------------------------------------------\n# üìÅ Data root ‚Äî your Kaggle dataset\n# -------------------------------------------------------\ndata_root = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\ntrain_dir = os.path.join(data_root, \"train\")\nval_dir   = os.path.join(data_root, \"val\")\ntest_dir  = os.path.join(data_root, \"test\")\n\n# -------------------------------------------------------\n# üñºÔ∏è Transforms\n# -------------------------------------------------------\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\nval_test_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\n# -------------------------------------------------------\n# üì¶ Build unified sample list then 70/15/15 split (stratified)\n# -------------------------------------------------------\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\ndef list_class_dirs(parent): return sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])\n\nif os.path.exists(train_dir) and len(list_class_dirs(train_dir)) > 0:\n    classes = list_class_dirs(train_dir)\nelif os.path.exists(val_dir) and len(list_class_dirs(val_dir)) > 0:\n    classes = list_class_dirs(val_dir)\nelse:\n    classes = list_class_dirs(test_dir)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\ndef gather_samples(split_dir):\n    samples = []\n    for cls in classes:\n        cdir = os.path.join(split_dir, cls)\n        if not os.path.isdir(cdir): continue\n        for root, _, files in os.walk(cdir):\n            for f in files:\n                if f.lower().endswith(IMG_EXTS):\n                    samples.append((os.path.join(root, f), class_to_idx[cls]))\n    return samples\n\nall_samples = []\nfor d in [train_dir, val_dir, test_dir]:\n    all_samples.extend(gather_samples(d))\nassert len(all_samples) > 0, \"No images found. Check dataset paths.\"\n\nlabels  = np.array([lbl for _, lbl in all_samples])\nindices = np.arange(len(all_samples))\n\nsss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, test_size=0.30, random_state=42)\ntrain_idx, temp_idx = next(sss1.split(indices, labels))\ntemp_labels = labels[temp_idx]\nsss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, test_size=0.50, random_state=42)\nval_rel_idx, test_rel_idx = next(sss2.split(temp_idx, temp_labels))\nval_idx, test_idx = temp_idx[val_rel_idx], temp_idx[test_rel_idx]\n\ndef take(idxs): return [all_samples[i] for i in idxs]\ntrain_samples, val_samples, test_samples = take(train_idx), take(val_idx), take(test_idx)\n\nprint(f\"‚úÖ Split sizes ‚Äî Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\nprint(f\"    Classes ({len(classes)}): {classes}\")\n\n# -------------------------------------------------------\n# üß∞ Dataset from file paths\n# -------------------------------------------------------\nclass PathImageDataset(Dataset):\n    def __init__(self, samples, classes, transform=None):\n        self.samples = samples; self.classes = classes; self.transform = transform\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img, target\n\ntrain_dataset = PathImageDataset(train_samples, classes, transform=train_transforms)\nval_dataset   = PathImageDataset(val_samples,   classes, transform=val_test_transforms)\ntest_dataset  = PathImageDataset(test_samples,  classes, transform=val_test_transforms)\n\ndef make_loader(dataset, batch_size=32, shuffle=False):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ntrain_loader = make_loader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = make_loader(val_dataset,   batch_size=32, shuffle=False)\ntest_loader  = make_loader(test_dataset,  batch_size=32, shuffle=False)\n\n# -------------------------------------------------------\n# üî• Attention wrapper (temperature scaling)\n# -------------------------------------------------------\nfrom timm.models.vision_transformer import Attention\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv        = base_attn.qkv\n        self.proj       = base_attn.proj\n        self.proj_drop  = base_attn.proj_drop\n        self.attn_drop  = base_attn.attn_drop\n        self.num_heads  = base_attn.num_heads\n        self.q_norm     = getattr(base_attn, 'q_norm', None)\n        self.k_norm     = getattr(base_attn, 'k_norm', None)\n        self.scale      = base_attn.scale\n        self.temperature= float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n        attn_logits = (q @ k.transpose(-2,-1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad(): self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n        attn = attn_logits.softmax(dim=-1); attn = self.attn_drop(attn)\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\ndef apply_temperature_to_vit(model, T=1.5):\n    for _, blk in enumerate(model.blocks):\n        blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\n# -------------------------------------------------------\n# ‚úÖ Post-attention LayerNorm (safe block wrapper)\n# -------------------------------------------------------\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1, self.ls2 = getattr(blk,\"ls1\",None), getattr(blk,\"ls2\",None)\n        self.gamma_1, self.gamma_2 = getattr(blk,\"gamma_1\",None), getattr(blk,\"gamma_2\",None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, gamma): return ls(x) if ls is not None else (gamma*x if gamma is not None else x)\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1)\n        x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2)\n        x = x + self._dp(y, self.dp2)\n        return x\n\ndef add_post_attn_layernorm_safe(model, eps=1e-6):\n    device = next(model.parameters()).device\n    new_blocks = [PostAttnLNBlock(blk, eps=eps).to(device) for blk in model.blocks]\n    model.blocks = nn.Sequential(*new_blocks)\n\n# -------------------------------------------------------\n# üß† Build model + patches (‚û°Ô∏è ViT-TINY)\n# -------------------------------------------------------\nnum_classes = len(classes)\nmodel = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=num_classes).to(device)\nATTN_T = 1.5\napply_temperature_to_vit(model, T=ATTN_T)\nadd_post_attn_layernorm_safe(model, eps=1e-6)\n\nLOGIT_CLIP = 0.0\n\n# -------------------------------------------------------\n# üìè FLOPs / Params helper (with safe fallback)\n# -------------------------------------------------------\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef flops_and_params(model, input_res=(3,224,224)):\n    # Try ptflops, fall back to \"N/A\" FLOPs if it fails\n    try:\n        from ptflops import get_model_complexity_info\n        model_eval = copy.deepcopy(model).to('cpu').eval()\n        with torch.no_grad():\n            macs, params = get_model_complexity_info(model_eval, input_res, as_strings=False, verbose=False)\n        # ptflops returns MACs (multiply-adds). FLOPs ‚âà 2*MACs for convs; for transformers it's common to report MACs.\n        return macs, count_params(model)\n    except Exception as e:\n        return None, count_params(model)\n\n# -------------------------------------------------------\n# üß™ Training / Eval helpers (with LIGHT validation)\n# -------------------------------------------------------\ndef forward_with_optional_logit_clip(model, images):\n    logits = model(images)\n    if LOGIT_CLIP and LOGIT_CLIP > 0:\n        logits = torch.clamp(logits, min=-LOGIT_CLIP, max=LOGIT_CLIP)\n    return logits\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\n@torch.no_grad()\ndef report_attention_stats(model, loader, max_batches=1, tag=\"\"):\n    model.eval()\n    batches, entropies, max_abs_logits = 0, [], []\n    for images, _ in loader:\n        images = images.to(device)\n        with autocast('cuda'): _ = model(images)\n        blk_ents, blk_max = [], []\n        for blk in model.blocks:\n            attn = blk.attn\n            if isinstance(attn, TempScaledAttention) and attn.last_entropy is not None:\n                blk_ents.append(attn.last_entropy.numpy()); blk_max.append(attn.last_max_abs_pre_softmax)\n            else:\n                blk_ents.append(None); blk_max.append(None)\n        entropies.append(blk_ents); max_abs_logits.append(blk_max)\n        batches += 1\n        if batches >= max_batches: break\n    print(f\"\\nüîé Attention saturation ({tag}) ‚Äî T={ATTN_T}\")\n    for i, _ in enumerate(model.blocks):\n        head_arrays = [e[i] for e in entropies if e[i] is not None]\n        max_vals    = [m[i] for m in max_abs_logits if m[i] is not None]\n        if head_arrays:\n            H = np.stack(head_arrays, axis=0)\n            mean_ent, p10, p90 = H.mean(), np.percentile(H,10), np.percentile(H,90)\n            mx = np.mean(max_vals) if len(max_vals)>0 else float('nan')\n            print(f\"  Block {i:02d}: entropy mean={mean_ent:.3f}, p10={p10:.3f}, p90={p90:.3f} | max|pre-softmax|‚âà{mx:.3f}\")\n        else:\n            print(f\"  Block {i:02d}: (no stats)\")\n\ndef make_optimizer(model, lr=3e-4, weight_decay=0.01):\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\ndef make_scheduler(optimizer, t_initial=10):\n    return CosineLRScheduler(optimizer, t_initial=t_initial, lr_min=1e-6, warmup_lr_init=1e-5, warmup_t=3)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ndef eval_light(model, loader, max_batches=VAL_MAX_BATCHES):\n    model.eval()\n    total_loss, correct, total, batches = 0.0, 0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n                loss   = criterion(logits, labels)\n            total_loss += loss.item()\n            correct    += (logits.argmax(1) == labels).sum().item()\n            total      += labels.size(0)\n            batches    += 1\n            if batches >= max_batches: break\n    avg_loss = total_loss / max(1, batches)\n    acc = 100.0 * correct / max(1, total)\n    return avg_loss, acc\n\ndef evaluate_full(model, loader, class_names):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n            y_true += labels.cpu().tolist(); y_pred += preds.cpu().tolist()\n    acc = 100.0 * correct / max(1,total)\n    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    return acc, report, (pr, rc, f1)\n\n# -----------------------------\n# üõë Early stopping helper\n# -----------------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience; self.min_delta = min_delta\n        self.best = float('inf'); self.count = 0\n    def step(self, val_loss):\n        if val_loss < self.best - self.min_delta:\n            self.best = val_loss; self.count = 0; return False\n        else:\n            self.count += 1; return self.count > self.patience\n\n# -----------------------------\n# üîç Tiny pilot hyperparam tuning (LR/WD)\n# -----------------------------\ndef make_subset(dataset, frac):\n    n = len(dataset); k = max(1, int(n*frac))\n    idx = np.random.RandomState(42).choice(n, size=k, replace=False)\n    return Subset(dataset, idx.tolist())\n\ndef pilot_tune(base_model, train_ds, val_loader, candidates, epochs=TUNE_EPOCHS):\n    print(\"\\nüß™ Tiny pilot tuning (subset) ...\")\n    best_cfg, best_score, best_loss = None, -1, float('inf')\n    for (lr, wd) in candidates:\n        model = copy.deepcopy(base_model).to(device)\n        opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        scaler = GradScaler('cuda')\n        loader = make_loader(make_subset(train_ds, TUNE_TRAIN_FRAC), batch_size=32, shuffle=True)\n        for ep in range(epochs):\n            model.train()\n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                opt.zero_grad(set_to_none=True)\n                with autocast('cuda'):\n                    logits = forward_with_optional_logit_clip(model, images)\n                    loss   = criterion(logits, labels)\n                scaler.scale(loss).backward()\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(opt); scaler.update()\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=TUNE_VAL_MAX_BATCH)\n        print(f\"  LR={lr:.1e}, WD={wd:.3f} -> light ValAcc={val_acc:.2f}%, ValLoss={val_loss:.4f}\")\n        if (val_acc > best_score) or (abs(val_acc - best_score) < 1e-6 and val_loss < best_loss):\n            best_score, best_loss = val_acc, val_loss\n            best_cfg = (lr, wd)\n    print(f\"‚úÖ Chosen hyperparams: LR={best_cfg[0]:.1e}, WD={best_cfg[1]:.3f} (best light ValAcc={best_score:.2f}%)\")\n    return best_cfg\n\n# -------------------------------------------------------\n# üöÄ Train with LIGHT validation + Early Stopping\n# -------------------------------------------------------\ncandidates = [\n    (5e-5, 0.005), (5e-5, 0.01), (1e-4, 0.005), (1e-4, 0.01),\n    (2e-4, 0.005), (2e-4, 0.01), (3e-4, 0.01), (3e-4, 0.02),\n    (1e-4, 0.02), (3e-4, 0.05)\n]\n\nif ENABLE_TUNING:\n    chosen_lr, chosen_wd = pilot_tune(model, train_dataset, val_loader, candidates)\nelse:\n    chosen_lr, chosen_wd = 3e-4, 0.01\nprint(f\"\\n‚öôÔ∏è Using LR={chosen_lr:.1e}, WD={chosen_wd:.3f}\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=10)\nscaler    = GradScaler('cuda')\n\n# --- FLOPs/Params BEFORE pruning\nmacs_before, params_before = flops_and_params(model, input_res=(3,224,224))\nif macs_before is not None:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: {macs_before/1e9:.2f} G, Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: N/A, Params: {params_before/1e6:.2f} M\")\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\nprint_memory(\"Before Training\")\nEPOCHS = 10\nearly = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\nbest_state, best_val_acc = None, -1\n\n# Track training time and peak memory\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\ntrain_t0 = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    train_loss = total_loss / max(1,len(train_loader))\n    train_acc_epoch  = 100.0 * correct / max(1,total)\n    scheduler.step(epoch)\n\n    if (epoch+1) % VAL_EVERY_N_EPOCHS == 0:\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}% | \"\n              f\"Light Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n        report_attention_stats(model, val_loader, max_batches=1, tag=f\"epoch {epoch+1}\")\n\n        improve = (val_acc > best_val_acc + 1e-6) or (abs(val_acc - best_val_acc) < 1e-6 and (early.best - val_loss) > MIN_DELTA)\n        if improve:\n            best_val_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n\n        if early.step(val_loss):\n            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}. Best light ValAcc={best_val_acc:.2f}%\")\n            break\n    else:\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}%\")\n\ntrain_elapsed = time.time() - train_t0\ntrain_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Training\")\n\n# Restore best model (model selection)\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(\"üìå Restored best checkpoint by validation performance (light).\")\n\n# -------------------------------------------------------\n# ‚úÖ Full evaluation on TRAIN and TEST before pruning\n# -------------------------------------------------------\ndef measure_inference_performance(model, loader, tag=\"\"):\n    model.eval()\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\n    t0 = time.time()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n    elapsed = time.time() - t0\n    peak = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\n    acc = 100.0 * correct / max(1,total)\n    print(f\"\\U0001f9e0 {tag} Inference - Acc: {acc:.2f}%, Time: {elapsed:.2f}s, Peak Memory: {peak:.2f} MB\")\n    return acc, elapsed, peak\n\n# Training set metrics (accuracy only quick), plus full metrics\ntrain_acc_full, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\ntest_acc_before, test_report_before, test_prf_before = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ BEFORE Pruning: Test Acc: {test_acc_before:.2f}%\")\nprint(test_report_before)\n_, test_time_before, test_peak_mem_before = measure_inference_performance(model, test_loader, tag=\"Before Pruning\")\n\n# --- Print consolidated BEFORE metrics\nprint(\"\\nüìä BEFORE Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_full:.2f}%\")\nprint(f\"  Test  Accuracy: {test_acc_before:.2f}%\")\nprint(f\"  Test  Macro Precision: {test_prf_before[0]*100:.2f}% | Recall: {test_prf_before[1]*100:.2f}% | F1: {test_prf_before[2]*100:.2f}%\")\nprint(f\"  Training Time: {train_elapsed:.2f}s | Training Peak Memory: {train_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_before:.2f}s | Testing  Peak Memory: {test_peak_mem_before:.2f} MB\")\nif macs_before is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_before/1e9:.2f} G | Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_before/1e6:.2f} M\")\n\n# ===== SILENT SAVE (BEFORE PRUNING) =====\ntorch.save(model.state_dict(), \"/kaggle/working/before_weights.pth\")\ntorch.save(model, \"/kaggle/working/before_architecture.pth\")\n# ========================================\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Block-importance pruning + brief fine-tune\n#   üîÅ PRUNE 6 LOW-IMPORTANCE BLOCKS (keep 6)\n# -------------------------------------------------------\ndef compute_block_importance(model, loader):\n    model.train()\n    scores = torch.zeros(len(model.blocks), device=device)\n    # single-batch approximation\n    images, labels = next(iter(loader))\n    images, labels = images.to(device), labels.to(device)\n    with autocast('cuda'):\n        logits = forward_with_optional_logit_clip(model, images)\n        loss   = criterion(logits, labels)\n    loss.backward()\n    for i, blk in enumerate(model.blocks):\n        s, c = 0.0, 0\n        for p in blk.parameters():\n            if p.grad is not None:\n                s += p.grad.abs().mean(); c += 1\n        if c>0: scores[i] = s/c\n    model.zero_grad(set_to_none=True)\n    return scores\n\ndef prune_transformer_blocks(model, indices_to_remove):\n    model.blocks = nn.Sequential(*[blk for i, blk in enumerate(model.blocks) if i not in indices_to_remove])\n\nscores = compute_block_importance(model, val_loader)\n# ‚¨áÔ∏è Select 6 least-important blocks\nprune_indices = torch.argsort(scores)[:6].tolist()\nprint(f\"\\n‚úÇÔ∏è Pruned Blocks (6): {prune_indices}\")\nprune_transformer_blocks(model, prune_indices)\n\n# FLOPs/params AFTER pruning\nmacs_after, params_after = flops_and_params(model, input_res=(3,224,224))\nif macs_after is not None:\n    print(f\"üìê AFTER pruning ‚Äî MACs: {macs_after/1e9:.2f} G, Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"üìê AFTER pruning ‚Äî MACs: N/A, Params: {params_after/1e6:.2f} M\")\n\nprint(\"\\nüîß Fine-tuning for 2 epochs after pruning (with light val checks)...\")\nprint_memory(\"Before Fine-tuning\")\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=2)\nscaler    = GradScaler('cuda')\n\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\nft_t0 = time.time()\n\nfor epoch in range(2):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n    scheduler.step(epoch)\n    vloss, vacc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n    print(f\"[Fine-tune] Epoch {epoch+1}/2 - TrainLoss {total_loss/len(train_loader):.4f}, \"\n          f\"TrainAcc {100.0*correct/max(1,total):.2f}% | Light ValLoss {vloss:.4f}, ValAcc {vacc:.2f}%\")\n\nft_elapsed = time.time() - ft_t0\nft_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Fine-tuning\")\n\n# -------------------------------------------------------\n# ‚úÖ Final TRAIN/TEST after pruning\n# -------------------------------------------------------\ntrain_acc_after, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\npruned_test_acc, pruned_report, pruned_prf = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ AFTER Pruning: Test Acc: {pruned_test_acc:.2f}%\")\nprint(pruned_report)\n_, test_time_after, test_peak_mem_after = measure_inference_performance(model, test_loader, tag=\"After Pruning\")\n\nprint(\"\\nüìä AFTER Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_after:.2f}%\")\nprint(f\"  Test  Accuracy: {pruned_test_acc:.2f}%\")\nprint(f\"  Test  Macro Precision: {pruned_prf[0]*100:.2f}% | Recall: {pruned_prf[1]*100:.2f}% | F1: {pruned_prf[2]*100:.2f}%\")\nprint(f\"  Fine-tune Time: {ft_elapsed:.2f}s | Fine-tune Peak Memory: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_after:.2f}s | Testing  Peak Memory: {test_peak_mem_after:.2f} MB\")\nif macs_after is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_after/1e9:.2f} G | Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_after/1e6:.2f} M\")\n\n# ===== SILENT SAVE (AFTER PRUNING) =====\ntorch.save(model.state_dict(), \"/kaggle/working/after_weights.pth\")\ntorch.save(model, \"/kaggle/working/after_architecture.pth\")\n# =======================================\n\n# -------------------------------------------------------\n# üíæ Save\n# -------------------------------------------------------\nprint(\"\\nüíæ Saving pruned model ...\")\nsave_path = \"/kaggle/working/pruned_vit_tiny_6blocks.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"   -> {save_path}\")\n\n# -------------------------------------------------------\n# üßæ Final Overall Summary\n# -------------------------------------------------------\nprint(\"\\n================ FINAL SUMMARY ================\")\nprint(f\"Blocks pruned: 6 (kept {len(model.blocks)} blocks)\")\nif (macs_before is not None) and (macs_after is not None):\n    macs_delta = (macs_before - macs_after)/1e9\n    print(f\"MACs change: {macs_before/1e9:.2f} G -> {macs_after/1e9:.2f} G (Œî {macs_delta:.2f} G)\")\nelse:\n    print(\"MACs change: N/A\")\nprint(f\"Params change: {params_before/1e6:.2f} M -> {params_after/1e6:.2f} M (Œî {(params_before-params_after)/1e6:.2f} M)\")\nprint(f\"Train time: {train_elapsed:.2f}s | Train peak mem: {train_peak_mem_mb:.2f} MB\")\nprint(f\"Fine-tune time: {ft_elapsed:.2f}s | Fine-tune peak mem: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"Test(before) acc: {test_acc_before:.2f}% | Test(after) acc: {pruned_test_acc:.2f}%\")\nprint(\"===============================================\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"evalaute the original architecture before prunining","metadata":{}},{"cell_type":"code","source":"!pip install -q timm fvcore ptflops\n\nimport os, time, gc, random, json, math, collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Subset\n\nfrom timm import create_model\nfrom timm.models.vision_transformer import VisionTransformer, Attention\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.format import Format\nfrom torch.nn import Identity, Conv2d\nfrom torch.serialization import add_safe_globals\n\nfrom fvcore.nn import FlopCountAnalysis\nfrom ptflops import get_model_complexity_info\n\n# --------------------- Config ---------------------\nNUM_CLASSES   = 8\nIMG_SIZE      = 224\nATTN_T        = 1.5\nPOST_LN_EPS   = 1e-6\nBATCH_SIZE    = 32\nNUM_WORKERS   = min(8, os.cpu_count())\nMAX_GRAD_NORM = 1.0\n\nDATA_DIR      = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\nVAL_DIR_NAME  = \"val\"  # your folder is 'val', not 'validation'\n\nTS_MODEL_PATH = \"/kaggle/input/vittinyorgnoadv/pytorch/default/1/before_architecture.pth\"  # pickled full model (not TorchScript)\nWEIGHTS_PATH  = \"/kaggle/input/vittinyorgnoadvweights/pytorch/default/1/before_weights.pth\"\n\n# Adversarial params\nEPSILON    = 8/255\nALPHA      = 2/255\nPGD_STEPS  = 7\nBIM_STEPS  = 10\n\n# Train config\nEPOCHS      = 6\nLR          = 1e-4\nPATIENCE    = 2\nMIN_DELTA   = 0.0\n\n# Optional: force-prune loaded model to N blocks (e.g., 6). Keep None to use the checkpoint's block count.\nFORCE_KEEP_BLOCKS = None  # e.g., 6\n\n# --------------------- Utils ---------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\ndef print_memory(tag=\"\"):\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserv = torch.cuda.memory_reserved() / 1024**2\n        print(f\"üß† {tag} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\ndef _sync():\n    if torch.cuda.is_available(): torch.cuda.synchronize()\n\n# --------------------- Anti-saturation blocks ---------------------\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv = base_attn.qkv\n        self.proj = base_attn.proj\n        self.proj_drop = base_attn.proj_drop\n        self.attn_drop = base_attn.attn_drop\n        self.num_heads = base_attn.num_heads\n        self.q_norm = getattr(base_attn, 'q_norm', None)\n        self.k_norm = getattr(base_attn, 'k_norm', None)\n        self.scale = base_attn.scale\n        self.temperature = float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n\n        attn_logits = (q @ k.transpose(-2, -1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n\n        attn = attn_logits.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp   = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1 = getattr(blk, \"ls1\", None); self.ls2 = getattr(blk, \"ls2\", None)\n        self.gamma_1 = getattr(blk, \"gamma_1\", None); self.gamma_2 = getattr(blk, \"gamma_2\", None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, g): return ls(x) if ls is not None else (g * x if g is not None else x)\n\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1); x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2); x = x + self._dp(y, self.dp2)\n        return x\n\ndef apply_temperature_to_vit(v: VisionTransformer, T=1.5):\n    for blk in v.blocks: blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\ndef add_post_attn_layernorm_safe(v: VisionTransformer, eps=1e-6):\n    dev = next(v.parameters()).device\n    v.blocks = nn.Sequential(*[PostAttnLNBlock(blk, eps=eps).to(dev) for blk in v.blocks])\n\ndef prune_vit_to_n_blocks(v: VisionTransformer, n_keep: int):\n    v.blocks = nn.Sequential(*[blk for i, blk in enumerate(v.blocks) if i < n_keep])\n\n# --------------------- Model loading ---------------------\ndef detect_num_blocks_from_state(state_dict):\n    max_idx = -1\n    for k in state_dict.keys():\n        if k.startswith(\"blocks.\") and k[7:].split('.',1)[0].isdigit():\n            idx = int(k[7:].split('.',1)[0]); max_idx = max(max_idx, idx)\n    return max_idx + 1 if max_idx >= 0 else None\n\ndef _infer_embed_dim_from_sd(sd: dict):\n    if \"cls_token\" in sd:\n        return sd[\"cls_token\"].shape[-1]\n    if \"pos_embed\" in sd:\n        return sd[\"pos_embed\"].shape[-1]\n    if \"patch_embed.proj.weight\" in sd:\n        return sd[\"patch_embed.proj.weight\"].shape[0]\n    return None\n\ndef _pick_timm_from_dim(d):\n    if d == 192:  return \"vit_tiny_patch16_224\"\n    if d == 384:  return \"vit_small_patch16_224\"\n    if d == 768:  return \"vit_base_patch16_224\"\n    if d == 1024: return \"vit_large_patch16_224\"\n    return \"vit_base_patch16_224\"\n\ndef _filter_load_state_dict(model, sd):\n    msd = model.state_dict()\n    ok = {k:v for k,v in sd.items() if k in msd and msd[k].shape == v.shape}\n    missing = [k for k in msd.keys() if k not in ok]\n    unexpected = [k for k in sd.keys() if k not in msd]\n    model.load_state_dict(ok, strict=False)\n    print(f\"‚úÖ Loaded {len(ok)} matching tensors | ‚ö†Ô∏è skipped {len(missing)} missing + {len(unexpected)} unexpected due to shape/name\")\n    if missing[:5]:    print(\"   missing (first 5):\", missing[:5])\n    if unexpected[:5]: print(\"   unexpected (first 5):\", unexpected[:5])\n\ndef build_model_from_weights(weights_path, num_classes=NUM_CLASSES):\n    print(f\"üì¶ Loading weights from: {weights_path}\")\n    sd = torch.load(weights_path, map_location=\"cpu\")\n    if \"state_dict\" in sd and isinstance(sd[\"state_dict\"], dict):\n        sd = sd[\"state_dict\"]\n\n    d = _infer_embed_dim_from_sd(sd)\n    if d is None:\n        raise RuntimeError(\"Could not infer embed dim from checkpoint.\")\n    model_name = _pick_timm_from_dim(d)\n    print(f\"üîé Inferred embed_dim={d} ‚Üí using timm model: {model_name}\")\n\n    n_blocks = detect_num_blocks_from_state(sd) or 12\n    print(f\"üîß Detected block count in weights: {n_blocks}\")\n\n    m = create_model(model_name, pretrained=False, num_classes=num_classes)\n    apply_temperature_to_vit(m, T=ATTN_T)\n    add_post_attn_layernorm_safe(m, eps=POST_LN_EPS)\n    prune_vit_to_n_blocks(m, n_blocks)\n\n    _filter_load_state_dict(m, sd)\n    return m, n_blocks\n\n# Register all custom classes BEFORE any torch.load of a full model\nadd_safe_globals([\n    VisionTransformer, PatchEmbed, Conv2d, Identity, Format, nn.Dropout,\n    TempScaledAttention, PostAttnLNBlock\n])\n\ndef try_load_full_model():\n    # Prefer reconstruct-from-weights (patchable & trainable), else load pickled full model\n    try:\n        m, n = build_model_from_weights(WEIGHTS_PATH, num_classes=NUM_CLASSES)\n        print(f\"‚úÖ Reconstructed TIMM model with {n} blocks\")\n        return m, n, True\n    except Exception as e:\n        print(f\"‚ùå Reconstruct-from-weights failed: {e}\\nüîÑ Falling back to loading a pickled full model: {TS_MODEL_PATH}\")\n        # This is NOT TorchScript; it was saved via torch.save(model, ...)\n        full = torch.load(TS_MODEL_PATH, map_location=\"cpu\")  # relies on add_safe_globals above\n        n = None\n        try:\n            if hasattr(full, \"blocks\"):\n                n = len(list(full.blocks))\n        except Exception:\n            pass\n        print(f\"‚úÖ Loaded pickled model; blocks: {n if n is not None else 'unknown'}\")\n        return full, n, False\n\n# --------------------- Normalization wrapper ---------------------\nclass NormalizeLayer(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1,3,1,1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1,3,1,1))\n    def forward(self, x): return (x - self.mean) / self.std\n\nclass ModelWithNorm(nn.Module):\n    def __init__(self, base_model, mean, std, logit_clip=0.0):\n        super().__init__()\n        self.norm = NormalizeLayer(mean, std)\n        self.base = base_model\n        self.logit_clip = float(logit_clip)\n    def forward(self, x):\n        x = self.norm(x); logits = self.base(x)\n        if self.logit_clip > 0.0:\n            logits = torch.clamp(logits, min=-self.logit_clip, max=self.logit_clip)\n        return logits\n\n# --------------------- Data ---------------------\ntfm = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=tfm)\nval_full      = datasets.ImageFolder(os.path.join(DATA_DIR, VAL_DIR_NAME), transform=tfm)\ntest_dataset  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"),  transform=tfm)\n\ndef stratified_subset_indices(dataset, num_classes=NUM_CLASSES, total=1000, seed=42):\n    per_class = total // num_classes\n    cls2idxs = {c: [] for c in range(num_classes)}\n    for i, (_, c) in enumerate(dataset.samples): cls2idxs[c].append(i)\n    rng = np.random.default_rng(seed)\n    chosen = []\n    for c in range(num_classes):\n        idxs = cls2idxs[c]; rng.shuffle(idxs); chosen.extend(idxs[:per_class])\n    return chosen\n\nVAL_COUNT = 1000\nval_indices = stratified_subset_indices(val_full, total=VAL_COUNT)\nval_dataset = Subset(val_full, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n\nprint(f\"üì¶ Datasets ‚Üí train:{len(train_dataset)} | val(full):{len(val_full)} | val(subset used):{len(val_dataset)} | test:{len(test_dataset)}\")\n\n# --------------------- Attacks ---------------------\ndef fgsm_attack(images, labels, model, epsilon=EPSILON):\n    images = images.clone().detach().to(device).requires_grad_(True)\n    labels = labels.to(device)\n    with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n    grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n    return (images + epsilon * grad.sign()).clamp(0,1).detach()\n\ndef pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=PGD_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); ori = images.clone()\n    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n    images = (ori + delta).clamp(0,1).detach()\n    for _ in range(steps):\n        images.requires_grad_(True)\n        with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n        images = images.detach() + alpha * grad.sign()\n        images = torch.max(torch.min(images, ori + epsilon), ori - epsilon).clamp(0,1).detach()\n    model.train(was_training); return images\n\ndef bim_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=BIM_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); adv = images.clone()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, images + epsilon), images - epsilon).clamp(0,1)\n    model.train(was_training); return adv.detach()\n\ndef hybrid_fgsm_pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, refinement_steps=5):\n    was_training = model.training; model.eval()\n    base = images.detach().clone().to(device); labels = labels.to(device)\n    adv = fgsm_attack(base, labels, model, epsilon)\n    for _ in range(refinement_steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, base + epsilon), base - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\n# --------------------- Eval / Bench ---------------------\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n@torch.no_grad()\ndef evaluate(model, loader, attack=None, return_loss=False):\n    model.eval(); correct=0; total=0; run_loss=0.0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n        data = ims\n        if   attack==\"FGSM\":  torch.set_grad_enabled(True); data = fgsm_attack(ims, lbs, model); torch.set_grad_enabled(False)\n        elif attack==\"PGD\":   torch.set_grad_enabled(True); data = pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"BIM\":   torch.set_grad_enabled(True); data = bim_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"Hybrid_FGSM_PGD\":\n            torch.set_grad_enabled(True); data = hybrid_fgsm_pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        with autocast('cuda'):\n            logits = model(data); loss = F.cross_entropy(logits, lbs); preds = logits.argmax(1)\n        correct += (preds==lbs).sum().item(); total += lbs.size(0); run_loss += loss.item()\n    acc = correct / max(total,1)\n    return (acc, run_loss / max(len(loader),1)) if return_loss else acc\n\ndef get_model_complexity(vit_or_ts, img_size=IMG_SIZE):\n    try:\n        m = vit_or_ts.eval().to(device)\n        dummy = torch.randn(1,3,img_size,img_size, device=device)\n        _sync()\n        try:\n            flops = FlopCountAnalysis(m, dummy).total()\n        except Exception:\n            m_cpu = m.to(\"cpu\")\n            with torch.no_grad():\n                macs, _ = get_model_complexity_info(m_cpu, (3,img_size,img_size), as_strings=False, print_per_layer_stat=False)\n            flops = macs * 2\n            m = m.to(device)\n        params = sum(p.numel() for p in m.parameters())\n        return flops, params\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Complexity analysis failed: {e}\")\n        return None, None\n\n@torch.no_grad()\ndef benchmark_inference(model, loader, max_batches=20, warmup_batches=3):\n    model.eval(); total_imgs=0; total_time=0.0\n    # warmup\n    w=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        with autocast('cuda'): _ = model(ims)\n        w+=1\n        if w>=warmup_batches: break\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    _sync(); measured=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        _sync(); t0=time.time()\n        with autocast('cuda'): _ = model(ims)\n        _sync(); dt=time.time()-t0\n        total_time += dt; total_imgs += ims.size(0); measured+=1\n        if measured>=max_batches: break\n    lat_ms = (total_time/max(1,measured))*1000.0\n    thr    = total_imgs / max(total_time,1e-9)\n    peak   = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"batches\":measured,\"images\":total_imgs,\"avg_batch_latency_ms\":lat_ms,\"throughput_img_per_s\":thr,\"peak_infer_mem_mb\":peak}\n\n# --------------------- Early stopping ---------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, restore_best=True):\n        self.patience=patience; self.min_delta=min_delta; self.restore_best=restore_best\n        self.best=math.inf; self.bad=0; self.best_state=None\n    def step(self, val_loss, model):\n        if (self.best - val_loss) > self.min_delta:\n            self.best = val_loss; self.bad=0\n            if self.restore_best:\n                self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n            return False\n        else:\n            self.bad+=1; return self.bad>self.patience\n    def restore(self, model):\n        if self.restore_best and self.best_state is not None: model.load_state_dict(self.best_state)\n\n# --------------------- Train loop ---------------------\ndef train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    model.train(); opt = torch.optim.AdamW(model.parameters(), lr=lr); scaler = GradScaler('cuda'); stopper = EarlyStopper()\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    hist=[]\n    for ep in range(1,epochs+1):\n        model.train(); run_loss=0.0; correct=0; total=0\n        for bi,(ims,lbs) in enumerate(train_loader):\n            ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n            adv_pgd = pgd_attack(model, ims, lbs); adv_bim = bim_attack(model, ims, lbs)\n            mix_x = torch.cat([ims, adv_pgd, adv_bim], dim=0); mix_y = torch.cat([lbs, lbs, lbs], dim=0)\n            opt.zero_grad(set_to_none=True)\n            with autocast('cuda'): logits = model(mix_x); loss = loss_fn(logits, mix_y)\n            preds = logits.argmax(1); correct += (preds==mix_y).sum().item(); total += mix_y.size(0)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n            run_loss += loss.item()\n            if bi%50==0: print(f\"Epoch {ep}/{epochs} | Batch {bi}/{len(train_loader)} | Loss {loss.item():.4f}\")\n        train_loss = run_loss/len(train_loader); train_acc = correct/max(total,1)\n        val_acc, val_loss = evaluate(model, val_loader, attack=None, return_loss=True)\n        print(f\"‚úÖ Epoch {ep}/{epochs} | TrainLoss {train_loss:.4f} | TrainAcc(mixed) {train_acc*100:.2f}% | ValLoss {val_loss:.4f} | ValAcc {val_acc*100:.2f}%\")\n        hist.append({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc_mixed\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc})\n        if stopper.step(val_loss, model):\n            print(f\"‚õî Early stopping at epoch {ep}\")\n            break\n    stopper.restore(model)\n    peak_train_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"train_peak_mem_mb\":peak_train_mb, \"history\":hist}\n\n# --------------------- Load model ---------------------\nprint(\"üß© Loading your pruned/variant-correct model...\")\nbase_model, detected_blocks, patchable = try_load_full_model()\n\n# (Optional) force prune to N blocks\nif FORCE_KEEP_BLOCKS is not None:\n    prune_vit_to_n_blocks(base_model, FORCE_KEEP_BLOCKS)\n    print(f\"‚úÇÔ∏è Force-pruned to {FORCE_KEEP_BLOCKS} blocks\")\n\nbase_model = base_model.to(device).eval()\nprint(f\"üìä Model ready on {device} | blocks: {detected_blocks if detected_blocks is not None else 'unknown'} | patchable={patchable}\")\n\n# Wrap with ImageNet normalization\nmodel = ModelWithNorm(base_model, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], logit_clip=0.0).to(device)\nmodel.to(memory_format=torch.channels_last).eval()\n\n# --------------------- FLOPs / Params ---------------------\nflops, params = get_model_complexity(base_model, IMG_SIZE)\nif flops is not None: print(f\"üßÆ Complexity: {flops/1e9:.3f} GFLOPs | Params: {params/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\n\n# --------------------- BEFORE training (TEST) ---------------------\nprint(\"\\nüîç BEFORE adversarial training (TEST set)...\")\nclean_b = evaluate(model, test_loader, attack=None)\nfgsm_b  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_b   = evaluate(model, test_loader, attack=\"PGD\")\nbim_b   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_b   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_b = benchmark_inference(model, test_loader, max_batches=20)\n\nprint(f\"üìä BEFORE (TEST): Clean {clean_b*100:.2f}% | FGSM {fgsm_b*100:.2f}% | PGD {pgd_b*100:.2f}% | BIM {bim_b*100:.2f}% | Hybrid {hyb_b*100:.2f}%\")\nprint(f\"‚ö° Inference BEFORE: latency {infer_b['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_b['throughput_img_per_s']:.2f} img/s | peak mem {infer_b['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Train ---------------------\nprint(\"\\nüöÄ Training (PGD + BIM mixed) with validation & early stopping...\")\nt0 = time.time()\ntrain_metrics = train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\ntrain_time = time.time() - t0\nprint(f\"‚è±Ô∏è Training time: {train_time:.2f} s\")\nprint(f\"üß† Peak training memory: {train_metrics['train_peak_mem_mb']:.2f} MB\")\nprint_memory(\"After Training\")\n\n# --------------------- AFTER training ---------------------\ntrain_acc_after = evaluate(model, train_loader, attack=None)\nval_acc_after   = evaluate(model, val_loader,   attack=None)\nprint(f\"\\nüéØ AFTER Training ‚Üí Train(clean): {train_acc_after*100:.2f}% | Val(clean,1000): {val_acc_after*100:.2f}%\")\n\nprint(\"\\nüîç AFTER adversarial training (TEST set)...\")\nclean_a = evaluate(model, test_loader, attack=None)\nfgsm_a  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_a   = evaluate(model, test_loader, attack=\"PGD\")\nbim_a   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_a   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_a = benchmark_inference(model, test_loader, max_batches=20)\n\nprint(f\"üìä AFTER (TEST): Clean {clean_a*100:.2f}% | FGSM {fgsm_a*100:.2f}% | PGD {pgd_a*100:.2f}% | BIM {bim_a*100:.2f}% | Hybrid {hyb_a*100:.2f}%\")\nprint(f\"‚ö° Inference AFTER: latency {infer_a['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_a['throughput_img_per_s']:.2f} img/s | peak mem {infer_a['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Save ---------------------\n# Save AFTER-training model (architecture) and weights\nARCH_PATH = \"/kaggle/working/architecture.pth\"\nWEI_PATH  = \"/kaggle/working/weights.pth\"\n\n# Save the full wrapped model (includes normalization + base model)\ntorch.save(model, ARCH_PATH)\n# Save just the weights of the wrapped model\ntorch.save(model.state_dict(), WEI_PATH)\n\nprint(f\"\\nüíæ Saved models:\")\nprint(f\"   ‚Ä¢ Full architecture  ‚Üí {ARCH_PATH}\")\nprint(f\"   ‚Ä¢ Weights only       ‚Üí {WEI_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# mobileVIT clean + adversarial pipelines in one code","metadata":{}},{"cell_type":"code","source":"!pip install -q timm ptflops scikit-learn\n\nimport os, time, gc, math, random, copy, csv\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# AMP imports (PyTorch ‚â•2.1 first, else fallback)\ntry:\n    from torch.amp import autocast, GradScaler\nexcept Exception:\n    from torch.cuda.amp import autocast, GradScaler\n\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Subset\n\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\n\nimport timm\nfrom timm import create_model\nfrom ptflops import get_model_complexity_info\n\n# --------------------- Config (mirrors CODE 1) ---------------------\nNUM_CLASSES   = 8\nIMG_SIZE      = 224\nATTN_T        = 1.5\nPOST_LN_EPS   = 1e-6\nBATCH_SIZE    = 32\nNUM_WORKERS   = min(8, os.cpu_count() or 2)\nMAX_GRAD_NORM = 1.0\nSEED          = 42\n\nDATA_DIR      = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\nVAL_DIR_NAME  = \"val\"   # your folder is 'val', not 'validation'\n\n# === YOUR CLEAN MODEL PATHS (preferred: state_dict) ===\nCLEAN_STATE_DICT = \"/kaggle/working/mobilevit_xxs_CLEAN_FOR_ADV_weights.pth\"\nCLEAN_FULL_MODEL = \"/kaggle/working/mobilevit_xxs_CLEAN_FOR_ADV.pth\"\n\n# Adversarial params (same spirit as CODE 1)\nEPSILON    = 8/255\nALPHA      = 2/255\nPGD_STEPS  = 7\nBIM_STEPS  = 10\n\n# Train config\nEPOCHS       = 6\nLR           = 1e-4\nPATIENCE     = 2\nMIN_DELTA    = 0.0\nLABEL_SMOOTH = 0.1\nWEIGHT_DECAY = 0.01  # AdamW good default\n\nSAVE_PREFIX = \"mobilevit_xxs_adv_PGDBIM_T15_MATCHCODE1\"\n\n# --------------------- Utils ---------------------\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\nset_seed()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\ndef print_memory(tag=\"\"):\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserv = torch.cuda.memory_reserved() / 1024**2\n        print(f\"üß† {tag} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\ndef _sync():\n    if torch.cuda.is_available(): torch.cuda.synchronize()\n\ndef write_metrics_csv(path, rows, fieldnames):\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    new_file = not path.exists()\n    with path.open(\"a\", newline=\"\") as f:\n        w = csv.DictWriter(f, fieldnames=fieldnames)\n        if new_file: w.writeheader()\n        for r in rows: w.writerow(r)\n\n# --------------------- Normalization wrapper ---------------------\nclass NormalizeLayer(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1,3,1,1))\n        self.register_buffer(\"std\",  torch.tensor(std).view(1,3,1,1))\n    def forward(self, x): return (x - self.mean) / self.std\n\nclass ModelWithNorm(nn.Module):\n    def __init__(self, base_model, mean, std, logit_clip=0.0):\n        super().__init__()\n        self.norm = NormalizeLayer(mean, std)\n        self.base = base_model\n        self.logit_clip = float(logit_clip)\n    def forward(self, x):\n        x = self.norm(x); logits = self.base(x)\n        if self.logit_clip > 0.0:\n            logits = torch.clamp(logits, -self.logit_clip, self.logit_clip)\n        return logits\n\n# --------------------- Temp-scaled attention + Post-Attn LN ---------------------\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: nn.Module, temperature: float = 1.5):\n        super().__init__()\n        self.base = base_attn\n        self.qkv = getattr(base_attn, 'qkv', None)\n        self.proj = getattr(base_attn, 'proj', None)\n        self.proj_drop = getattr(base_attn, 'proj_drop', None)\n        self.attn_drop = getattr(base_attn, 'attn_drop', None)\n        self.num_heads = getattr(base_attn, 'num_heads', None)\n        self.scale = getattr(base_attn, 'scale', None)\n        self.q_norm = getattr(base_attn, 'q_norm', None)\n        self.k_norm = getattr(base_attn, 'k_norm', None)\n        self.temperature = float(temperature)\n        self._has_qkv = (self.qkv is not None) and (self.num_heads is not None) and (self.scale is not None)\n        self.last_entropy = None\n        self.last_max_abs_pre_softmax = None\n\n    def forward(self, x):\n        if not self._has_qkv:\n            return self.base(x)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n        logits = (q @ k.transpose(-2, -1)) * self.scale\n        logits = logits / self.temperature\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = float(logits.abs().amax().item())\n        attn = torch.softmax(logits.float(), dim=-1).type_as(logits)\n        if self.attn_drop is not None: attn = self.attn_drop(attn)\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            self.last_entropy = float((-(p * p.log()).sum(dim=-1).mean()))\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        if self.proj is not None: x = self.proj(x)\n        if self.proj_drop is not None: x = self.proj_drop(x)\n        return x\n\nclass PostAttnLNBlock(nn.Module):\n    \"\"\"Wrap a ViT-like block with extra LN after attention residual.\"\"\"\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1 = getattr(blk, \"norm1\", None)\n        self.norm2 = getattr(blk, \"norm2\", None)\n        self.attn  = getattr(blk, \"attn\", None)\n        self.mlp   = getattr(blk, \"mlp\", None)\n        self.dp1 = getattr(blk, \"drop_path1\", getattr(blk, \"drop_path\", None))\n        self.dp2 = getattr(blk, \"drop_path2\", getattr(blk, \"drop_path\", None))\n        self.ls1 = getattr(blk, \"ls1\", None); self.ls2 = getattr(blk, \"ls2\", None)\n        self.g1  = getattr(blk, \"gamma_1\", None); self.g2 = getattr(blk, \"gamma_2\", None)\n        d = None\n        if self.norm1 is not None:\n            try: d = self.norm1.normalized_shape[0]\n            except Exception: d = None\n        if d is None and self.norm2 is not None:\n            try: d = self.norm2.normalized_shape[0]\n            except Exception: d = None\n        self.post_ln = nn.LayerNorm(d, eps=eps) if d is not None else None\n\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, g): return ls(x) if ls is not None else (g * x if g is not None else x)\n\n    def forward(self, x):\n        if not all([self.norm1, self.norm2, self.attn, self.mlp, self.post_ln]):\n            return self.attn(x) if self.attn is not None else x\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.g1); x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.g2); x = x + self._dp(y, self.dp2)\n        return x\n\ndef _get_parent(model, dotted):\n    cur = model\n    path = dotted.split('.')\n    for p in path[:-1]:\n        if p.isdigit(): cur = cur[int(p)]\n        else:           cur = getattr(cur, p)\n    return cur\n\ndef _replace_child(parent, child_name, new_child):\n    if isinstance(parent, nn.Sequential) and child_name.isdigit():\n        parent[int(child_name)] = new_child\n    elif isinstance(parent, nn.Module):\n        setattr(parent, child_name, new_child)\n    else:\n        raise RuntimeError(\"Unsupported parent type for replacement\")\n\ndef _looks_like_vit_block(mod: nn.Module):\n    return all(hasattr(mod, a) for a in [\"attn\", \"mlp\", \"norm1\", \"norm2\"])\n\ndef _has_mhsa(mod: nn.Module):\n    attn = getattr(mod, \"attn\", None)\n    return isinstance(attn, nn.Module) and hasattr(attn, \"qkv\") and hasattr(attn, \"num_heads\") and hasattr(attn, \"scale\")\n\ndef patch_mobilevit_attention(model: nn.Module, T=1.5, eps_post_ln=1e-6, apply_post_ln=True):\n    patched_mhsa = 0\n    patched_full = 0\n    for name, mod in model.named_modules():\n        if _has_mhsa(mod):\n            try:\n                mod.attn = TempScaledAttention(mod.attn, temperature=T)\n                patched_mhsa += 1\n                did_full = False\n                if apply_post_ln and _looks_like_vit_block(mod):\n                    parent = _get_parent(model, name)\n                    _replace_child(parent, name.split('.')[-1], PostAttnLNBlock(mod, eps=eps_post_ln))\n                    patched_full += 1\n                    did_full = True\n                print(f\"‚úÖ Patched {'MHSA+postLN' if did_full else 'MHSA-only'}: {name}\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Skipped patching {name}: {e}\")\n    if patched_mhsa == 0:\n        print(\"‚ö†Ô∏è No MHSA-like modules matched (qkv/num_heads/scale). Check timm version/arch.\")\n    else:\n        print(f\"‚ú® Patches summary ‚Üí MHSA-only: {patched_mhsa - patched_full} | MHSA+postLN: {patched_full}\")\n    return model\n\n# --------------------- Data ---------------------\n# Note: NO normalization here; norm is inside the model wrapper.\ntfm = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=tfm)\nval_full      = datasets.ImageFolder(os.path.join(DATA_DIR, VAL_DIR_NAME), transform=tfm)\ntest_dataset  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"),  transform=tfm)\n\ndef stratified_subset_indices(dataset, num_classes=NUM_CLASSES, total=1000, seed=SEED):\n    per_class = total // num_classes\n    cls2idxs = {c: [] for c in range(num_classes)}\n    for i, (_, c) in enumerate(dataset.samples):\n        cls2idxs[c].append(i)\n    rng = np.random.default_rng(seed); chosen=[]\n    for c in range(num_classes):\n        idxs = cls2idxs[c]; rng.shuffle(idxs); chosen.extend(idxs[:per_class])\n    return chosen\n\nVAL_COUNT = 1000\nval_indices = stratified_subset_indices(val_full, total=VAL_COUNT)\nval_dataset = Subset(val_full, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n\nprint(f\"üì¶ Datasets ‚Üí train:{len(train_dataset)} | val(full):{len(val_full)} | val(subset used):{len(val_dataset)} | test:{len(test_dataset)}\")\n\n# --------------------- Build & Load CLEAN model ---------------------\ndef build_blank_model(num_classes):\n    base = timm.create_model('mobilevit_xxs', pretrained=True, num_classes=num_classes)\n    base = patch_mobilevit_attention(base, T=ATTN_T, eps_post_ln=POST_LN_EPS, apply_post_ln=True)\n    model = ModelWithNorm(base, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], logit_clip=0.0)\n    return model\n\ndef load_clean_trained_model():\n    # Prefer state_dict (safer across sessions)\n    if os.path.isfile(CLEAN_STATE_DICT):\n        print(f\"üì¶ Loading CLEAN state_dict: {CLEAN_STATE_DICT}\")\n        m = build_blank_model(NUM_CLASSES).to(device)\n        sd = torch.load(CLEAN_STATE_DICT, map_location=\"cpu\")\n        missing, unexpected = m.load_state_dict(sd, strict=False)\n        if missing:    print(f\"   Missing keys: {len(missing)}\")\n        if unexpected: print(f\"   Unexpected keys: {len(unexpected)}\")\n        return m\n    # Fallback: full model (must match class defs)\n    if os.path.isfile(CLEAN_FULL_MODEL):\n        print(f\"üì¶ Loading CLEAN full model: {CLEAN_FULL_MODEL}\")\n        m = torch.load(CLEAN_FULL_MODEL, map_location=device)\n        return m\n    raise FileNotFoundError(\"Could not find CLEAN model. Update CLEAN_STATE_DICT / CLEAN_FULL_MODEL.\")\n\nprint(\"üöÄ Loading clean-trained MobileViT-XXS...\")\nmodel = load_clean_trained_model().to(device)\nmodel.to(memory_format=torch.channels_last).eval()\nprint(\"‚úÖ Model loaded & ready (clean weights + patches + in-model norm)\")\n\n# --------------------- Complexity ---------------------\ndef model_macs_params_size(m, input_size=(3, IMG_SIZE, IMG_SIZE)):\n    m_cpu = copy.deepcopy(m).to(\"cpu\").eval()\n    with torch.no_grad():\n        macs, params = get_model_complexity_info(m_cpu, input_size, as_strings=False, print_per_layer_stat=False)\n    param_size = sum(p.nelement() * p.element_size() for p in m_cpu.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in m_cpu.buffers())\n    size_mb = (param_size + buffer_size) / 1024**2\n    return macs, params, size_mb\n\nMACs, PARAMS, MODEL_SIZE_MB = model_macs_params_size(model)\nFLOPs = MACs * 2\nprint(f\"üìä Model stats ‚Üí MACs: {MACs/1e9:.3f} G | FLOPs‚âà: {FLOPs/1e9:.3f} G | Params: {PARAMS/1e6:.2f} M | Size: {MODEL_SIZE_MB:.1f} MB\")\n\n# --------------------- Attacks ---------------------\nloss_fn = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n\ndef fgsm_attack(images, labels, model, epsilon=EPSILON):\n    images = images.clone().detach().to(device).requires_grad_(True)\n    labels = labels.to(device)\n    with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n    grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n    return (images + epsilon * grad.sign()).clamp(0,1).detach()\n\ndef pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=PGD_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); ori = images.clone()\n    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n    adv = (ori + delta).clamp(0,1).detach()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, ori + epsilon), ori - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\ndef bim_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=BIM_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); adv = images.clone()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, images + epsilon), images - epsilon).clamp(0,1)\n    model.train(was_training); return adv.detach()\n\ndef hybrid_fgsm_pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, refinement_steps=5):\n    was_training = model.training; model.eval()\n    base = images.detach().clone().to(device); labels = labels.to(device)\n    adv = fgsm_attack(base, labels, model, epsilon)\n    for _ in range(refinement_steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, base + epsilon), base - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\n# --------------------- Eval / Bench ---------------------\n@torch.no_grad()\ndef evaluate(model, loader, attack=None, return_loss=False, collect_preds=False):\n    model.eval(); correct=0; total=0; run_loss=0.0\n    y_true=[]; y_pred=[]\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n        data = ims\n        if   attack==\"FGSM\":  torch.set_grad_enabled(True); data = fgsm_attack(ims, lbs, model); torch.set_grad_enabled(False)\n        elif attack==\"PGD\":   torch.set_grad_enabled(True); data = pgd_attack(model, ims, lbs);  torch.set_grad_enabled(False)\n        elif attack==\"BIM\":   torch.set_grad_enabled(True); data = bim_attack(model, ims, lbs);  torch.set_grad_enabled(False)\n        elif attack==\"Hybrid_FGSM_PGD\":\n            torch.set_grad_enabled(True); data = hybrid_fgsm_pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        with autocast('cuda'):\n            logits = model(data); loss = F.cross_entropy(logits, lbs); preds = logits.argmax(1)\n        correct += (preds==lbs).sum().item(); total += lbs.size(0); run_loss += loss.item()\n        if collect_preds:\n            y_true.extend(lbs.detach().cpu().tolist()); y_pred.extend(preds.detach().cpu().tolist())\n    acc = correct / max(total,1)\n    if collect_preds:\n        return (acc, run_loss / max(len(loader),1), y_true, y_pred)\n    return (acc, run_loss / max(len(loader),1)) if return_loss else acc\n\n@torch.no_grad()\ndef benchmark_inference(model, loader, max_batches=20, warmup_batches=3):\n    model.eval(); total_imgs=0; total_time=0.0\n    # warmup\n    for i,(ims,_) in enumerate(loader):\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        with autocast('cuda'): _ = model(ims)\n        if i+1>=warmup_batches: break\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    _sync(); measured=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        _sync(); t0=time.time()\n        with autocast('cuda'): _ = model(ims)\n        _sync(); dt=time.time()-t0\n        total_time += dt; total_imgs += ims.size(0); measured+=1\n        if measured>=max_batches: break\n    lat_ms = (total_time/max(1,measured))*1000.0\n    thr    = total_imgs / max(total_time,1e-9)\n    peak   = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"batches\":measured,\"images\":total_imgs,\"avg_batch_latency_ms\":lat_ms,\"throughput_img_per_s\":thr,\"peak_infer_mem_mb\":peak}\n\n# --------------------- Early stopping ---------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, restore_best=True):\n        self.patience=patience; self.min_delta=min_delta; self.restore_best=restore_best\n        self.best=math.inf; self.bad=0; self.best_state=None\n    def step(self, val_loss, model):\n        if (self.best - val_loss) > self.min_delta:\n            self.best = val_loss; self.bad=0\n            if self.restore_best:\n                self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n            return False\n        else:\n            self.bad+=1; return self.bad>self.patience\n    def restore(self, model):\n        if self.restore_best and self.best_state is not None: model.load_state_dict(self.best_state)\n\n# --------------------- Train loop (PGD + BIM mix) ---------------------\ndef train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    model.train()\n    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n    scaler = GradScaler('cuda')\n    stopper = EarlyStopper()\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    hist=[]\n    for ep in range(1,epochs+1):\n        model.train(); run_loss=0.0; correct=0; total=0\n        for bi,(ims,lbs) in enumerate(train_loader):\n            ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n            adv_pgd = pgd_attack(model, ims, lbs)\n            adv_bim = bim_attack(model, ims, lbs)\n            mix_x = torch.cat([ims, adv_pgd, adv_bim], dim=0)\n            mix_y = torch.cat([lbs, lbs, lbs], dim=0)\n            opt.zero_grad(set_to_none=True)\n            with autocast('cuda'): logits = model(mix_x); loss = loss_fn(logits, mix_y)\n            preds = logits.argmax(1); correct += (preds==mix_y).sum().item(); total += mix_y.size(0)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n            run_loss += loss.item()\n            if bi%50==0: print(f\"Epoch {ep}/{epochs} | Batch {bi}/{len(train_loader)} | Loss {loss.item():.4f}\")\n        train_loss = run_loss/len(train_loader); train_acc = correct/max(total,1)\n        val_acc, val_loss = evaluate(model, val_loader, attack=None, return_loss=True)\n        print(f\"‚úÖ Epoch {ep}/{epochs} | TrainLoss {train_loss:.4f} | TrainAcc(mixed) {train_acc*100:.2f}% | ValLoss {val_loss:.4f} | ValAcc {val_acc*100:.2f}%\")\n        hist.append({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc_mixed\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc})\n        if stopper.step(val_loss, model):\n            print(f\"‚õî Early stopping at epoch {ep}\")\n            break\n    stopper.restore(model)\n    peak_train_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"train_peak_mem_mb\":peak_train_mb, \"history\":hist}\n\n# --------------------- BEFORE training (TEST) ---------------------\nprint_memory(\"Before Training\")\nprint(\"\\nüîç BEFORE adversarial training (TEST set)...\")\nclean_b, _, y_true_b, y_pred_b = evaluate(model, test_loader, attack=None, return_loss=True, collect_preds=True)\nfgsm_b  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_b   = evaluate(model, test_loader, attack=\"PGD\")\nbim_b   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_b   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_b = benchmark_inference(model, test_loader, max_batches=20)\nprec_b, rec_b, f1_b, _ = precision_recall_fscore_support(y_true_b, y_pred_b, average='weighted')\nreport_b = classification_report(y_true_b, y_pred_b, digits=4)\n\nprint(f\"üìä BEFORE (TEST): Clean {clean_b*100:.2f}% | FGSM {fgsm_b*100:.2f}% | PGD {pgd_b*100:.2f}% | BIM {bim_b*100:.2f}% | Hybrid {hyb_b*100:.2f}%\")\nprint(f\"‚ö° Inference BEFORE: latency {infer_b['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_b['throughput_img_per_s']:.2f} img/s | peak mem {infer_b['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Train (PGD + BIM mixed) ---------------------\nprint(\"\\nüöÄ Training (PGD + BIM mixed) with validation & early stopping...\")\nt0 = time.time()\ntrain_metrics = train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\ntrain_time = time.time() - t0\nprint(f\"‚è±Ô∏è Training time: {train_time:.2f} s\")\nprint(f\"üß† Peak training memory: {train_metrics['train_peak_mem_mb']:.2f} MB\")\nprint_memory(\"After Training\")\n\n# --------------------- AFTER training ---------------------\ntrain_acc_after = evaluate(model, train_loader, attack=None)\nval_acc_after   = evaluate(model, val_loader,   attack=None)\nprint(f\"\\nüéØ AFTER Training ‚Üí Train(clean): {train_acc_after*100:.2f}% | Val(clean,1000): {val_acc_after*100:.2f}%\")\n\nprint(\"\\nüîç AFTER adversarial training (TEST set)...\")\nclean_a, _, y_true_a, y_pred_a = evaluate(model, test_loader, attack=None, return_loss=True, collect_preds=True)\nfgsm_a  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_a   = evaluate(model, test_loader, attack=\"PGD\")\nbim_a   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_a   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_a = benchmark_inference(model, test_loader, max_batches=20)\nprec_a, rec_a, f1_a, _ = precision_recall_fscore_support(y_true_a, y_pred_a, average='weighted')\nreport_a = classification_report(y_true_a, y_pred_a, digits=4)\n\nprint(f\"üìä AFTER  (TEST): Clean {clean_a*100:.2f}% | FGSM {fgsm_a*100:.2f}% | PGD {pgd_a*100:.2f}% | BIM {bim_a*100:.2f}% | Hybrid {hyb_a*100:.2f}%\")\nprint(f\"‚ö° Inference AFTER: latency {infer_a['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_a['throughput_img_per_s']:.2f} img/s | peak mem {infer_a['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Tables & Saves (incl. CSV) ---------------------\ndef success_rate(acc): return (1.0 - acc) * 100.0\ndef robustness_gap(clean_acc, adv_acc): return (clean_acc - adv_acc) * 100.0\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Before Adversarial Training\")\nprint(\"-\"*80)\nprint(f\"Clean Accuracy                : {clean_b*100:.2f}%\")\nprint(f\"FGSM accuracy                 : {fgsm_b*100:.2f}%\")\nprint(f\"PGD accuracy                  : {pgd_b*100:.2f}%\")\nprint(f\"BIM accuracy                  : {bim_b*100:.2f}%\")\nprint(f\"Hybrid(FGSM‚ÜíPGD) accuracy     : {hyb_b*100:.2f}%\")\nprint(f\"FGSM Success Rate             : {success_rate(fgsm_b):.2f}%\")\nprint(f\"PGD  Success Rate             : {success_rate(pgd_b):.2f}%\")\nprint(f\"BIM  Success Rate             : {success_rate(bim_b):.2f}%\")\nprint(f\"Hybrid Success Rate           : {success_rate(hyb_b):.2f}%\")\nprint(f\"FGSM Robustness Gap           : {robustness_gap(clean_b, fgsm_b):.2f}%\")\nprint(f\"PGD  Robustness Gap           : {robustness_gap(clean_b, pgd_b):.2f}%\")\nprint(f\"BIM  Robustness Gap           : {robustness_gap(clean_b, bim_b):.2f}%\")\nprint(f\"Hybrid Robustness Gap         : {robustness_gap(clean_b, hyb_b):.2f}%\")\n\nprint(\"\\nAfter Adversarial Training\")\nprint(\"-\"*80)\nprint(f\"Test Clean Accuracy           : {clean_a*100:.2f}%\")\nprint(f\"FGSM accuracy                 : {fgsm_a*100:.2f}%\")\nprint(f\"PGD accuracy                  : {pgd_a*100:.2f}%\")\nprint(f\"BIM accuracy                  : {bim_a*100:.2f}%\")\nprint(f\"Hybrid(FGSM‚ÜíPGD) accuracy     : {hyb_a*100:.2f}%\")\nprint(f\"FGSM Success Rate             : {success_rate(fgsm_a):.2f}%\")\nprint(f\"PGD  Success Rate             : {success_rate(pgd_a):.2f}%\")\nprint(f\"BIM  Success Rate             : {success_rate(bim_a):.2f}%\")\nprint(f\"Hybrid Success Rate           : {success_rate(hyb_a):.2f}%\")\nprint(f\"FGSM Robustness Gap           : {robustness_gap(clean_a, fgsm_a):.2f}%\")\nprint(f\"PGD  Robustness Gap           : {robustness_gap(clean_a, pgd_a):.2f}%\")\nprint(f\"BIM  Robustness Gap           : {robustness_gap(clean_a, bim_a):.2f}%\")\nprint(f\"Hybrid Robustness Gap         : {robustness_gap(clean_a, hyb_a):.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Full Metrics (After Adv Training)\")\nprint(\"=\"*80)\nprint(f\"Precision                    : {prec_a*100:.1f}%\")\nprint(f\"Recall                       : {rec_a*100:.1f}%\")\nprint(f\"F1                           : {f1_a*100:.1f}%\")\nprint(f\"MACs (FLOPs/2)               : {MACs/1e9:.3f} G\")\nprint(f\"FLOPs (‚âà2√óMACs)              : {FLOPs/1e9:.3f} G\")\nprint(f\"Parameters (M)               : {PARAMS/1e6:.2f}\")\nprint(f\"Training time (sec)          : {train_time:.0f}\")\nprint(f\"Inference Time (sec)         : {infer_a['avg_batch_latency_ms']/1000.0:.3f} per batch\")\nprint(f\"Inference Mem usage GPU (MB) : {infer_a['peak_infer_mem_mb']:.2f}\")\nprint(f\"Training Mem usage GPU (MB)  : {train_metrics['train_peak_mem_mb']:.2f}\")\nprint(f\"Model size (MB)              : {MODEL_SIZE_MB:.1f}\")\n\nprint(\"\\nŒî Improvements (After - Before)\")\nprint(\"-\"*80)\nprint(f\"FGSM Acc Œî                   : {(fgsm_a - fgsm_b)*100.0:.2f} pp\")\nprint(f\"PGD  Acc Œî                   : {(pgd_a  - pgd_b )*100.0:.2f} pp\")\nprint(f\"BIM  Acc Œî                   : {(bim_a  - bim_b )*100.0:.2f} pp\")\nprint(f\"Hybrid Acc Œî                 : {(hyb_a  - hyb_b )*100.0:.2f} pp\")\n\n# -------- Save artifacts --------\nos.makedirs(\"/kaggle/working\", exist_ok=True)\ntorch.save(model, f\"/kaggle/working/{SAVE_PREFIX}.pth\")\ntorch.save(model.state_dict(), f\"/kaggle/working/{SAVE_PREFIX}_weights.pth\")\n\n# CSV rows & save\nbefore_row = {\n    \"phase\":\"BEFORE\",\n    \"clean_acc\": round(clean_b*100,4),\n    \"fgsm_acc\":  round(fgsm_b*100,4),\n    \"pgd_acc\":   round(pgd_b*100,4),\n    \"bim_acc\":   round(bim_b*100,4),\n    \"hybrid_acc\":round(hyb_b*100,4),\n    \"fgsm_success_rate\": round((1.0 - fgsm_b)*100,4),\n    \"pgd_success_rate\":  round((1.0 - pgd_b)*100,4),\n    \"bim_success_rate\":  round((1.0 - bim_b)*100,4),\n    \"hybrid_success_rate\": round((1.0 - hyb_b)*100,4),\n    \"fgsm_gap\": round((clean_b - fgsm_b)*100,4),\n    \"pgd_gap\":  round((clean_b - pgd_b)*100,4),\n    \"bim_gap\":  round((clean_b - bim_b)*100,4),\n    \"hybrid_gap\": round((clean_b - hyb_b)*100,4),\n    \"latency_ms_per_batch\": round(infer_b['avg_batch_latency_ms'],4),\n    \"throughput_img_per_s\": round(infer_b['throughput_img_per_s'],4),\n    \"peak_infer_mem_mb\": round(infer_b['peak_infer_mem_mb'],2),\n    \"macs_g\": round(MACs/1e9,6),\n    \"flops_g\": round((MACs*2)/1e9,6),\n    \"params_m\": round(PARAMS/1e6,6),\n    \"model_size_mb\": round(MODEL_SIZE_MB,3),\n    \"fine_tune_time_s\": 0.0,\n    \"train_peak_mem_mb\": 0.0,\n    \"model_name\": \"mobilevit_xxs\",\n    \"attack_eps\": EPSILON, \"attack_alpha\": ALPHA, \"pgd_steps\": PGD_STEPS, \"bim_steps\": BIM_STEPS,\n    \"label_smoothing\": LABEL_SMOOTH, \"weight_decay\": WEIGHT_DECAY, \"T\": ATTN_T\n}\nafter_row = {\n    \"phase\":\"AFTER\",\n    \"clean_acc\": round(clean_a*100,4),\n    \"fgsm_acc\":  round(fgsm_a*100,4),\n    \"pgd_acc\":   round(pgd_a*100,4),\n    \"bim_acc\":   round(bim_a*100,4),\n    \"hybrid_acc\":round(hyb_a*100,4),\n    \"fgsm_success_rate\": round((1.0 - fgsm_a)*100,4),\n    \"pgd_success_rate\":  round((1.0 - pgd_a)*100,4),\n    \"bim_success_rate\":  round((1.0 - bim_a)*100,4),\n    \"hybrid_success_rate\": round((1.0 - hyb_a)*100,4),\n    \"fgsm_gap\": round((clean_a - fgsm_a)*100,4),\n    \"pgd_gap\":  round((clean_a - pgd_a)*100,4),\n    \"bim_gap\":  round((clean_a - bim_a)*100,4),\n    \"hybrid_gap\": round((clean_a - hyb_a)*100,4),\n    \"latency_ms_per_batch\": round(infer_a['avg_batch_latency_ms'],4),\n    \"throughput_img_per_s\": round(infer_a['throughput_img_per_s'],4),\n    \"peak_infer_mem_mb\": round(infer_a['peak_infer_mem_mb'],2),\n    \"macs_g\": round(MACs/1e9,6),\n    \"flops_g\": round((MACs*2)/1e9,6),\n    \"params_m\": round(PARAMS/1e6,6),\n    \"model_size_mb\": round(MODEL_SIZE_MB,3),\n    \"fine_tune_time_s\": round(train_time,2),\n    \"train_peak_mem_mb\": round(train_metrics['train_peak_mem_mb'],2),\n    \"model_name\": \"mobilevit_xxs\",\n    \"attack_eps\": EPSILON, \"attack_alpha\": ALPHA, \"pgd_steps\": PGD_STEPS, \"bim_steps\": BIM_STEPS,\n    \"label_smoothing\": LABEL_SMOOTH, \"weight_decay\": WEIGHT_DECAY, \"T\": ATTN_T\n}\ncsv_fields = list(after_row.keys())\nwrite_metrics_csv(f\"/kaggle/working/{SAVE_PREFIX}_results.csv\", [before_row, after_row], csv_fields)\n\n# Save classification reports\nPath(f\"/kaggle/working/classification_report_mobilevit_before_MATCHCODE1.txt\").write_text(\n    classification_report(y_true_b, y_pred_b, digits=4)\n)\nPath(f\"/kaggle/working/classification_report_mobilevit_after_MATCHCODE1.txt\").write_text(\n    classification_report(y_true_a, y_pred_a, digits=4)\n)\n\nprint(\"\\nüíæ Saved:\")\nprint(f\" - /kaggle/working/{SAVE_PREFIX}.pth\")\nprint(f\" - /kaggle/working/{SAVE_PREFIX}_weights.pth\")\nprint(f\" - /kaggle/working/{SAVE_PREFIX}_results.csv\")\nprint(f\" - /kaggle/working/classification_report_mobilevit_before_MATCHCODE1.txt\")\nprint(f\" - /kaggle/working/classification_report_mobilevit_after_MATCHCODE1.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# small -vit clean training pipeline","metadata":{}},{"cell_type":"code","source":"!pip install -q timm scikit-learn ptflops\n\nimport os, random, time, gc, math, copy\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom timm.scheduler import CosineLRScheduler\nfrom torch.amp import GradScaler, autocast\n\n# -----------------------------\n# üîß Reproducibility\n# -----------------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------\n# ‚öôÔ∏è Validation / Tuning knobs\n# -----------------------------\nVAL_EVERY_N_EPOCHS = 1         # run lightweight val every N epochs\nVAL_MAX_BATCHES     = 6        # validate on at most this many batches (keep small)\nPATIENCE            = 3        # early stopping patience (epochs)\nMIN_DELTA           = 1e-3     # minimum improvement in val loss to reset patience\n\nENABLE_TUNING       = True     # pilot sweep to pick LR/WD quickly\nTUNE_EPOCHS         = 5        # 5 epochs per candidate\nTUNE_TRAIN_FRAC     = 0.08     # use 8% of train for quick tuning\nTUNE_VAL_MAX_BATCH  = 3        # validate only a handful of batches in tuning\n\n# -------------------------------------------------------\n# üìÅ Data root ‚Äî your Kaggle dataset\n# -------------------------------------------------------\ndata_root = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\ntrain_dir = os.path.join(data_root, \"train\")\nval_dir   = os.path.join(data_root, \"val\")\ntest_dir  = os.path.join(data_root, \"test\")\n\n# -------------------------------------------------------\n# üñºÔ∏è Transforms\n# -------------------------------------------------------\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\nval_test_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n])\n\n# -------------------------------------------------------\n# üì¶ Build unified sample list then 70/15/15 split (stratified)\n# -------------------------------------------------------\nIMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\")\ndef list_class_dirs(parent): return sorted([d for d in os.listdir(parent) if os.path.isdir(os.path.join(parent, d))])\n\nif os.path.exists(train_dir) and len(list_class_dirs(train_dir)) > 0:\n    classes = list_class_dirs(train_dir)\nelif os.path.exists(val_dir) and len(list_class_dirs(val_dir)) > 0:\n    classes = list_class_dirs(val_dir)\nelse:\n    classes = list_class_dirs(test_dir)\nclass_to_idx = {c: i for i, c in enumerate(classes)}\n\ndef gather_samples(split_dir):\n    samples = []\n    for cls in classes:\n        cdir = os.path.join(split_dir, cls)\n        if not os.path.isdir(cdir): continue\n        for root, _, files in os.walk(cdir):\n            for f in files:\n                if f.lower().endswith(IMG_EXTS):\n                    samples.append((os.path.join(root, f), class_to_idx[cls]))\n    return samples\n\nall_samples = []\nfor d in [train_dir, val_dir, test_dir]:\n    all_samples.extend(gather_samples(d))\nassert len(all_samples) > 0, \"No images found. Check dataset paths.\"\n\nlabels  = np.array([lbl for _, lbl in all_samples])\nindices = np.arange(len(all_samples))\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsss1 = StratifiedShuffleSplit(n_splits=1, train_size=0.70, test_size=0.30, random_state=42)\ntrain_idx, temp_idx = next(sss1.split(indices, labels))\ntemp_labels = labels[temp_idx]\nsss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.50, test_size=0.50, random_state=42)\nval_rel_idx, test_rel_idx = next(sss2.split(temp_idx, temp_labels))\nval_idx, test_idx = temp_idx[val_rel_idx], temp_idx[test_rel_idx]\n\ndef take(idxs): return [all_samples[i] for i in idxs]\ntrain_samples, val_samples, test_samples = take(train_idx), take(val_idx), take(test_idx)\n\nprint(f\"‚úÖ Split sizes ‚Äî Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")\nprint(f\"    Classes ({len(classes)}): {classes}\")\n\n# -------------------------------------------------------\n# üß∞ Dataset from file paths\n# -------------------------------------------------------\nclass PathImageDataset(Dataset):\n    def __init__(self, samples, classes, transform=None):\n        self.samples = samples; self.classes = classes; self.transform = transform\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        path, target = self.samples[idx]\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img, target\n\ntrain_dataset = PathImageDataset(train_samples, classes, transform=train_transforms)\nval_dataset   = PathImageDataset(val_samples,   classes, transform=val_test_transforms)\ntest_dataset  = PathImageDataset(test_samples,  classes, transform=val_test_transforms)\n\ndef make_loader(dataset, batch_size=32, shuffle=False):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ntrain_loader = make_loader(train_dataset, batch_size=32, shuffle=True)\nval_loader   = make_loader(val_dataset,   batch_size=32, shuffle=False)\ntest_loader  = make_loader(test_dataset,  batch_size=32, shuffle=False)\n\n# -------------------------------------------------------\n# üî• Attention wrapper (temperature scaling)\n# -------------------------------------------------------\nfrom timm.models.vision_transformer import Attention\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv        = base_attn.qkv\n        self.proj       = base_attn.proj\n        self.proj_drop  = base_attn.proj_drop\n        self.attn_drop  = base_attn.attn_drop\n        self.num_heads  = base_attn.num_heads\n        self.q_norm     = getattr(base_attn, 'q_norm', None)\n        self.k_norm     = getattr(base_attn, 'k_norm', None)\n        self.scale      = base_attn.scale\n        self.temperature= float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2,0,3,1,4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n        attn_logits = (q @ k.transpose(-2,-1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad(): self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n        attn = attn_logits.softmax(dim=-1); attn = self.attn_drop(attn)\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\ndef apply_temperature_to_vit(model, T=1.5):\n    for _, blk in enumerate(model.blocks):\n        blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\n# -------------------------------------------------------\n# ‚úÖ Post-attention LayerNorm (safe block wrapper)\n# -------------------------------------------------------\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1, self.ls2 = getattr(blk,\"ls1\",None), getattr(blk,\"ls2\",None)\n        self.gamma_1, self.gamma_2 = getattr(blk,\"gamma_1\",None), getattr(blk,\"gamma_2\",None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, gamma): return ls(x) if ls is not None else (gamma*x if gamma is not None else x)\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1)\n        x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2)\n        x = x + self._dp(y, self.dp2)\n        return x\n\ndef add_post_attn_layernorm_safe(model, eps=1e-6):\n    device = next(model.parameters()).device\n    new_blocks = [PostAttnLNBlock(blk, eps=eps).to(device) for blk in model.blocks]\n    model.blocks = nn.Sequential(*new_blocks)\n\n# -------------------------------------------------------\n# üß† Build model + patches  (‚üµ CHANGED to ViT-Small)\n# -------------------------------------------------------\nnum_classes = len(classes)\nmodel = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=num_classes).to(device)\n\n# Quick sanity print for S/16 spec:\nprint(\"\\nüîé ViT-Small spec check:\")\nprint(f\"  depth={len(model.blocks)} (expected 12)\")\nprint(f\"  embed_dim={model.embed_dim} (expected 384)\")\nprint(f\"  num_heads={model.blocks[0].attn.num_heads} (expected 6)\")\n# mlp hidden dim is embed_dim * mlp_ratio -> expected 1536\nfirst_mlp_hidden = model.blocks[0].mlp.fc1.out_features\nprint(f\"  mlp_hidden_dim‚âà{first_mlp_hidden} (expected 1536)\")\n\nATTN_T = 1.5\napply_temperature_to_vit(model, T=ATTN_T)\nadd_post_attn_layernorm_safe(model, eps=1e-6)\n\nLOGIT_CLIP = 0.0\n\n# -------------------------------------------------------\n# üìè FLOPs / Params helper (with safe fallback)\n# -------------------------------------------------------\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef flops_and_params(model, input_res=(3,224,224)):\n    try:\n        from ptflops import get_model_complexity_info\n        model_eval = copy.deepcopy(model).to('cpu').eval()\n        with torch.no_grad():\n            macs, params = get_model_complexity_info(model_eval, input_res, as_strings=False, verbose=False)\n        return macs, count_params(model)\n    except Exception as e:\n        return None, count_params(model)\n\n# -------------------------------------------------------\n# üß™ Training / Eval helpers (with LIGHT validation)\n# -------------------------------------------------------\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\ndef forward_with_optional_logit_clip(model, images):\n    logits = model(images)\n    if LOGIT_CLIP and LOGIT_CLIP > 0:\n        logits = torch.clamp(logits, min=-LOGIT_CLIP, max=LOGIT_CLIP)\n    return logits\n\ndef print_memory(label=\"\"):\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        alloc = torch.cuda.memory_allocated()/1024**2\n        reserv = torch.cuda.memory_reserved()/1024**2\n        print(f\"\\U0001f9e0 {label} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\n@torch.no_grad()\ndef report_attention_stats(model, loader, max_batches=1, tag=\"\"):\n    model.eval()\n    batches, entropies, max_abs_logits = 0, [], []\n    for images, _ in loader:\n        images = images.to(device)\n        with autocast('cuda'): _ = model(images)\n        blk_ents, blk_max = [], []\n        for blk in model.blocks:\n            attn = blk.attn\n            if isinstance(attn, TempScaledAttention) and attn.last_entropy is not None:\n                blk_ents.append(attn.last_entropy.numpy()); blk_max.append(attn.last_max_abs_pre_softmax)\n            else:\n                blk_ents.append(None); blk_max.append(None)\n        entropies.append(blk_ents); max_abs_logits.append(blk_max)\n        batches += 1\n        if batches >= max_batches: break\n    print(f\"\\nüîé Attention saturation ({tag}) ‚Äî T={ATTN_T}\")\n    for i, _ in enumerate(model.blocks):\n        head_arrays = [e[i] for e in entropies if e[i] is not None]\n        max_vals    = [m[i] for m in max_abs_logits if m[i] is not None]\n        if head_arrays:\n            H = np.stack(head_arrays, axis=0)\n            mean_ent, p10, p90 = H.mean(), np.percentile(H,10), np.percentile(H,90)\n            mx = np.mean(max_vals) if len(max_vals)>0 else float('nan')\n            print(f\"  Block {i:02d}: entropy mean={mean_ent:.3f}, p10={p10:.3f}, p90={p90:.3f} | max|pre-softmax|‚âà{mx:.3f}\")\n        else:\n            print(f\"  Block {i:02d}: (no stats)\")\n\ndef make_optimizer(model, lr=3e-4, weight_decay=0.01):\n    return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\ndef make_scheduler(optimizer, t_initial=10):\n    return CosineLRScheduler(optimizer, t_initial=t_initial, lr_min=1e-6, warmup_lr_init=1e-5, warmup_t=3)\n\ndef eval_light(model, loader, max_batches=VAL_MAX_BATCHES):\n    model.eval()\n    total_loss, correct, total, batches = 0.0, 0, 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n                loss   = criterion(logits, labels)\n            total_loss += loss.item()\n            correct    += (logits.argmax(1) == labels).sum().item()\n            total      += labels.size(0)\n            batches    += 1\n            if batches >= max_batches: break\n    avg_loss = total_loss / max(1, batches)\n    acc = 100.0 * correct / max(1, total)\n    return avg_loss, acc\n\ndef evaluate_full(model, loader, class_names):\n    model.eval()\n    correct, total = 0, 0\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n            y_true += labels.cpu().tolist(); y_pred += preds.cpu().tolist()\n    acc = 100.0 * correct / max(1,total)\n    report = classification_report(y_true, y_pred, target_names=class_names, digits=4)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n    return acc, report, (pr, rc, f1)\n\n# -----------------------------\n# üõë Early stopping helper\n# -----------------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience; self.min_delta = min_delta\n        self.best = float('inf'); self.count = 0\n    def step(self, val_loss):\n        if val_loss < self.best - self.min_delta:\n            self.best = val_loss; self.count = 0; return False\n        else:\n            self.count += 1; return self.count > self.patience\n\n# -----------------------------\n# üîç Tiny pilot hyperparam tuning (LR/WD)\n# -----------------------------\ndef make_subset(dataset, frac):\n    n = len(dataset); k = max(1, int(n*frac))\n    idx = np.random.RandomState(42).choice(n, size=k, replace=False)\n    return Subset(dataset, idx.tolist())\n\ndef pilot_tune(base_model, train_ds, val_loader, candidates, epochs=TUNE_EPOCHS):\n    print(\"\\nüß™ Tiny pilot tuning (subset) ...\")\n    best_cfg, best_score, best_loss = None, -1, float('inf')\n    for (lr, wd) in candidates:\n        model = copy.deepcopy(base_model).to(device)\n        opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n        scaler = GradScaler('cuda')\n        loader = make_loader(make_subset(train_ds, TUNE_TRAIN_FRAC), batch_size=32, shuffle=True)\n        for ep in range(epochs):\n            model.train()\n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                opt.zero_grad(set_to_none=True)\n                with autocast('cuda'):\n                    logits = forward_with_optional_logit_clip(model, images)\n                    loss   = criterion(logits, labels)\n                scaler.scale(loss).backward()\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(opt); scaler.update()\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=TUNE_VAL_MAX_BATCH)\n        print(f\"  LR={lr:.1e}, WD={wd:.3f} -> light ValAcc={val_acc:.2f}%, ValLoss={val_loss:.4f}\")\n        if (val_acc > best_score) or (abs(val_acc - best_score) < 1e-6 and val_loss < best_loss):\n            best_score, best_loss = val_acc, val_loss\n            best_cfg = (lr, wd)\n    print(f\"‚úÖ Chosen hyperparams: LR={best_cfg[0]:.1e}, WD={best_cfg[1]:.3f} (best light ValAcc={best_score:.2f}%)\")\n    return best_cfg\n\n# -------------------------------------------------------\n# üöÄ Train with LIGHT validation + Early Stopping\n# -------------------------------------------------------\ncandidates = [\n    (5e-5, 0.005), (5e-5, 0.01), (1e-4, 0.005), (1e-4, 0.01),\n    (2e-4, 0.005), (2e-4, 0.01), (3e-4, 0.01), (3e-4, 0.02),\n    (1e-4, 0.02), (3e-4, 0.05)\n]\n\nif ENABLE_TUNING:\n    chosen_lr, chosen_wd = pilot_tune(model, train_dataset, val_loader, candidates)\nelse:\n    chosen_lr, chosen_wd = 3e-4, 0.01\nprint(f\"\\n‚öôÔ∏è Using LR={chosen_lr:.1e}, WD={chosen_wd:.3f}\")\n\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=10)\nscaler    = GradScaler('cuda')\n\n# --- FLOPs/Params BEFORE pruning\nmacs_before, params_before = flops_and_params(model, input_res=(3,224,224))\nif macs_before is not None:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: {macs_before/1e9:.2f} G, Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"üìê BEFORE pruning ‚Äî MACs: N/A, Params: {params_before/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\nEPOCHS = 10\nearly = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA)\nbest_state, best_val_acc = None, -1\n\n# Track training time and peak memory\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\ntrain_t0 = time.time()\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n\n    train_loss = total_loss / max(1,len(train_loader))\n    train_acc_epoch  = 100.0 * correct / max(1,total)\n    scheduler.step(epoch)\n\n    if (epoch+1) % VAL_EVERY_N_EPOCHS == 0:\n        val_loss, val_acc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}% | \"\n              f\"Light Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%\")\n        report_attention_stats(model, val_loader, max_batches=1, tag=f\"epoch {epoch+1}\")\n\n        improve = (val_acc > best_val_acc + 1e-6) or (abs(val_acc - best_val_acc) < 1e-6 and (early.best - val_loss) > MIN_DELTA)\n        if improve:\n            best_val_acc = val_acc\n            best_state = copy.deepcopy(model.state_dict())\n\n        if early.step(val_loss):\n            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}. Best light ValAcc={best_val_acc:.2f}%\")\n            break\n    else:\n        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss {train_loss:.4f}, Acc {train_acc_epoch:.2f}%\")\n\ntrain_elapsed = time.time() - train_t0\ntrain_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Training\")\n\n# Restore best model (model selection)\nif best_state is not None:\n    model.load_state_dict(best_state)\n    print(\"üìå Restored best checkpoint by validation performance (light).\")\n\n# -------------------------------------------------------\n# üíæ Save BEFORE (architecture + weights)\n# -------------------------------------------------------\nbefore_arch_path = \"/kaggle/working/vit_small_before_architecture.pth\"\nbefore_wts_path  = \"/kaggle/working/vit_small_before_weights.pth\"\ntorch.save(model, before_arch_path)               # architecture + weights\ntorch.save(model.state_dict(), before_wts_path)   # weights only\nprint(f\"\\nüíæ Saved BEFORE model:\\n   architecture -> {before_arch_path}\\n   weights      -> {before_wts_path}\")\n\n# -------------------------------------------------------\n# ‚úÖ Full evaluation on TRAIN and TEST before pruning\n# -------------------------------------------------------\ndef measure_inference_performance(model, loader, tag=\"\"):\n    model.eval()\n    gc.collect(); torch.cuda.empty_cache()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\n    t0 = time.time()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            with autocast('cuda'):\n                logits = forward_with_optional_logit_clip(model, images)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total   += labels.size(0)\n    elapsed = time.time() - t0\n    peak = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\n    acc = 100.0 * correct / max(1,total)\n    print(f\"\\U0001f9e0 {tag} Inference - Acc: {acc:.2f}%, Time: {elapsed:.2f}s, Peak Memory: {peak:.2f} MB\")\n    return acc, elapsed, peak\n\ntrain_acc_full, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\ntest_acc_before, test_report_before, test_prf_before = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ BEFORE Pruning: Test Acc: {test_acc_before:.2f}%\")\nprint(test_report_before)\n_, test_time_before, test_peak_mem_before = measure_inference_performance(model, test_loader, tag=\"Before Pruning\")\n\n# --- Print consolidated BEFORE metrics\nprint(\"\\nüìä BEFORE Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_full:.2f}%\")\nprint(f\"  Test  Accuracy: {test_acc_before:.2f}%\")\nprint(f\"  Test  Macro Precision: {test_prf_before[0]*100:.2f}% | Recall: {test_prf_before[1]*100:.2f}% | F1: {test_prf_before[2]*100:.2f}%\")\nprint(f\"  Training Time: {train_elapsed:.2f}s | Training Peak Memory: {train_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_before:.2f}s | Testing  Peak Memory: {test_peak_mem_before:.2f} MB\")\nif macs_before is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_before/1e9:.2f} G | Params: {params_before/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_before/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# ‚úÇÔ∏è Block-importance pruning + brief fine-tune\n#   üîÅ PRUNE 6 LOW-IMPORTANCE BLOCKS (keep 6)\n# -------------------------------------------------------\ndef compute_block_importance(model, loader):\n    model.train()\n    scores = torch.zeros(len(model.blocks), device=device)\n    images, labels = next(iter(loader))  # single-batch approximation\n    images, labels = images.to(device), labels.to(device)\n    with autocast('cuda'):\n        logits = forward_with_optional_logit_clip(model, images)\n        loss   = criterion(logits, labels)\n    loss.backward()\n    for i, blk in enumerate(model.blocks):\n        s, c = 0.0, 0\n        for p in blk.parameters():\n            if p.grad is not None:\n                s += p.grad.abs().mean(); c += 1\n        if c>0: scores[i] = s/c\n    model.zero_grad(set_to_none=True)\n    return scores\n\ndef prune_transformer_blocks(model, indices_to_remove):\n    model.blocks = nn.Sequential(*[blk for i, blk in enumerate(model.blocks) if i not in indices_to_remove])\n\nscores = compute_block_importance(model, val_loader)\nprune_indices = torch.argsort(scores)[:6].tolist()\nprint(f\"\\n‚úÇÔ∏è Pruned Blocks (6): {prune_indices}\")\nprune_transformer_blocks(model, prune_indices)\n\n# FLOPs/params AFTER pruning\nmacs_after, params_after = flops_and_params(model, input_res=(3,224,224))\nif macs_after is not None:\n    print(f\"üìê AFTER pruning ‚Äî MACs: {macs_after/1e9:.2f} G, Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"üìê AFTER pruning ‚Äî MACs: N/A, Params: {params_after/1e6:.2f} M\")\n\nprint(\"\\nüîß Fine-tuning for 2 epochs after pruning (with light val checks)...\")\nprint_memory(\"Before Fine-tuning\")\noptimizer = make_optimizer(model, lr=chosen_lr, weight_decay=chosen_wd)\nscheduler = make_scheduler(optimizer, t_initial=2)\nscaler    = GradScaler('cuda')\n\nif torch.cuda.is_available():\n    torch.cuda.reset_peak_memory_stats(); torch.cuda.synchronize()\nft_t0 = time.time()\n\nfor epoch in range(2):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast('cuda'):\n            logits = forward_with_optional_logit_clip(model, images)\n            loss   = criterion(logits, labels)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer); scaler.update()\n        total_loss += loss.item()\n        correct    += (logits.argmax(1) == labels).sum().item()\n        total      += labels.size(0)\n    scheduler.step(epoch)\n    vloss, vacc = eval_light(model, val_loader, max_batches=VAL_MAX_BATCHES)\n    print(f\"[Fine-tune] Epoch {epoch+1}/2 - TrainLoss {total_loss/len(train_loader):.4f}, \"\n          f\"TrainAcc {100.0*correct/max(1,total):.2f}% | Light ValLoss {vloss:.4f}, ValAcc {vacc:.2f}%\")\n\nft_elapsed = time.time() - ft_t0\nft_peak_mem_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else None\nprint_memory(\"After Fine-tuning\")\n\n# -------------------------------------------------------\n# ‚úÖ Final TRAIN/TEST after pruning\n# -------------------------------------------------------\ntrain_acc_after, _, _ = evaluate_full(model, train_loader, class_names=classes)\n\npruned_test_acc, pruned_report, pruned_prf = evaluate_full(model, test_loader, class_names=classes)\nprint(f\"\\n‚úÖ AFTER Pruning: Test Acc: {pruned_test_acc:.2f}%\")\nprint(pruned_report)\n_, test_time_after, test_peak_mem_after = measure_inference_performance(model, test_loader, tag=\"After Pruning\")\n\nprint(\"\\nüìä AFTER Pruning ‚Äî Summary\")\nprint(f\"  Train Accuracy: {train_acc_after:.2f}%\")\nprint(f\"  Test  Accuracy: {pruned_test_acc:.2f}%\")\nprint(f\"  Test  Macro Precision: {pruned_prf[0]*100:.2f}% | Recall: {pruned_prf[1]*100:.2f}% | F1: {pruned_prf[2]*100:.2f}%\")\nprint(f\"  Fine-tune Time: {ft_elapsed:.2f}s | Fine-tune Peak Memory: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"  Testing  Time: {test_time_after:.2f}s | Testing  Peak Memory: {test_peak_mem_after:.2f} MB\")\nif macs_after is not None:\n    print(f\"  MACs (‚âà FLOPs/2): {macs_after/1e9:.2f} G | Params: {params_after/1e6:.2f} M\")\nelse:\n    print(f\"  MACs: N/A | Params: {params_after/1e6:.2f} M\")\n\n# -------------------------------------------------------\n# üíæ Save AFTER (architecture + weights)\n# -------------------------------------------------------\nafter_arch_path = \"/kaggle/working/vit_small_after_architecture.pth\"\nafter_wts_path  = \"/kaggle/working/vit_small_after_weights.pth\"\ntorch.save(model, after_arch_path)               # architecture + weights (includes pruned blocks)\ntorch.save(model.state_dict(), after_wts_path)   # weights only\nprint(f\"\\nüíæ Saved AFTER model:\\n   architecture -> {after_arch_path}\\n   weights      -> {after_wts_path}\")\n\n# -------------------------------------------------------\n# üßæ Final Overall Summary\n# -------------------------------------------------------\nprint(\"\\n================ FINAL SUMMARY ================\")\nprint(f\"Blocks pruned: 6 (kept {len(model.blocks)} blocks)\")\nif (macs_before is not None) and (macs_after is not None):\n    macs_delta = (macs_before - macs_after)/1e9\n    print(f\"MACs change: {macs_before/1e9:.2f} G -> {macs_after/1e9:.2f} G (Œî {macs_delta:.2f} G)\")\nelse:\n    print(\"MACs change: N/A\")\nprint(f\"Params change: {params_before/1e6:.2f} M -> {params_after/1e6:.2f} M (Œî {(params_before-params_after)/1e6:.2f} M)\")\nprint(f\"Train time: {train_elapsed:.2f}s | Train peak mem: {train_peak_mem_mb:.2f} MB\")\nprint(f\"Fine-tune time: {ft_elapsed:.2f}s | Fine-tune peak mem: {ft_peak_mem_mb:.2f} MB\")\nprint(f\"Test(before) acc: {test_acc_before:.2f}% | Test(after) acc: {pruned_test_acc:.2f}%\")\nprint(\"===============================================\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"adversarial training pipeline and evaluation the originial architecture before pruning","metadata":{}},{"cell_type":"code","source":"!pip install -q timm fvcore ptflops\n\nimport os, time, gc, random, json, math, collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.amp import autocast, GradScaler\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Subset\n\nfrom timm import create_model\nfrom timm.models.vision_transformer import VisionTransformer, Attention\nfrom timm.layers.patch_embed import PatchEmbed\nfrom timm.layers.format import Format\nfrom torch.nn import Identity, Conv2d\nfrom torch.serialization import add_safe_globals\n\nfrom fvcore.nn import FlopCountAnalysis\nfrom ptflops import get_model_complexity_info\n\n# --------------------- Config ---------------------\nNUM_CLASSES   = 8\nIMG_SIZE      = 224\nATTN_T        = 1.5\nPOST_LN_EPS   = 1e-6\nBATCH_SIZE    = 32\nNUM_WORKERS   = min(8, os.cpu_count())\nMAX_GRAD_NORM = 1.0\n\nDATA_DIR      = \"/kaggle/input/retinal-oct-c8/RetinalOCT_Dataset/RetinalOCT_Dataset\"\nVAL_DIR_NAME  = \"val\"  # your folder is 'val', not 'validation'\n\nTS_MODEL_PATH = \"/kaggle/input/smallvitnoadv/pytorch/default/1/vit_small_before_architecture.pth\"  # pickled full model (not TorchScript)\nWEIGHTS_PATH  = \"/kaggle/input/smallvitnoadvweights/pytorch/default/1/vit_small_before_weights.pth\"\n\n# Adversarial params\nEPSILON    = 8/255\nALPHA      = 2/255\nPGD_STEPS  = 7\nBIM_STEPS  = 10\n\n# Train config\nEPOCHS      = 6\nLR          = 1e-4\nPATIENCE    = 2\nMIN_DELTA   = 0.0\n\n# Optional: force-prune loaded model to N blocks (e.g., 6). Keep None to use the checkpoint's block count.\nFORCE_KEEP_BLOCKS = None  # e.g., 6\n\n# --------------------- Utils ---------------------\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\ndef print_memory(tag=\"\"):\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserv = torch.cuda.memory_reserved() / 1024**2\n        print(f\"üß† {tag} - Allocated: {alloc:.2f} MB | Reserved: {reserv:.2f} MB\")\n\ndef _sync():\n    if torch.cuda.is_available(): torch.cuda.synchronize()\n\n# --------------------- Anti-saturation blocks ---------------------\nclass TempScaledAttention(nn.Module):\n    def __init__(self, base_attn: Attention, temperature: float = 1.5):\n        super().__init__()\n        self.qkv = base_attn.qkv\n        self.proj = base_attn.proj\n        self.proj_drop = base_attn.proj_drop\n        self.attn_drop = base_attn.attn_drop\n        self.num_heads = base_attn.num_heads\n        self.q_norm = getattr(base_attn, 'q_norm', None)\n        self.k_norm = getattr(base_attn, 'k_norm', None)\n        self.scale = base_attn.scale\n        self.temperature = float(temperature)\n        self.last_max_abs_pre_softmax = None\n        self.last_entropy = None\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.q_norm is not None: q = self.q_norm(q)\n        if self.k_norm is not None: k = self.k_norm(k)\n\n        attn_logits = (q @ k.transpose(-2, -1)) * self.scale\n        attn_logits = attn_logits / self.temperature\n        with torch.no_grad():\n            self.last_max_abs_pre_softmax = attn_logits.abs().amax().item()\n\n        attn = attn_logits.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        with torch.no_grad():\n            p = attn.clamp_min(1e-12)\n            ent = -(p * p.log()).sum(dim=-1).mean(dim=-1)\n            self.last_entropy = ent.mean(dim=0).detach().cpu()\n        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n        x = self.proj(x); x = self.proj_drop(x)\n        return x\n\nclass PostAttnLNBlock(nn.Module):\n    def __init__(self, blk: nn.Module, eps=1e-6):\n        super().__init__()\n        self.norm1, self.norm2 = blk.norm1, blk.norm2\n        self.attn, self.mlp   = blk.attn, blk.mlp\n        self.dp1 = getattr(blk, \"drop_path1\", None)\n        self.dp2 = getattr(blk, \"drop_path2\", None)\n        if self.dp1 is None and self.dp2 is None:\n            dp = getattr(blk, \"drop_path\", None); self.dp1, self.dp2 = dp, dp\n        self.ls1 = getattr(blk, \"ls1\", None); self.ls2 = getattr(blk, \"ls2\", None)\n        self.gamma_1 = getattr(blk, \"gamma_1\", None); self.gamma_2 = getattr(blk, \"gamma_2\", None)\n        embed_dim = self.norm1.normalized_shape[0]\n        self.post_ln = nn.LayerNorm(embed_dim, eps=eps)\n\n    def _dp(self, x, dp): return dp(x) if dp is not None else x\n    def _ls(self, x, ls, g): return ls(x) if ls is not None else (g * x if g is not None else x)\n\n    def forward(self, x):\n        y = self.attn(self.norm1(x)); y = self._ls(y, self.ls1, self.gamma_1); x = x + self._dp(y, self.dp1)\n        x = self.post_ln(x)\n        y = self.mlp(self.norm2(x)); y = self._ls(y, self.ls2, self.gamma_2); x = x + self._dp(y, self.dp2)\n        return x\n\ndef apply_temperature_to_vit(v: VisionTransformer, T=1.5):\n    for blk in v.blocks: blk.attn = TempScaledAttention(blk.attn, temperature=T)\n\ndef add_post_attn_layernorm_safe(v: VisionTransformer, eps=1e-6):\n    dev = next(v.parameters()).device\n    v.blocks = nn.Sequential(*[PostAttnLNBlock(blk, eps=eps).to(dev) for blk in v.blocks])\n\ndef prune_vit_to_n_blocks(v: VisionTransformer, n_keep: int):\n    v.blocks = nn.Sequential(*[blk for i, blk in enumerate(v.blocks) if i < n_keep])\n\n# --------------------- Model loading ---------------------\ndef detect_num_blocks_from_state(state_dict):\n    max_idx = -1\n    for k in state_dict.keys():\n        if k.startswith(\"blocks.\") and k[7:].split('.',1)[0].isdigit():\n            idx = int(k[7:].split('.',1)[0]); max_idx = max(max_idx, idx)\n    return max_idx + 1 if max_idx >= 0 else None\n\ndef _infer_embed_dim_from_sd(sd: dict):\n    if \"cls_token\" in sd:\n        return sd[\"cls_token\"].shape[-1]\n    if \"pos_embed\" in sd:\n        return sd[\"pos_embed\"].shape[-1]\n    if \"patch_embed.proj.weight\" in sd:\n        return sd[\"patch_embed.proj.weight\"].shape[0]\n    return None\n\ndef _pick_timm_from_dim(d):\n    if d == 192:  return \"vit_tiny_patch16_224\"\n    if d == 384:  return \"vit_small_patch16_224\"\n    if d == 768:  return \"vit_base_patch16_224\"\n    if d == 1024: return \"vit_large_patch16_224\"\n    return \"vit_base_patch16_224\"\n\ndef _filter_load_state_dict(model, sd):\n    msd = model.state_dict()\n    ok = {k:v for k,v in sd.items() if k in msd and msd[k].shape == v.shape}\n    missing = [k for k in msd.keys() if k not in ok]\n    unexpected = [k for k in sd.keys() if k not in msd]\n    model.load_state_dict(ok, strict=False)\n    print(f\"‚úÖ Loaded {len(ok)} matching tensors | ‚ö†Ô∏è skipped {len(missing)} missing + {len(unexpected)} unexpected due to shape/name\")\n    if missing[:5]:    print(\"   missing (first 5):\", missing[:5])\n    if unexpected[:5]: print(\"   unexpected (first 5):\", unexpected[:5])\n\ndef build_model_from_weights(weights_path, num_classes=NUM_CLASSES):\n    print(f\"üì¶ Loading weights from: {weights_path}\")\n    sd = torch.load(weights_path, map_location=\"cpu\")\n    if \"state_dict\" in sd and isinstance(sd[\"state_dict\"], dict):\n        sd = sd[\"state_dict\"]\n\n    d = _infer_embed_dim_from_sd(sd)\n    if d is None:\n        raise RuntimeError(\"Could not infer embed dim from checkpoint.\")\n    model_name = _pick_timm_from_dim(d)\n    print(f\"üîé Inferred embed_dim={d} ‚Üí using timm model: {model_name}\")\n\n    n_blocks = detect_num_blocks_from_state(sd) or 12\n    print(f\"üîß Detected block count in weights: {n_blocks}\")\n\n    m = create_model(model_name, pretrained=False, num_classes=num_classes)\n    apply_temperature_to_vit(m, T=ATTN_T)\n    add_post_attn_layernorm_safe(m, eps=POST_LN_EPS)\n    prune_vit_to_n_blocks(m, n_blocks)\n\n    _filter_load_state_dict(m, sd)\n    return m, n_blocks\n\n# Register all custom classes BEFORE any torch.load of a full model\nadd_safe_globals([\n    VisionTransformer, PatchEmbed, Conv2d, Identity, Format, nn.Dropout,\n    TempScaledAttention, PostAttnLNBlock\n])\n\ndef try_load_full_model():\n    # Prefer reconstruct-from-weights (patchable & trainable), else load pickled full model\n    try:\n        m, n = build_model_from_weights(WEIGHTS_PATH, num_classes=NUM_CLASSES)\n        print(f\"‚úÖ Reconstructed TIMM model with {n} blocks\")\n        return m, n, True\n    except Exception as e:\n        print(f\"‚ùå Reconstruct-from-weights failed: {e}\\nüîÑ Falling back to loading a pickled full model: {TS_MODEL_PATH}\")\n        # This is NOT TorchScript; it was saved via torch.save(model, ...)\n        full = torch.load(TS_MODEL_PATH, map_location=\"cpu\")  # relies on add_safe_globals above\n        n = None\n        try:\n            if hasattr(full, \"blocks\"):\n                n = len(list(full.blocks))\n        except Exception:\n            pass\n        print(f\"‚úÖ Loaded pickled model; blocks: {n if n is not None else 'unknown'}\")\n        return full, n, False\n\n# --------------------- Normalization wrapper ---------------------\nclass NormalizeLayer(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1,3,1,1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1,3,1,1))\n    def forward(self, x): return (x - self.mean) / self.std\n\nclass ModelWithNorm(nn.Module):\n    def __init__(self, base_model, mean, std, logit_clip=0.0):\n        super().__init__()\n        self.norm = NormalizeLayer(mean, std)\n        self.base = base_model\n        self.logit_clip = float(logit_clip)\n    def forward(self, x):\n        x = self.norm(x); logits = self.base(x)\n        if self.logit_clip > 0.0:\n            logits = torch.clamp(logits, min=-self.logit_clip, max=self.logit_clip)\n        return logits\n\n# --------------------- Data ---------------------\ntfm = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=tfm)\nval_full      = datasets.ImageFolder(os.path.join(DATA_DIR, VAL_DIR_NAME), transform=tfm)\ntest_dataset  = datasets.ImageFolder(os.path.join(DATA_DIR, \"test\"),  transform=tfm)\n\ndef stratified_subset_indices(dataset, num_classes=NUM_CLASSES, total=1000, seed=42):\n    per_class = total // num_classes\n    cls2idxs = {c: [] for c in range(num_classes)}\n    for i, (_, c) in enumerate(dataset.samples): cls2idxs[c].append(i)\n    rng = np.random.default_rng(seed)\n    chosen = []\n    for c in range(num_classes):\n        idxs = cls2idxs[c]; rng.shuffle(idxs); chosen.extend(idxs[:per_class])\n    return chosen\n\nVAL_COUNT = 1000\nval_indices = stratified_subset_indices(val_full, total=VAL_COUNT)\nval_dataset = Subset(val_full, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n                          num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)\n\nprint(f\"üì¶ Datasets ‚Üí train:{len(train_dataset)} | val(full):{len(val_full)} | val(subset used):{len(val_dataset)} | test:{len(test_dataset)}\")\n\n# --------------------- Attacks ---------------------\ndef fgsm_attack(images, labels, model, epsilon=EPSILON):\n    images = images.clone().detach().to(device).requires_grad_(True)\n    labels = labels.to(device)\n    with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n    grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n    return (images + epsilon * grad.sign()).clamp(0,1).detach()\n\ndef pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=PGD_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); ori = images.clone()\n    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n    images = (ori + delta).clamp(0,1).detach()\n    for _ in range(steps):\n        images.requires_grad_(True)\n        with autocast('cuda'): logits = model(images); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n        images = images.detach() + alpha * grad.sign()\n        images = torch.max(torch.min(images, ori + epsilon), ori - epsilon).clamp(0,1).detach()\n    model.train(was_training); return images\n\ndef bim_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, steps=BIM_STEPS):\n    was_training = model.training; model.eval()\n    images = images.detach().to(device); labels = labels.to(device); adv = images.clone()\n    for _ in range(steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, images + epsilon), images - epsilon).clamp(0,1)\n    model.train(was_training); return adv.detach()\n\ndef hybrid_fgsm_pgd_attack(model, images, labels, epsilon=EPSILON, alpha=ALPHA, refinement_steps=5):\n    was_training = model.training; model.eval()\n    base = images.detach().clone().to(device); labels = labels.to(device)\n    adv = fgsm_attack(base, labels, model, epsilon)\n    for _ in range(refinement_steps):\n        adv.requires_grad_(True)\n        with autocast('cuda'): logits = model(adv); loss = F.cross_entropy(logits, labels)\n        grad = torch.autograd.grad(loss, adv, retain_graph=False, create_graph=False)[0]\n        adv = adv.detach() + alpha * grad.sign()\n        adv = torch.max(torch.min(adv, base + epsilon), base - epsilon).clamp(0,1).detach()\n    model.train(was_training); return adv\n\n# --------------------- Eval / Bench ---------------------\nloss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n@torch.no_grad()\ndef evaluate(model, loader, attack=None, return_loss=False):\n    model.eval(); correct=0; total=0; run_loss=0.0\n    for ims, lbs in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n        data = ims\n        if   attack==\"FGSM\":  torch.set_grad_enabled(True); data = fgsm_attack(ims, lbs, model); torch.set_grad_enabled(False)\n        elif attack==\"PGD\":   torch.set_grad_enabled(True); data = pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"BIM\":   torch.set_grad_enabled(True); data = bim_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        elif attack==\"Hybrid_FGSM_PGD\":\n            torch.set_grad_enabled(True); data = hybrid_fgsm_pgd_attack(model, ims, lbs); torch.set_grad_enabled(False)\n        with autocast('cuda'):\n            logits = model(data); loss = F.cross_entropy(logits, lbs); preds = logits.argmax(1)\n        correct += (preds==lbs).sum().item(); total += lbs.size(0); run_loss += loss.item()\n    acc = correct / max(total,1)\n    return (acc, run_loss / max(len(loader),1)) if return_loss else acc\n\ndef get_model_complexity(vit_or_ts, img_size=IMG_SIZE):\n    try:\n        m = vit_or_ts.eval().to(device)\n        dummy = torch.randn(1,3,img_size,img_size, device=device)\n        _sync()\n        try:\n            flops = FlopCountAnalysis(m, dummy).total()\n        except Exception:\n            m_cpu = m.to(\"cpu\")\n            with torch.no_grad():\n                macs, _ = get_model_complexity_info(m_cpu, (3,img_size,img_size), as_strings=False, print_per_layer_stat=False)\n            flops = macs * 2\n            m = m.to(device)\n        params = sum(p.numel() for p in m.parameters())\n        return flops, params\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Complexity analysis failed: {e}\")\n        return None, None\n\n@torch.no_grad()\ndef benchmark_inference(model, loader, max_batches=20, warmup_batches=3):\n    model.eval(); total_imgs=0; total_time=0.0\n    # warmup\n    w=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        with autocast('cuda'): _ = model(ims)\n        w+=1\n        if w>=warmup_batches: break\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    _sync(); measured=0\n    for ims,_ in loader:\n        ims = ims.to(device).to(memory_format=torch.channels_last)\n        _sync(); t0=time.time()\n        with autocast('cuda'): _ = model(ims)\n        _sync(); dt=time.time()-t0\n        total_time += dt; total_imgs += ims.size(0); measured+=1\n        if measured>=max_batches: break\n    lat_ms = (total_time/max(1,measured))*1000.0\n    thr    = total_imgs / max(total_time,1e-9)\n    peak   = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"batches\":measured,\"images\":total_imgs,\"avg_batch_latency_ms\":lat_ms,\"throughput_img_per_s\":thr,\"peak_infer_mem_mb\":peak}\n\n# --------------------- Early stopping ---------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA, restore_best=True):\n        self.patience=patience; self.min_delta=min_delta; self.restore_best=restore_best\n        self.best=math.inf; self.bad=0; self.best_state=None\n    def step(self, val_loss, model):\n        if (self.best - val_loss) > self.min_delta:\n            self.best = val_loss; self.bad=0\n            if self.restore_best:\n                self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n            return False\n        else:\n            self.bad+=1; return self.bad>self.patience\n    def restore(self, model):\n        if self.restore_best and self.best_state is not None: model.load_state_dict(self.best_state)\n\n# --------------------- Train loop ---------------------\ndef train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    model.train(); opt = torch.optim.AdamW(model.parameters(), lr=lr); scaler = GradScaler('cuda'); stopper = EarlyStopper()\n    if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n    hist=[]\n    for ep in range(1,epochs+1):\n        model.train(); run_loss=0.0; correct=0; total=0\n        for bi,(ims,lbs) in enumerate(train_loader):\n            ims = ims.to(device).to(memory_format=torch.channels_last); lbs = lbs.to(device)\n            adv_pgd = pgd_attack(model, ims, lbs); adv_bim = bim_attack(model, ims, lbs)\n            mix_x = torch.cat([ims, adv_pgd, adv_bim], dim=0); mix_y = torch.cat([lbs, lbs, lbs], dim=0)\n            opt.zero_grad(set_to_none=True)\n            with autocast('cuda'): logits = model(mix_x); loss = loss_fn(logits, mix_y)\n            preds = logits.argmax(1); correct += (preds==mix_y).sum().item(); total += mix_y.size(0)\n            scaler.scale(loss).backward(); scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n            run_loss += loss.item()\n            if bi%50==0: print(f\"Epoch {ep}/{epochs} | Batch {bi}/{len(train_loader)} | Loss {loss.item():.4f}\")\n        train_loss = run_loss/len(train_loader); train_acc = correct/max(total,1)\n        val_acc, val_loss = evaluate(model, val_loader, attack=None, return_loss=True)\n        print(f\"‚úÖ Epoch {ep}/{epochs} | TrainLoss {train_loss:.4f} | TrainAcc(mixed) {train_acc*100:.2f}% | ValLoss {val_loss:.4f} | ValAcc {val_acc*100:.2f}%\")\n        hist.append({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc_mixed\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc})\n        if stopper.step(val_loss, model):\n            print(f\"‚õî Early stopping at epoch {ep}\")\n            break\n    stopper.restore(model)\n    peak_train_mb = torch.cuda.max_memory_allocated()/1024**2 if torch.cuda.is_available() else 0.0\n    return {\"train_peak_mem_mb\":peak_train_mb, \"history\":hist}\n\n# --------------------- Load model ---------------------\nprint(\"üß© Loading your pruned/variant-correct model...\")\nbase_model, detected_blocks, patchable = try_load_full_model()\n\n# (Optional) force prune to N blocks\nif FORCE_KEEP_BLOCKS is not None:\n    prune_vit_to_n_blocks(base_model, FORCE_KEEP_BLOCKS)\n    print(f\"‚úÇÔ∏è Force-pruned to {FORCE_KEEP_BLOCKS} blocks\")\n\nbase_model = base_model.to(device).eval()\nprint(f\"üìä Model ready on {device} | blocks: {detected_blocks if detected_blocks is not None else 'unknown'} | patchable={patchable}\")\n\n# Wrap with ImageNet normalization\nmodel = ModelWithNorm(base_model, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], logit_clip=0.0).to(device)\nmodel.to(memory_format=torch.channels_last).eval()\n\n# --------------------- FLOPs / Params ---------------------\nflops, params = get_model_complexity(base_model, IMG_SIZE)\nif flops is not None: print(f\"üßÆ Complexity: {flops/1e9:.3f} GFLOPs | Params: {params/1e6:.2f} M\")\n\nprint_memory(\"Before Training\")\n\n# --------------------- BEFORE training (TEST) ---------------------\nprint(\"\\nüîç BEFORE adversarial training (TEST set)...\")\nclean_b = evaluate(model, test_loader, attack=None)\nfgsm_b  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_b   = evaluate(model, test_loader, attack=\"PGD\")\nbim_b   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_b   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_b = benchmark_inference(model, test_loader, max_batches=20)\n\nprint(f\"üìä BEFORE (TEST): Clean {clean_b*100:.2f}% | FGSM {fgsm_b*100:.2f}% | PGD {pgd_b*100:.2f}% | BIM {bim_b*100:.2f}% | Hybrid {hyb_b*100:.2f}%\")\nprint(f\"‚ö° Inference BEFORE: latency {infer_b['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_b['throughput_img_per_s']:.2f} img/s | peak mem {infer_b['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Train ---------------------\nprint(\"\\nüöÄ Training (PGD + BIM mixed) with validation & early stopping...\")\nt0 = time.time()\ntrain_metrics = train_mixed_adv(model, train_loader, val_loader, epochs=EPOCHS, lr=LR)\ntrain_time = time.time() - t0\nprint(f\"‚è±Ô∏è Training time: {train_time:.2f} s\")\nprint(f\"üß† Peak training memory: {train_metrics['train_peak_mem_mb']:.2f} MB\")\nprint_memory(\"After Training\")\n\n# --------------------- AFTER training ---------------------\ntrain_acc_after = evaluate(model, train_loader, attack=None)\nval_acc_after   = evaluate(model, val_loader,   attack=None)\nprint(f\"\\nüéØ AFTER Training ‚Üí Train(clean): {train_acc_after*100:.2f}% | Val(clean,1000): {val_acc_after*100:.2f}%\")\n\nprint(\"\\nüîç AFTER adversarial training (TEST set)...\")\nclean_a = evaluate(model, test_loader, attack=None)\nfgsm_a  = evaluate(model, test_loader, attack=\"FGSM\")\npgd_a   = evaluate(model, test_loader, attack=\"PGD\")\nbim_a   = evaluate(model, test_loader, attack=\"BIM\")\nhyb_a   = evaluate(model, test_loader, attack=\"Hybrid_FGSM_PGD\")\ninfer_a = benchmark_inference(model, test_loader, max_batches=20)\n\nprint(f\"üìä AFTER (TEST): Clean {clean_a*100:.2f}% | FGSM {fgsm_a*100:.2f}% | PGD {pgd_a*100:.2f}% | BIM {bim_a*100:.2f}% | Hybrid {hyb_a*100:.2f}%\")\nprint(f\"‚ö° Inference AFTER: latency {infer_a['avg_batch_latency_ms']:.2f} ms/batch | throughput {infer_a['throughput_img_per_s']:.2f} img/s | peak mem {infer_a['peak_infer_mem_mb']:.2f} MB\")\n\n# --------------------- Save ---------------------\n# Save AFTER-training model (architecture) and weights\nARCH_PATH = \"/kaggle/working/architecture.pth\"\nWEI_PATH  = \"/kaggle/working/weights.pth\"\n\n# Save the full wrapped model (includes normalization + base model)\ntorch.save(model, ARCH_PATH)\n# Save just the weights of the wrapped model\ntorch.save(model.state_dict(), WEI_PATH)\n\nprint(f\"\\nüíæ Saved models:\")\nprint(f\"   ‚Ä¢ Full architecture  ‚Üí {ARCH_PATH}\")\nprint(f\"   ‚Ä¢ Weights only       ‚Üí {WEI_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"this evalautes models original arc against attacks without prunuing","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}